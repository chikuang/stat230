<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Stat 230 Introduction to Probability</title>
  <meta name="description" content="<p>This is an example site for UW Stat 230, 2024 Winter</p>" />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="Stat 230 Introduction to Probability" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is an example site for UW Stat 230, 2024 Winter</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Stat 230 Introduction to Probability" />
  
  <meta name="twitter:description" content="<p>This is an example site for UW Stat 230, 2024 Winter</p>" />
  



<meta name="date" content="2024-03-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">Stat 230 Introduction to Probability</h1>
<h2 class="subtitle"><em>Winter 2024</em></h2>
<p class="author"><em><div class="line-block">Chi-Kuang Yeh<br />
University of Waterloo</div></em></p>
<p class="date"><em>2024-03-24</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#information-of-the-course" id="toc-information-of-the-course"><span class="toc-section-number">1</span> Information of the course</a>
<ul>
<li><a href="#course-description" id="toc-course-description"><span class="toc-section-number">1.1</span> Course description</a>
<ul>
<li><a href="#instructor" id="toc-instructor"><span class="toc-section-number">1.1.1</span> Instructor</a></li>
<li><a href="#course-coordinator" id="toc-course-coordinator"><span class="toc-section-number">1.1.2</span> Course Coordinator</a></li>
<li><a href="#logistic-issue" id="toc-logistic-issue"><span class="toc-section-number">1.1.3</span> Logistic Issue</a></li>
<li><a href="#exam-and-tutorial-assessment-date" id="toc-exam-and-tutorial-assessment-date"><span class="toc-section-number">1.1.4</span> EXAM and Tutorial assessment Date</a></li>
</ul></li>
<li><a href="#chapters-and-associated-lectures" id="toc-chapters-and-associated-lectures"><span class="toc-section-number">1.2</span> Chapters and associated Lectures</a></li>
</ul></li>
<li><a href="#lecture-1-january-08-2024" id="toc-lecture-1-january-08-2024"><span class="toc-section-number">2</span> Lecture 1, January 08, 2024</a></li>
<li><a href="#lecture-2-january-10-2024" id="toc-lecture-2-january-10-2024"><span class="toc-section-number">3</span> Lecture 2, January 10, 2024</a>
<ul>
<li><a href="#questions-from-the-class" id="toc-questions-from-the-class"><span class="toc-section-number">3.1</span> Questions from the class</a></li>
</ul></li>
<li><a href="#lecture-3-january-12-2024" id="toc-lecture-3-january-12-2024"><span class="toc-section-number">4</span> Lecture 3, January 12, 2024</a>
<ul>
<li><a href="#questions-from-the-class-1" id="toc-questions-from-the-class-1"><span class="toc-section-number">4.1</span> Questions from the class</a></li>
</ul></li>
<li><a href="#lecture-4-january-15-2024" id="toc-lecture-4-january-15-2024"><span class="toc-section-number">5</span> Lecture 4, January 15, 2024</a></li>
<li><a href="#lecture-5-january-17-2024" id="toc-lecture-5-january-17-2024"><span class="toc-section-number">6</span> Lecture 5, January 17, 2024</a>
<ul>
<li><a href="#example-in-class" id="toc-example-in-class"><span class="toc-section-number">6.1</span> Example in class</a></li>
</ul></li>
<li><a href="#lecture-6-january-19-2024" id="toc-lecture-6-january-19-2024"><span class="toc-section-number">7</span> Lecture 6, January 19, 2024</a>
<ul>
<li><a href="#multinomial-coefficient" id="toc-multinomial-coefficient"><span class="toc-section-number">7.0.1</span> Multinomial Coefficient</a></li>
<li><a href="#the-birthday-problem" id="toc-the-birthday-problem"><span class="toc-section-number">7.0.2</span> The Birthday Problem</a></li>
<li><a href="#chapter-4-probbility-rules-and-conditional-probability" id="toc-chapter-4-probbility-rules-and-conditional-probability"><span class="toc-section-number">7.0.3</span> Chapter 4 Probbility Rules and Conditional Probability</a></li>
</ul></li>
<li><a href="#lecture-7-january-22-2024" id="toc-lecture-7-january-22-2024"><span class="toc-section-number">8</span> Lecture 7, January 22, 2024</a>
<ul>
<li><a href="#some-terminology-about-the-set-thoery." id="toc-some-terminology-about-the-set-thoery."><span class="toc-section-number">8.0.1</span> Some terminology about the set thoery.</a></li>
<li><a href="#independence" id="toc-independence"><span class="toc-section-number">8.0.2</span> Independence</a></li>
<li><a href="#independence-v.s.-multually-exclusivedisjoint" id="toc-independence-v.s.-multually-exclusivedisjoint"><span class="toc-section-number">8.0.3</span> Independence v.s. Multually Exclusive/Disjoint</a></li>
</ul></li>
<li><a href="#lecture-8-january-24-2024" id="toc-lecture-8-january-24-2024"><span class="toc-section-number">9</span> Lecture 8, January 24, 2024</a>
<ul>
<li><a href="#properties-of-conditional-probability" id="toc-properties-of-conditional-probability"><span class="toc-section-number">9.0.1</span> Properties of Conditional Probability</a></li>
</ul></li>
<li><a href="#lecture-9-january-26-2024" id="toc-lecture-9-january-26-2024"><span class="toc-section-number">10</span> Lecture 9, January 26, 2024</a>
<ul>
<li><a href="#law-of-total-probability" id="toc-law-of-total-probability"><span class="toc-section-number">10.0.1</span> Law of Total Probability</a></li>
<li><a href="#bayes-rule" id="toc-bayes-rule"><span class="toc-section-number">10.0.2</span> Bayes Rule</a></li>
</ul></li>
<li><a href="#lecture-10-january-29-2024" id="toc-lecture-10-january-29-2024"><span class="toc-section-number">11</span> Lecture 10, January 29, 2024</a>
<ul>
<li><a href="#chapter-5.-discrete-random-variable" id="toc-chapter-5.-discrete-random-variable"><span class="toc-section-number">11.0.1</span> Chapter 5. Discrete Random Variable</a></li>
</ul></li>
<li><a href="#lecture-11-january-31-2024" id="toc-lecture-11-january-31-2024"><span class="toc-section-number">12</span> Lecture 11, January 31, 2024</a>
<ul>
<li><a href="#distinction-of-the-definition-discrete-of-the-sample-space-and-the-random-variable" id="toc-distinction-of-the-definition-discrete-of-the-sample-space-and-the-random-variable"><span class="toc-section-number">12.1</span> Distinction of the definition “discrete” of the sample space and the random variable</a></li>
<li><a href="#cumulative-distribution-function" id="toc-cumulative-distribution-function"><span class="toc-section-number">12.2</span> Cumulative distribution function</a>
<ul>
<li><a href="#properties-of-the-cumulative-distribution-function" id="toc-properties-of-the-cumulative-distribution-function"><span class="toc-section-number">12.2.1</span> Properties of the cumulative distribution function</a></li>
</ul></li>
<li><a href="#special-distributions-with-names" id="toc-special-distributions-with-names"><span class="toc-section-number">12.3</span> Special distributions with names</a>
<ul>
<li><a href="#discrete-uniform-distribution" id="toc-discrete-uniform-distribution"><span class="toc-section-number">12.3.1</span> Discrete uniform distribution</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-12-feburary-02-2024" id="toc-lecture-12-feburary-02-2024"><span class="toc-section-number">13</span> Lecture 12, Feburary 02, 2024</a>
<ul>
<li><a href="#hypergeometric-distribution" id="toc-hypergeometric-distribution"><span class="toc-section-number">13.1</span> Hypergeometric distribution</a>
<ul>
<li><a href="#range-of-the-hypergeomnetric-distribution" id="toc-range-of-the-hypergeomnetric-distribution"><span class="toc-section-number">13.1.1</span> Range of the Hypergeomnetric distribution</a></li>
<li><a href="#probability-function-of-hypergeometric-distribution" id="toc-probability-function-of-hypergeometric-distribution"><span class="toc-section-number">13.1.2</span> Probability function of hypergeometric distribution</a></li>
</ul></li>
<li><a href="#bernoulli-and-binimial-distributions" id="toc-bernoulli-and-binimial-distributions"><span class="toc-section-number">13.2</span> Bernoulli and Binimial distributions</a>
<ul>
<li><a href="#probability-function-and-cumulative-distribution-function" id="toc-probability-function-and-cumulative-distribution-function"><span class="toc-section-number">13.2.1</span> Probability function and cumulative distribution function</a></li>
<li><a href="#bernoulli" id="toc-bernoulli"><span class="toc-section-number">13.2.2</span> Bernoulli</a></li>
<li><a href="#binomial" id="toc-binomial"><span class="toc-section-number">13.2.3</span> Binomial</a></li>
</ul></li>
<li><a href="#relationship-and-difference-between-binomial-and-hypergeometric" id="toc-relationship-and-difference-between-binomial-and-hypergeometric"><span class="toc-section-number">13.3</span> Relationship and difference between binomial and hypergeometric</a></li>
</ul></li>
<li><a href="#lecture-13-feburary-05-2024" id="toc-lecture-13-feburary-05-2024"><span class="toc-section-number">14</span> Lecture 13, Feburary 05, 2024</a>
<ul>
<li><a href="#binomial-v.s.-hypergeometric" id="toc-binomial-v.s.-hypergeometric"><span class="toc-section-number">14.1</span> Binomial v.s. hypergeometric</a></li>
<li><a href="#negative-binomial-distribution" id="toc-negative-binomial-distribution"><span class="toc-section-number">14.2</span> Negative binomial distribution</a>
<ul>
<li><a href="#probability-function" id="toc-probability-function"><span class="toc-section-number">14.2.1</span> Probability function</a></li>
</ul></li>
<li><a href="#binomial-v.s.-negative-binomial" id="toc-binomial-v.s.-negative-binomial"><span class="toc-section-number">14.3</span> Binomial v.s. Negative Binomial</a></li>
<li><a href="#geometric-distribution" id="toc-geometric-distribution"><span class="toc-section-number">14.4</span> Geometric distribution</a>
<ul>
<li><a href="#probability-function-and-distribution-function-1" id="toc-probability-function-and-distribution-function-1"><span class="toc-section-number">14.4.1</span> Probability function and distribution function</a></li>
<li><a href="#memoryless-property" id="toc-memoryless-property"><span class="toc-section-number">14.4.2</span> Memoryless property</a></li>
<li><a href="#aside-reason-to-call-the-negative-binomial-distribution" id="toc-aside-reason-to-call-the-negative-binomial-distribution"><span class="toc-section-number">14.4.3</span> Aside, Reason to call the Negative binomial distribution</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-14-feburary-07-2024" id="toc-lecture-14-feburary-07-2024"><span class="toc-section-number">15</span> Lecture 14, Feburary 07, 2024</a>
<ul>
<li><a href="#poisson-distribution" id="toc-poisson-distribution"><span class="toc-section-number">15.1</span> Poisson distribution</a>
<ul>
<li><a href="#notation" id="toc-notation"><span class="toc-section-number">15.1.1</span> Notation</a></li>
<li><a href="#interpreation-of-the-poisson-distribution" id="toc-interpreation-of-the-poisson-distribution"><span class="toc-section-number">15.1.2</span> Interpreation of the Poisson distribution</a></li>
</ul></li>
<li><a href="#poisson-as-the-limiting-distribution-of-the-binomial-distribution" id="toc-poisson-as-the-limiting-distribution-of-the-binomial-distribution"><span class="toc-section-number">15.2</span> Poisson as the limiting distribution of the binomial distribution</a></li>
<li><a href="#poisson-process" id="toc-poisson-process"><span class="toc-section-number">15.3</span> Poisson process</a></li>
<li><a href="#side-notes-rigorous-definition-of-convergence-in-distribution" id="toc-side-notes-rigorous-definition-of-convergence-in-distribution"><span class="toc-section-number">15.4</span> Side notes – Rigorous definition of convergence in distribution</a></li>
</ul></li>
<li><a href="#lecture-15-feburary-09-2024" id="toc-lecture-15-feburary-09-2024"><span class="toc-section-number">16</span> Lecture 15, Feburary 09, 2024</a>
<ul>
<li><a href="#review-of-the-distributions-we-covered-before" id="toc-review-of-the-distributions-we-covered-before"><span class="toc-section-number">16.1</span> Review of the distributions we covered before</a></li>
</ul></li>
<li><a href="#lecture-16-feburary-12-2024" id="toc-lecture-16-feburary-12-2024"><span class="toc-section-number">17</span> Lecture 16, Feburary 12, 2024</a>
<ul>
<li><a href="#chapter-7-expectation-and-variance" id="toc-chapter-7-expectation-and-variance"><span class="toc-section-number">17.1</span> Chapter 7 Expectation and Variance</a></li>
<li><a href="#theoretical-mean-and-the-sample-mean" id="toc-theoretical-mean-and-the-sample-mean"><span class="toc-section-number">17.2</span> Theoretical mean and the sample mean</a>
<ul>
<li><a href="#definition" id="toc-definition"><span class="toc-section-number">17.2.1</span> Definition</a></li>
<li><a href="#interpretation" id="toc-interpretation"><span class="toc-section-number">17.2.2</span> Interpretation</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-17-feburary-14-2024" id="toc-lecture-17-feburary-14-2024"><span class="toc-section-number">18</span> Lecture 17, Feburary 14, 2024</a>
<ul>
<li><a href="#more-about-expectation" id="toc-more-about-expectation"><span class="toc-section-number">18.1</span> More about expectation</a></li>
<li><a href="#law-of-unconscious-statistician" id="toc-law-of-unconscious-statistician"><span class="toc-section-number">18.2</span> Law of Unconscious Statistician</a></li>
<li><a href="#tricks" id="toc-tricks"><span class="toc-section-number">18.3</span> Tricks</a></li>
<li><a href="#property-of-expectation---linearity" id="toc-property-of-expectation---linearity"><span class="toc-section-number">18.4</span> Property of expectation - linearity</a>
<ul>
<li><a href="#proof." id="toc-proof."><span class="toc-section-number">18.4.1</span> Proof.</a></li>
</ul></li>
<li><a href="#mean-of-binomial-distribution" id="toc-mean-of-binomial-distribution"><span class="toc-section-number">18.5</span> Mean of binomial distribution</a></li>
</ul></li>
<li><a href="#lecture-18-feburary-16-2024" id="toc-lecture-18-feburary-16-2024"><span class="toc-section-number">19</span> Lecture 18, Feburary 16, 2024</a>
<ul>
<li><a href="#mean-of-poisson-distribution" id="toc-mean-of-poisson-distribution"><span class="toc-section-number">19.1</span> Mean of Poisson distribution</a></li>
<li><a href="#mean-of-hypergeometric-and-negative-binomial" id="toc-mean-of-hypergeometric-and-negative-binomial"><span class="toc-section-number">19.2</span> Mean of Hypergeometric and Negative binomial</a>
<ul>
<li><a href="#hypergeometric" id="toc-hypergeometric"><span class="toc-section-number">19.2.1</span> Hypergeometric</a></li>
<li><a href="#negative-binomial" id="toc-negative-binomial"><span class="toc-section-number">19.2.2</span> Negative binomial</a></li>
</ul></li>
<li><a href="#note-that-the-expectation-does-not-always-exist" id="toc-note-that-the-expectation-does-not-always-exist"><span class="toc-section-number">19.3</span> Note that the expectation does not always exist</a></li>
</ul></li>
<li><a href="#lecture-19-feburary-26-2024" id="toc-lecture-19-feburary-26-2024"><span class="toc-section-number">20</span> Lecture 19, Feburary 26, 2024</a>
<ul>
<li><a href="#motivation-to-have-higher-moments" id="toc-motivation-to-have-higher-moments"><span class="toc-section-number">20.1</span> Motivation to have higher moments</a>
<ul>
<li><a href="#deviations-from-the-mean" id="toc-deviations-from-the-mean"><span class="toc-section-number">20.1.1</span> Deviations from the mean</a></li>
</ul></li>
<li><a href="#the-defintion-of-variance" id="toc-the-defintion-of-variance"><span class="toc-section-number">20.2</span> The defintion of variance</a>
<ul>
<li><a href="#properties-of-variance" id="toc-properties-of-variance"><span class="toc-section-number">20.2.1</span> Properties of variance</a></li>
</ul></li>
<li><a href="#variance-of-a-binomial-distribution" id="toc-variance-of-a-binomial-distribution"><span class="toc-section-number">20.3</span> Variance of a binomial distribution</a></li>
</ul></li>
<li><a href="#lecture-20-feburary-28-2024" id="toc-lecture-20-feburary-28-2024"><span class="toc-section-number">21</span> Lecture 20, Feburary 28, 2024</a>
<ul>
<li><a href="#variance-of-poisson-hypergeometric-and-negative-binomial" id="toc-variance-of-poisson-hypergeometric-and-negative-binomial"><span class="toc-section-number">21.1</span> Variance of Poisson, Hypergeometric and Negative Binomial</a></li>
<li><a href="#standard-deviation" id="toc-standard-deviation"><span class="toc-section-number">21.2</span> Standard Deviation</a></li>
<li><a href="#last-note-of-the-chapter" id="toc-last-note-of-the-chapter"><span class="toc-section-number">21.3</span> Last note of the chapter</a></li>
<li><a href="#chapter-8-continuous-random-variables" id="toc-chapter-8-continuous-random-variables"><span class="toc-section-number">21.4</span> Chapter 8 Continuous Random Variables</a>
<ul>
<li><a href="#continuous-random-variable" id="toc-continuous-random-variable"><span class="toc-section-number">21.4.1</span> Continuous random variable</a></li>
<li><a href="#equality-does-not-matter-in-the-continous-case" id="toc-equality-does-not-matter-in-the-continous-case"><span class="toc-section-number">21.4.2</span> Equality does not matter in the continous case</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-21-march-01-2024" id="toc-lecture-21-march-01-2024"><span class="toc-section-number">22</span> Lecture 21, March 01, 2024</a>
<ul>
<li><a href="#law-of-unconciousness-of-statistician-continuous-version" id="toc-law-of-unconciousness-of-statistician-continuous-version"><span class="toc-section-number">22.1</span> Law of unconciousness of statistician, continuous version</a></li>
<li><a href="#function-of-random-variable" id="toc-function-of-random-variable"><span class="toc-section-number">22.2</span> function of random variable</a></li>
</ul></li>
<li><a href="#lecture-22-march-04-2024" id="toc-lecture-22-march-04-2024"><span class="toc-section-number">23</span> Lecture 22, March 04, 2024</a>
<ul>
<li><a href="#receipt-to-find-the-distribution-of-the-transformed-random-varaible-ygx." id="toc-receipt-to-find-the-distribution-of-the-transformed-random-varaible-ygx."><span class="toc-section-number">23.1</span> Receipt to find the distribution of the transformed random varaible <span class="math inline">\(Y=g(X)\)</span>.</a></li>
<li><a href="#quantile" id="toc-quantile"><span class="toc-section-number">23.2</span> Quantile</a></li>
</ul></li>
<li><a href="#lecture-23-march-06-2024" id="toc-lecture-23-march-06-2024"><span class="toc-section-number">24</span> Lecture 23, March 06, 2024</a>
<ul>
<li><a href="#recap-the-quantile-function" id="toc-recap-the-quantile-function"><span class="toc-section-number">24.1</span> Recap the quantile function</a></li>
<li><a href="#quantiles-for-discrete-distributions" id="toc-quantiles-for-discrete-distributions"><span class="toc-section-number">24.2</span> Quantiles for discrete distributions</a></li>
<li><a href="#special-named-distributions" id="toc-special-named-distributions"><span class="toc-section-number">24.3</span> Special named distributions</a>
<ul>
<li><a href="#continuous-uniform-distribution" id="toc-continuous-uniform-distribution"><span class="toc-section-number">24.3.1</span> Continuous uniform distribution</a></li>
<li><a href="#exponential-distribution" id="toc-exponential-distribution"><span class="toc-section-number">24.3.2</span> Exponential distribution</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-24-march-08-2024" id="toc-lecture-24-march-08-2024"><span class="toc-section-number">25</span> Lecture 24, March 08, 2024</a>
<ul>
<li><a href="#proof-of-the-moments-of-exponential-distribution" id="toc-proof-of-the-moments-of-exponential-distribution"><span class="toc-section-number">25.1</span> Proof of the moments of exponential distribution</a>
<ul>
<li><a href="#memoryless-property-of-exponential-distribution" id="toc-memoryless-property-of-exponential-distribution"><span class="toc-section-number">25.1.1</span> Memoryless property of exponential distribution</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-24-march-11-2024" id="toc-lecture-24-march-11-2024"><span class="toc-section-number">26</span> Lecture 24, March 11, 2024</a>
<ul>
<li><a href="#sampling-realizations-of-random-variables" id="toc-sampling-realizations-of-random-variables"><span class="toc-section-number">26.1</span> Sampling realizations of random variables</a></li>
<li><a href="#inversion-method" id="toc-inversion-method"><span class="toc-section-number">26.2</span> Inversion method</a>
<ul>
<li><a href="#with-strictly-increasing-and-continuous-assumption" id="toc-with-strictly-increasing-and-continuous-assumption"><span class="toc-section-number">26.2.1</span> With strictly increasing and continuous assumption</a></li>
<li><a href="#more-general-case-using-the-quantile" id="toc-more-general-case-using-the-quantile"><span class="toc-section-number">26.2.2</span> More general case using the quantile</a></li>
</ul></li>
<li><a href="#normalgaussian-distribution" id="toc-normalgaussian-distribution"><span class="toc-section-number">26.3</span> Normal/Gaussian distribution</a>
<ul>
<li><a href="#properties-of-gaussian-distribution" id="toc-properties-of-gaussian-distribution"><span class="toc-section-number">26.3.1</span> Properties of Gaussian distribution</a></li>
<li><a href="#problem" id="toc-problem"><span class="toc-section-number">26.3.2</span> Problem</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-26-march-13-2024" id="toc-lecture-26-march-13-2024"><span class="toc-section-number">27</span> Lecture 26, March 13, 2024</a>
<ul>
<li><a href="#normal-distribution-continue" id="toc-normal-distribution-continue"><span class="toc-section-number">27.1</span> Normal distribution continue</a>
<ul>
<li><a href="#standard-normal-distribution" id="toc-standard-normal-distribution"><span class="toc-section-number">27.1.1</span> Standard normal distribution</a></li>
<li><a href="#procedure" id="toc-procedure"><span class="toc-section-number">27.1.2</span> Procedure</a></li>
<li><a href="#quantile-1" id="toc-quantile-1"><span class="toc-section-number">27.1.3</span> Quantile</a></li>
<li><a href="#rule-for-1-2-3-standard-deviations" id="toc-rule-for-1-2-3-standard-deviations"><span class="toc-section-number">27.1.4</span> 68-95-99.7 rule for 1-2-3 standard deviation(s)</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-27-march-15-2024" id="toc-lecture-27-march-15-2024"><span class="toc-section-number">28</span> Lecture 27, March 15, 2024</a>
<ul>
<li><a href="#chapter-9-multivariate-distributions" id="toc-chapter-9-multivariate-distributions"><span class="toc-section-number">28.1</span> Chapter 9: Multivariate distributions</a>
<ul>
<li><a href="#properties-of-the-joint-probability-function" id="toc-properties-of-the-joint-probability-function"><span class="toc-section-number">28.1.1</span> Properties of the joint probability function</a></li>
<li><a href="#marginal-distribution-of-the-joint-distribution-function" id="toc-marginal-distribution-of-the-joint-distribution-function"><span class="toc-section-number">28.1.2</span> Marginal distribution of the joint distribution function</a></li>
<li><a href="#comments" id="toc-comments"><span class="toc-section-number">28.1.3</span> Comments</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-28-march-18-2024" id="toc-lecture-28-march-18-2024"><span class="toc-section-number">29</span> Lecture 28, March 18, 2024</a>
<ul>
<li><a href="#conditional-probability-function-for-multivaraite-random-variable" id="toc-conditional-probability-function-for-multivaraite-random-variable"><span class="toc-section-number">29.0.1</span> Conditional probability function for multivaraite random variable</a></li>
<li><a href="#probably-function-of-ugx_1cdotsx_n" id="toc-probably-function-of-ugx_1cdotsx_n"><span class="toc-section-number">29.0.2</span> Probably function of <span class="math inline">\(U=g(X_1,\cdots,X_n)\)</span></a></li>
<li><a href="#known-results-for-named-distributions" id="toc-known-results-for-named-distributions"><span class="toc-section-number">29.0.3</span> Known results for named distributions</a></li>
</ul></li>
<li><a href="#lecture-29-march-20-2024" id="toc-lecture-29-march-20-2024"><span class="toc-section-number">30</span> Lecture 29, March 20, 2024</a>
<ul>
<li><a href="#multinomial-distribution" id="toc-multinomial-distribution"><span class="toc-section-number">30.1</span> Multinomial distribution</a>
<ul>
<li><a href="#the-probability-distribution-function" id="toc-the-probability-distribution-function"><span class="toc-section-number">30.1.1</span> The probability distribution function</a></li>
</ul></li>
</ul></li>
<li><a href="#lecture-30-march-22-2024" id="toc-lecture-30-march-22-2024"><span class="toc-section-number">31</span> Lecture 30, March 22, 2024</a>
<ul>
<li><a href="#summary-statistics-of-multivariate-distributions" id="toc-summary-statistics-of-multivariate-distributions"><span class="toc-section-number">31.0.1</span> Summary statistics of multivariate distributions</a></li>
<li><a href="#relationship-betwen-the-random-variables" id="toc-relationship-betwen-the-random-variables"><span class="toc-section-number">31.0.2</span> Relationship betwen the random variables</a></li>
<li><a href="#relationship-between-independence-and-covariance" id="toc-relationship-between-independence-and-covariance"><span class="toc-section-number">31.0.3</span> Relationship between independence and covariance</a></li>
</ul></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stat 230 Introduction to Probability</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="information-of-the-course" class="section level1 hasAnchor" number="1">
<h1 class="hasAnchor"><span class="header-section-number">1</span> Information of the course<a href="#information-of-the-course" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The purpose of this page is to hold some of the additional materials provided by myself. Students should consult UW <a href="https://api-4ccc589b.duosecurity.com/frame/v4/preauth/healthcheck?sid=frameless-c0657e9d-cb86-4ac9-a6a7-fd054ae21fd5">Learn</a> system.</p>
<div id="course-description" class="section level2 hasAnchor" number="1.1">
<h2 class="hasAnchor"><span class="header-section-number">1.1</span> Course description<a href="#course-description" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This course provides an introduction to probability models including sample spaces, mutually exclusive and independent events, conditional probability and Bayes’ Theorem. The named distributions (Discrete Uniform, Hypergeometric, Binomial, Negative Binomial, Geometric, Poisson, Continuous Uniform, Exponential, Normal (Gaussian), and Multinomial) are used to model real phenomena. Discrete and continuous univariate random variables and their distributions are discussed. Joint probability functions, marginal probability functions, and conditional probability functions of two or more discrete random variables and functions of random variables are also discussed. Students learn how to calculate and interpret means, variances and covariances particularly for the named distributions. The Central Limit Theorem is used to approximate probabilities.</p>
<div id="instructor" class="section level3 hasAnchor" number="1.1.1">
<h3 class="hasAnchor"><span class="header-section-number">1.1.1</span> Instructor<a href="#instructor" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Chi-Kuang Yeh, I am a postdoc at the <em>Department of Statistics and Actuarial Science</em>.</p>
<ul>
<li>Office: M3–3102 Desk 10, but I hold office hour at M3 - 2101 Desk 1, 9:30 – 10:30 on Tuesday.</li>
<li>Email: <a href="mailto:chi-kuang.yeh@uwaterloo.ca">chi-kuang.yeh@uwaterloo.ca</a></li>
</ul>
</div>
<div id="course-coordinator" class="section level3 hasAnchor" number="1.1.2">
<h3 class="hasAnchor"><span class="header-section-number">1.1.2</span> Course Coordinator<a href="#course-coordinator" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dr. <a href="https://uwaterloo.ca/scholar/ehintz">Erik Hintz</a>.</p>
<ul>
<li>Office: M3–2106</li>
<li>Email: <a href="mailto:erik.hintz@uwaterloo.ca">erik.hintz@uuwaterloo.ca</a></li>
</ul>
</div>
<div id="logistic-issue" class="section level3 hasAnchor" number="1.1.3">
<h3 class="hasAnchor"><span class="header-section-number">1.1.3</span> Logistic Issue<a href="#logistic-issue" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Contact Divya Lala</p>
<ul>
<li>Email: <a href="mailto:divya.lala@uwaterloo.ca">divya.lala@uwaterloo.ca</a> or the undergrad advising email <a href="mailto:sasugradadv@uwaterloo.ca">sasugradadv@uwaterloo.ca</a>.</li>
</ul>
</div>
<div id="exam-and-tutorial-assessment-date" class="section level3 hasAnchor" number="1.1.4">
<h3 class="hasAnchor"><span class="header-section-number">1.1.4</span> EXAM and Tutorial assessment Date<a href="#exam-and-tutorial-assessment-date" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Midterm</p>
<ul class="task-list">
<li><input type="checkbox" checked="" />Midterm 1: February 08, 2024 16:30–17:50 (Coverage: Ch. 1 – 5.1)</li>
<li><input type="checkbox" checked="" />Midterm 2: March 14, 2024 16:30–17:50 (Coverage: Ch. 1–5, 7-8, up to Sec. 8.3)</li>
</ul>
<p>Final</p>
<ul class="task-list">
<li><input type="checkbox" />Tuesday April 16, 2024 19:30 – 22:00. Location: DC 1350 and DC 1351</li>
</ul>
<p>Tutorial assessment</p>
<ul class="task-list">
<li><input type="checkbox" checked="" />Tutorial quiz 1: January 26, 2024 (Coverage: Ch. 1–3)</li>
<li><input type="checkbox" checked="" />Tutorial test 1: February 02, 2024 (Coverage: Ch. 1–4)</li>
<li><input type="checkbox" checked="" />Tutorial quiz 2: March 01, 2024 (Coverage: Ch. 1-4, and Ch. 7, up to Sec. 7.3)</li>
<li><input type="checkbox" checked="" />Tutorial test 2: March 08, 2024 (Coverage: Ch. 1-5, 7-8 up to Sec. 8.1)</li>
<li><input type="checkbox" checked="" />Tutorial quiz 3: March 22, 2024 (Coverage: Up to Sec. 9.1, exclude the independence)</li>
<li><input type="checkbox" />Tutorial test 3: April 05, 2024</li>
</ul>
</div>
</div>
<div id="chapters-and-associated-lectures" class="section level2 hasAnchor" number="1.2">
<h2 class="hasAnchor"><span class="header-section-number">1.2</span> Chapters and associated Lectures<a href="#chapters-and-associated-lectures" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Those chapters are based on the lecture notes. The lecture covered is based on <em>Section 002</em>. This part will be updated frequently.</p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th>Chapter</th>
<th>Lecture Covered</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1. Introduction to Probability</td>
<td>L1</td>
</tr>
<tr class="even">
<td>2. Mathematical Probability Models</td>
<td>L2–3</td>
</tr>
<tr class="odd">
<td>3. Probability and Counting Techniques</td>
<td>L3–6</td>
</tr>
<tr class="even">
<td>4. Probability rules and Conditional Probability</td>
<td>6–9</td>
</tr>
<tr class="odd">
<td>5. Discrete Random Variable</td>
<td>L10 –16</td>
</tr>
<tr class="even">
<td>6. Computational Methods and the Statistical Software R</td>
<td>In tutorial (not testable)</td>
</tr>
<tr class="odd">
<td>7. Expected Value and Variance</td>
<td>L16 –20</td>
</tr>
<tr class="even">
<td>8. Continuous Random Variable</td>
<td>L20 – 27</td>
</tr>
<tr class="odd">
<td>9. Multivariate Distributions</td>
<td>L27 –</td>
</tr>
<tr class="even">
<td>10. TBA</td>
<td>TBA</td>
</tr>
</tbody>
</table>
<!--chapter:end:index.Rmd-->
</div>
</div>
<div id="lecture-1-january-08-2024" class="section level1 hasAnchor" number="2">
<h1 class="hasAnchor"><span class="header-section-number">2</span> Lecture 1, January 08, 2024<a href="#lecture-1-january-08-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this lecture, we went over</p>
<ol style="list-style-type: decimal">
<li>Course syllabus and rules</li>
<li>Chapter 1 – Basic definition of probability. We also saw the potential ambiguities when defining probabilities.</li>
</ol>
<hr />
<div class="definition">
<p><span id="def:unlabeled-div-1" class="definition"><strong>(#def:unlabeled-div-1) (Classical Definition of probability) </strong></span>The <strong>classical</strong> definition: The probability of some event is
<span class="math display">\[
\frac{\mathrm{number~of~ways~the~event~can~occur~}}
{\mathrm{{the~total~number~of~possible~outcomes}}},
\]</span>
provided all outcomes are <em>equally likely</em>.</p>
</div>
<hr />
<div class="definition">
<p><span id="def:unlabeled-div-2" class="definition"><strong>(#def:unlabeled-div-2) (Relative Frequency Definition of of probability) </strong></span>The <strong>relative frequency</strong> definition: The probability of an event
is the (limiting) proportion (or fraction) of times the event occurs in a very
long series of repetitions of an experiment.</p>
</div>
<hr />
<div class="definition">
<p><span id="def:unlabeled-div-3" class="definition"><strong>(#def:unlabeled-div-3) (Subjective Definition of Probability) </strong></span>The <strong>subjective</strong> definition: The probability of an event is a measure of how sure the person making the statement is that the event will happen.</p>
</div>
<hr />
<p><strong>Problem</strong>: Each of the above definitions has pitfall:</p>
<ul>
<li>Classical: We may not be able to know the total number of possible outcomes, or it may be uncountable</li>
<li>Relative frequency: We need “repetition”, which is often expensive and may not be possible.</li>
<li>Subjective: We want the probability to be consistent across different people, and and be rigorously defined.</li>
</ul>
<!--chapter:end:01-Lec1.Rmd-->
</div>
<div id="lecture-2-january-10-2024" class="section level1 hasAnchor" number="3">
<h1 class="hasAnchor"><span class="header-section-number">3</span> Lecture 2, January 10, 2024<a href="#lecture-2-january-10-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this lecture, we went over some basic definitions from the set theory, and using them as the building block for the rest of the course. We started Chapter 2 today, with many definitions.</p>
<p>As for the set operations, <span class="math inline">\(\cup,\cap,A^c,...\)</span>, the Venn diagrams help to visual the meaning behind. Here is a good reference <a href="https://www.edrawmax.com/article/venn-diagram-symbols-and-set-notations.html">HERE</a>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-4" class="definition"><strong>(#def:unlabeled-div-4) (sample space) </strong></span>A <strong>sample space</strong> <span class="math inline">\(S\)</span> is a <em>set</em> of distinct outcomes of an experiment with the property that in a single trial of the experiment only one of these outcomes occurs.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-5" class="definition"><strong>(#def:unlabeled-div-5) (Discrete and non-discrete sample space) </strong></span>A sample space <span class="math inline">\(S\)</span> is said to be <strong>discrete</strong> if it is finite, or ``countably infinite” (i.e.,there is a one-to-one correspondence with the natural numbers). Otherwise a sample space is said to be <strong>non-discrete</strong>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-6" class="definition"><strong>(#def:unlabeled-div-6) (Event) </strong></span>An <strong>event</strong> is a subset of the sample space that can be assigned probability.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-7" class="definition"><strong>(#def:unlabeled-div-7) (Simple and Compound event) </strong></span>Let <span class="math inline">\(S\)</span> be discrete and <span class="math inline">\(A\subset S\)</span> an event. If <span class="math inline">\(A\)</span> is indivisible so it contains only one point, we call it a <strong>simple event</strong>, otherwise <strong>compound event</strong>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-8" class="definition"><strong>(#def:unlabeled-div-8) (Probability distribution) </strong></span>Let <span class="math inline">\(S=\{a_1,a_2,\dots\}\)</span> be discrete. Assign numbers <span class="math inline">\(P(\{a_i\})\)</span> (or short: <span class="math inline">\(P(a_i)\)</span>), <span class="math inline">\(i=1,2,\dots\)</span>, so that</p>
<p>1.<span class="math inline">\(0\leq P(a_i)\leq 1,\quad i=1,2,\dots\)</span>
2. <span class="math inline">\(\sum_{\text{all }i}P(a_i)=1\)</span>.</p>
<p>We then call the set of probabilities <span class="math inline">\(\{P(a_i):i=1,2,\dots\}\)</span> a <strong>probability distribution</strong>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-9" class="definition"><strong>(#def:unlabeled-div-9) </strong></span>Let <span class="math inline">\(S=\{a_1,a_2,\dots\}\)</span> discrete. From any prob. distribution <span class="math inline">\(P\)</span> on <span class="math inline">\(S\)</span> we can define a prob. measure on $ {S} = 2^S$ (set of all subsets of <span class="math inline">\(S\)</span>) by
<span class="math display">\[\forall A \subseteq S \qquad P(A)=\sum_{a_i\in A}P(a_i).\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-10" class="definition"><strong>(#def:unlabeled-div-10) (Equally likely) </strong></span>We say a sample space <span class="math inline">\(S\)</span> with a finite number of outcomes is <strong>equally likely</strong> if the probability of every individual outcome in <span class="math inline">\(S\)</span> is the same.</p>
</div>
<p>Observe that</p>
<ul>
<li><p>If <span class="math inline">\(|A|\)</span> denote the number of outcomes in an event <span class="math inline">\(A\)</span>. In case of an equally likely sample space,
<span class="math display">\[
1=P(S)=\sum_{i=1}^{|S|}P(a_{i})= P(a_i)|S|.
\]</span>
<span class="math display">\[
P(a_i)=\frac{1}{|S|}.
\]</span></p></li>
<li><p>Hence,
<span class="math display">\[P(A) = \sum_{i:\;a_i \in A} P(a_i) = \sum_{i:\;a_i \in A} \frac{1}{|S|} =|A|\cdot  \frac{1}{|S|}\]</span></p></li>
</ul>
<p><strong>Conclusion</strong>: In a <strong>finite, equally likely sample space</strong>, the probability of an event <span class="math inline">\(A\)</span> can be computed as
<span class="math display">\[
P(A) = \sum_{i:\;a_i \in A} P(a_i) = \frac{|A|}{|S|}.
\]</span></p>
<div id="questions-from-the-class" class="section level2 hasAnchor" number="3.1">
<h2 class="hasAnchor"><span class="header-section-number">3.1</span> Questions from the class<a href="#questions-from-the-class" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>What is the difference between “countably infinite” v.s. “infinite”?</li>
</ol>
<p>Ans: A set is <em>countably infinite</em> if its elements can be put in one-to-one correspondence with the set of natural numbers <span class="math inline">\(\mathbb{N}\)</span>. Alternatively, you can think that a set is countably infinite if you can count off all elements in the set in such a way that, even though the counting will take forever, you will get to any particular element in a finite amount of time. [<a href="https://mathinsight.org/definition/countably_infinite">A good reference page to read</a>]. If a set is not countable or countably infinite, it is infinite.</p>
<ol start="2" style="list-style-type: decimal">
<li>Why do we have something such as <span class="math inline">\(2^\mathcal{S}\)</span> in the lecture?</li>
</ol>
<p>Ans: It is related to something called the <em>power set</em>. The power set consists all the possible subset of a set <span class="math inline">\(\mathcal{S}\)</span>. In a subset of <span class="math inline">\(S\)</span>, (i.e. <span class="math inline">\(A \subseteq \mathcal{S}\)</span>, every element in <span class="math inline">\(\mathcal{S}\)</span> may be either in <span class="math inline">\(A\)</span> or not in <span class="math inline">\(A\)</span>. Which means, each element has two possibilities, in <span class="math inline">\(A\)</span> or not in <span class="math inline">\(A\)</span>. Hence, the cardinality (i.e. the size) of the power set is <span class="math inline">\(2^\mathcal{S}\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li>What did we mean by “order does not matter” and “order matters” during the lecture.</li>
</ol>
<ul>
<li><p>Order does not matter: I said when you write out the element of a set, the order does not matter. For instance, in the rolling a six-sided dice, which side would be faced on the top example, we can write <span class="math inline">\(\mathcal{S} = \{1,2,3,4,5,6\}\)</span>, or <span class="math inline">\(\mathcal{S}^\prime=\{6,5,4,3,2,1\}\)</span>, and those two sets are essentially equal to each other. To represent a set, the order does not matter, but we tend to write in a way that is intuitive and easy to understand.</p></li>
<li><p>Order does matter: In rolling two dices example, the dots show on each of the dice is an <em>ordered pair</em>, denoted by <span class="math inline">\((x,y)\)</span>. Hence, for instance, <span class="math inline">\((1,2)\)</span> and <span class="math inline">\((2,1)\)</span> are different. It is problem-dependent so be careful.</p></li>
</ul>
<!--chapter:end:02-Lec2.Rmd-->
</div>
</div>
<div id="lecture-3-january-12-2024" class="section level1 hasAnchor" number="4">
<h1 class="hasAnchor"><span class="header-section-number">4</span> Lecture 3, January 12, 2024<a href="#lecture-3-january-12-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="definition">
<p><span id="def:unlabeled-div-11" class="definition"><strong>(#def:unlabeled-div-11) (Odds) </strong></span>Odds <strong>in favour</strong> of an event <span class="math inline">\(A\)</span> occurring is
<span class="math display">\[
  O(A) := \frac{P(A)}{1-P(A)}.
\]</span>
Odds again an event <span class="math inline">\(A\)</span> is
<span class="math display">\[
  \frac{1-P(A)}{P(A)}.
\]</span></p>
</div>
<p>The range of the odds is <span class="math inline">\([0,\infty)\)</span>.</p>
<p>It provide a measure of the likelihood of a particular outcome to happen.</p>
<p>Abbreviation: ““p:q”.</p>
<p>Note: Probability may be defined through the odds as follow.
<span class="math display">\[\begin{align*}
  &amp;O(A) := \frac{P(A)}{1-P(A)} \\
  &amp;\implies O(A) - P(A)O(A) = P(A) \\
  &amp;\implies O(A) = P(A) (1+O(A)) \\
  &amp;\implies P(A) = \frac{O(A)} {1+O(A)}
\end{align*}\]</span></p>
<p>Note: In finite, equally likely sample spaces, computing probabilities amounts to <em>counting the number of elements in a set</em>. It will often be difficult to do this manually, so we are looking for clever <em>counting techniques</em> in the next chapter.</p>
<hr />
<p><strong>Chapter 3 Counting Techniques</strong></p>
<p>Addition rule v.s. Multiplication rule</p>
<p>For addition rule</p>
<ul>
<li>Keyword for addition rule is “<strong>OR</strong>”;</li>
<li><span class="math inline">\(|A|\)</span> is defined to be the size of the set, aka the cardinality of the set.</li>
<li>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>disjoint</em> (i.e. <span class="math inline">\(A\cap B = \emptyset\)</span>), then <span class="math inline">\(|A\cup B| = |A| + |B|\)</span>.</li>
<li><span class="math inline">\(A\cup A^c = S\)</span> where <span class="math inline">\(A \cap A^c = \emptyset\)</span>. Thus <span class="math inline">\(|S|=|A|+|A^c|\)</span>.</li>
</ul>
<p>For multiplication rule</p>
<ul>
<li>for multiplication rule is “<strong>AND</strong>”</li>
<li>An ordered k-tuple is an ordered set of <span class="math inline">\(k\)</span> values: <span class="math inline">\((a_1,a_2,\dots,a_k)\)</span>. If the outcomes in A can be wrttien as an ordered k-tuple where there are <span class="math inline">\(n_1\)</span> choices for <span class="math inline">\(a_1\)</span>, <span class="math inline">\(n_2\)</span> choices for <span class="math inline">\(a_2,\dots\)</span> and in general <span class="math inline">\(n_i\)</span> choices for <span class="math inline">\(a_i\)</span>, then
<span class="math display">\[
|A| = n_1n_2\cdots n_k = \prod_{i=1}^k n_i.
  \]</span></li>
</ul>
<hr />
<div class="definition">
<p><span id="def:unlabeled-div-12" class="definition"><strong>(#def:unlabeled-div-12) (Factorial) </strong></span>Given <span class="math inline">\(n\)</span> distinct objects, there are
<span class="math display">\[
n! = n \times (n-1) \times \ldots 2 \times 1,
\]</span>
different ordered arrangements of length <span class="math inline">\(n\)</span> that can be made. Note that, we define, <span class="math inline">\(0! = 1\)</span>.</p>
</div>
<ul>
<li>We pronounce <span class="math inline">\(n!\)</span> as “n factorial”.</li>
<li>The following recursive definition is useful:
<span class="math display">\[
n! = n \cdot (n-1)!
\]</span>
When working with factorials, we can often cancel terms, e.g.,
<span class="math display">\[ \frac{9!}{7!} = \frac{9\cdot 8 \cdot 7\cdot 6 \cdot \dots \cdot 2 \cdot 1}{7\cdot 6 \cdot \dots \cdot 2 \cdot 1}=9\cdot 8 = 72\]</span></li>
</ul>
<hr />
<div class="definition">
<p><span id="def:unlabeled-div-13" class="definition"><strong>(#def:unlabeled-div-13) (Permutation) </strong></span>Given <span class="math inline">\(n\)</span> distinct objects, a <strong>permutation</strong> of size <span class="math inline">\(k\)</span> is an <span class="math inline">\(ordered\)</span> subset of <span class="math inline">\(k\)</span> of the individuals. The number of permutations of size <span class="math inline">\(k\)</span> taken from <span class="math inline">\(n\)</span> objects is denoted <span class="math inline">\(n^{(k)}\)</span> and
<span class="math display">\[
n^{(k)}=n(n-1)\dots (n-k+1) =\frac{n!}{(n-k)!}.
\]</span></p>
</div>
<p>The tricky part of this definition is the word “ordered”. An ordering need not be numerical, for example assigning labels like “President” and “Vice-President” has the effect of ordering the individuals.</p>
<div id="questions-from-the-class-1" class="section level2 hasAnchor" number="4.1">
<h2 class="hasAnchor"><span class="header-section-number">4.1</span> Questions from the class<a href="#questions-from-the-class-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li>Can we express the odds as <span class="math inline">\(a:b\)</span>?</li>
</ol>
<p>YES. For instance, the example we saw in class (or the clicker question 1), if we roll a fair six sided dice, and let our event <span class="math inline">\(A:=\{\text{# is } 5 \}\)</span>. Then the odds <span class="math inline">\(O(A)=\frac{1/6}{1/5}=\frac{1}{5}\)</span>. We can see that, there is exactly one possibility we have event <span class="math inline">\(A\)</span>, whereas there are 5 possibilities that <span class="math inline">\(A\)</span> does not happen (i.e. the number we roll out is <span class="math inline">\(1,2,3,4,6\)</span>). We <strong>can</strong> abbreviate it as “1:5”. For a good example of Odds, <a href="https://en.wikipedia.org/wiki/Odds">WIKI</a> provides a good one.</p>
<!--chapter:end:03-Lec3.Rmd-->
</div>
</div>
<div id="lecture-4-january-15-2024" class="section level1 hasAnchor" number="5">
<h1 class="hasAnchor"><span class="header-section-number">5</span> Lecture 4, January 15, 2024<a href="#lecture-4-january-15-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="definition">
<p><span id="def:unlabeled-div-14" class="definition"><strong>(#def:unlabeled-div-14) (Combination) </strong></span>Given <span class="math inline">\(n\)</span> distinct objects, a <em>combination</em> of size <span class="math inline">\(k\)</span> is an <em>unordered</em> subset of <span class="math inline">\(k\)</span> of the individuals. The number of combinations of size <span class="math inline">\(k\)</span> taken from <span class="math inline">\(n\)</span> objects is denoted <span class="math inline">\({n \choose k}\)</span> or <span class="math inline">\({}_n C_k\)</span> and can be computed as
<span class="math display">\[
{n \choose k}=\frac{n^{(k)}}{k!}=\frac{n!}{(n-k)!\ k!}.
\]</span></p>
</div>
<!--chapter:end:04-Lec4.Rmd-->
</div>
<div id="lecture-5-january-17-2024" class="section level1 hasAnchor" number="6">
<h1 class="hasAnchor"><span class="header-section-number">6</span> Lecture 5, January 17, 2024<a href="#lecture-5-january-17-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Properties of the Binomial coefficients</p>
<p>There are some useful/important results about permutation and combination.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(n^{(k)} = n (n - 1)^{(k-1)}\)</span> for <span class="math inline">\(k \geq 1\)</span></li>
<li><span class="math inline">\({n \choose k} = \frac{n^{(k)}}{k!}\)</span></li>
<li><span class="math inline">\({n \choose k} = {n \choose n-k}\)</span> for <span class="math inline">\(k \geq 0\)</span></li>
<li><span class="math inline">\({n \choose k} = {n-1 \choose k-1} + {n-1 \choose k}\)</span></li>
<li>Binomial theorem: <span class="math inline">\((1 + x)^n = \sum_{k=0}^n {n \choose k} x^k\)</span></li>
<li><span class="math inline">\({n \choose k}\)</span> is equal to the <span class="math inline">\(k\)</span>th entry in the <span class="math inline">\(n\)</span>th row of <strong>Pascal’s triangle</strong>.</li>
</ol>
<p>Note: Many of these idenetity may be proven using something called <em>combinatorial proof</em>. See <a href="https://en.wikipedia.org/wiki/Combinatorial_proof">Wiki</a> for an (easy) example.</p>
<div class="proof">
<p><span id="unlabeled-div-15" class="proof"><em>Proof</em> (4). </span><span class="math display">\[\begin{align*}
{n-1 \choose k-1} + {n-1 \choose k} &amp;= \frac{(n-1)!}{(k-1)! (n-k)!} + \frac{(n-1)!}{k! (n-k-1)!}\\
&amp;= \frac{(n-1)!k }{(k-1)! (n-k)!k} + \frac{(n-1)!(n-k)}{k! (n-k-1)!(n-1)} \\
&amp;= \frac{(n-1)!k + (n-1)! (n-k)}{k! (n-k)!} \\
&amp;- \frac{(n-1)!( k + (n-k))}{k! (n-k)!} \\
&amp;= \frac{(n-1)! n}{k! (n-k)!} \\
&amp;= {n \choose k}
\end{align*}\]</span></p>
</div>
<hr />
<p>Aside: Stirling’s formula</p>
<p><span class="math inline">\(n!\)</span> grows really fast as <span class="math inline">\(n\)</span> increases, so sometimes we need to approximate its value for computational reasons.</p>
<p><strong>Stirling’s formula</strong> provides one such method, and it is given by</p>
<p><span class="math display">\[
n! \sim \sqrt{2 \pi n} \left( \frac{n}{e} \right)^n,
\]</span>
where <span class="math inline">\(\sim\)</span> means their ratio approaches 1 as <span class="math inline">\(n\)</span> goes to infinity.</p>
<p>We won’t need this approximation, but it’s useful to know it exists.</p>
<p>Example of use:
Show that <span class="math inline">\(2^{-2n}\binom{2n}{n} \approx \sqrt{\frac{2}{\pi n}}\)</span></p>
<hr />
<div id="example-in-class" class="section level2 hasAnchor" number="6.1">
<h2 class="hasAnchor"><span class="header-section-number">6.1</span> Example in class<a href="#example-in-class" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="example">
<p><span id="exm:unlabeled-div-16" class="example"><strong>(#exm:unlabeled-div-16) (Application of Stirling's Formula/Approximation for factorial) </strong></span>Show that <span class="math inline">\(2^{-2n}{2n \choose n} \approx \sqrt{\frac{1}{\pi n}}\)</span></p>
<p><span class="math display">\[\begin{align*}
  2^{-2n}{2n \choose n} &amp;= 2^{-2n}\frac{2n!}{n!n!} \\
  &amp;\approx 2^{-2n} \frac{\sqrt{2\pi (2n)} (2n/e)^{2n}}{\sqrt{2\pi n} (n/e)^{n}\sqrt{2\pi n} (n/e)^{n}}\\
  &amp;= 2^{-2n} \frac{\sqrt{4}}{\sqrt{2}\sqrt{2}} \frac{\sqrt{\pi n}}{\sqrt{\pi n}\sqrt{\pi n}} \frac{(2n)^{2n}}{n^n n^n} \frac{e^{-2n}}{e^{-2n}}\\
  &amp;=  2^{-2n} \frac{1}{\sqrt{\pi n}} 2^{2n}\\
  &amp;= \frac{1}{\sqrt{\pi n}} = \sqrt{\frac{1}{\pi n}}
\end{align*}\]</span></p>
</div>
<!--chapter:end:05-Lec5.Rmd-->
</div>
</div>
<div id="lecture-6-january-19-2024" class="section level1 hasAnchor" number="7">
<h1 class="hasAnchor"><span class="header-section-number">7</span> Lecture 6, January 19, 2024<a href="#lecture-6-january-19-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="multinomial-coefficient" class="section level3 hasAnchor" number="7.0.1">
<h3 class="hasAnchor"><span class="header-section-number">7.0.1</span> Multinomial Coefficient<a href="#multinomial-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-17" class="definition"><strong>(#def:unlabeled-div-17) (Multinomial Coefficient) </strong></span>Consider <span class="math inline">\(n\)</span> objects which consist of <span class="math inline">\(k\)</span> types. Suppose that there are <span class="math inline">\(n_1\)</span> objects which are of type 1, <span class="math inline">\(n_2\)</span> which are of type 2, and in general <span class="math inline">\(n_i\)</span> objects of type <span class="math inline">\(i\)</span>. Then there are
<span class="math display">\[
\frac{n!}{n_1 ! n_2! \dots n_k !}
\]</span>
distinguishable arrangements of the <span class="math inline">\(n\)</span> objects. This quantity is known as a <strong>multinomial coefficient</strong> and denoted by
<span class="math display">\[
\binom{n}{n_1,n_2,\dots,n_k}= \frac{n !}{n_1 ! n_2! \dots n_k !}.
\]</span></p>
</div>
<p><strong>Note</strong>: Multinomial coefficient is an extension of the <em>binomial coefficient</em>. In binomial coefficient, there are only <strong>two groups/objects</strong>, and the first type has size <span class="math inline">\(n_1\)</span> and the size of the second type is consequently <span class="math inline">\(n-n_1\)</span>, where <span class="math inline">\(n\)</span> is the total number of objects. Hence we have <span class="math inline">\({n \choose n_1} = \frac{n!}{n_1! (n-n_1)!}\)</span>. Try to compare this with the multinomial coefficient.</p>
<hr />
</div>
<div id="the-birthday-problem" class="section level3 hasAnchor" number="7.0.2">
<h3 class="hasAnchor"><span class="header-section-number">7.0.2</span> The Birthday Problem<a href="#the-birthday-problem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose a room contains <span class="math inline">\(n\)</span> people. What is the probability at least two people in the room share a birthday?</p>
<p><strong>Assumption</strong>: Suppose that each of the <span class="math inline">\(n\)</span> people is equally likely to have any of the 365 days of the year as their birthday, so that all possible combinations of birthdays are equally likely.</p>
<p>Let <span class="math inline">\(A\)</span> be the event that at least two people share a birthday. Then
<span class="math display">\[ P(A) = 1 - P(A^c),\]</span>
where <span class="math inline">\(A^c\)</span> is the event that nobody shares birthday with each other.</p>
<p>For <span class="math inline">\(n\)</span> people to have unique birthdays, we need to arrange them among 365 days w/o replacement. Thus,
<span class="math display">\[|A^c| = 365^{(n)}.\]</span></p>
<p>For the size of the sample space, we see that each person has 365 possibilities for their birthday. Thus,
<span class="math display">\[|S| = 365^n.\]</span></p>
<p>Since we are assuming that all possible combinations of birthdays are equally likely, our desired probability becomes
<span class="math display">\[
P(A) = 1 - P(A^c) = 1 - \frac{365^{(n)}}{365^n} = 1 - \frac{n! {365 \choose n}}{365^n}.
\]</span></p>
<p>For <span class="math inline">\(n\in\{100, 30, 23\}\)</span> we find
<span class="math display">\[P(A_{100})= .9999997,\;\;\; P(A_{30})=.7063 \;\;\;\; P(A_{23})=.5073.\]</span></p>
<hr />
</div>
<div id="chapter-4-probbility-rules-and-conditional-probability" class="section level3 hasAnchor" number="7.0.3">
<h3 class="hasAnchor"><span class="header-section-number">7.0.3</span> Chapter 4 Probbility Rules and Conditional Probability<a href="#chapter-4-probbility-rules-and-conditional-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Review the Venn Diagram</p>
<!--chapter:end:06-Lec6.Rmd-->
</div>
</div>
<div id="lecture-7-january-22-2024" class="section level1 hasAnchor" number="8">
<h1 class="hasAnchor"><span class="header-section-number">8</span> Lecture 7, January 22, 2024<a href="#lecture-7-january-22-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="some-terminology-about-the-set-thoery." class="section level3 hasAnchor" number="8.0.1">
<h3 class="hasAnchor"><span class="header-section-number">8.0.1</span> Some terminology about the set thoery.<a href="#some-terminology-about-the-set-thoery." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="fundamental-law-of-set-algebra" class="section level4 hasAnchor" number="8.0.1.1">
<h4 class="hasAnchor"><span class="header-section-number">8.0.1.1</span> Fundamental law of set algebra<a href="#fundamental-law-of-set-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be any arbitrary sets/events.</p>
<ol style="list-style-type: decimal">
<li>Commutative</li>
</ol>
<p><span class="math display">\[
  A\cup B = B\cup A \quad \text{ and } A\cap B = B\cap A.
\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>Associativity</li>
</ol>
<p><span class="math display">\[
  (A\cup B)\cup C = A \cup (B\cup C), \quad \text{and } (A\cap B)\cap C =  A \cap (B \cap C).
\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Distributive Law</li>
</ol>
<p><span class="math display">\[
  A\cup (B\cap C) = (A \cup B) \cap (A \cap  C) , \quad \text{ and } A \cap (B\cup C) =  (A\cap B) \cup (A\cap C)
\]</span></p>
</div>
<div id="demorgans-laws" class="section level4 hasAnchor" number="8.0.1.2">
<h4 class="hasAnchor"><span class="header-section-number">8.0.1.2</span> DeMorgan’s Laws<a href="#demorgans-laws" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\((A\cup B)^c = A^c \cap B^c\)</span> (Complement of an union is the intersection of the complements)</p></li>
<li><p><span class="math inline">\(A\cap B)^c = A^c \cup B^c\)</span> (Complement of an intersection is the union of the complements)</p></li>
</ol>
</div>
<div id="inclusion-exclusion-principle" class="section level4 hasAnchor" number="8.0.1.3">
<h4 class="hasAnchor"><span class="header-section-number">8.0.1.3</span> Inclusion Exclusion Principle<a href="#inclusion-exclusion-principle" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P(A\cup B ) = P(A) + P(B) - P(A\cap B)\)</span></p></li>
<li><p><span class="math inline">\(P(A\cup B \cup C) = P(A) + P(B) + P(C) - P(A\cap B) - P(A \cap C) - P(B \cap C) + P(A\cap B \ cap C)\)</span></p></li>
<li><p>Note that we can generalized the (2), and obtain the following by <em>inducation.</em> For arbitrary events <span class="math inline">\(A_1,A_2,\cdots,A_n,\quad n\ge 2\)</span>,
<span class="math display">\[\begin{align*}
P(\bigcup_{i=1}^n A_i)  &amp;  =\sum_{i}P(A_{i}%
)-\sum_{i&lt;j}P(A_{i}A_{j})+\sum_{i&lt;j&lt;k}P(A_{i}A_{j}A_{k})\\
&amp;  -\sum_{i&lt;j&lt;k&lt;l}P(A_{i}A_{j}A_{k}A_{l})+\cdots
\end{align*}\]</span></p></li>
</ol>
</div>
</div>
<div id="independence" class="section level3 hasAnchor" number="8.0.2">
<h3 class="hasAnchor"><span class="header-section-number">8.0.2</span> Independence<a href="#independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-18" class="definition"><strong>(#def:unlabeled-div-18) (independence) </strong></span>Any two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <strong>independent</strong> if
<span class="math display">\[
  P(A \cap B) = P(A)\times P(B).
\]</span></p>
</div>
<p>Note: Intuitively, it means that two events do not have any influence of each other. You will see that how this concept plays an important role in statistics, in particular through something called the <em>covariance</em>, which is beyond this course so do not worry about this for now.</p>
</div>
<div id="independence-v.s.-multually-exclusivedisjoint" class="section level3 hasAnchor" number="8.0.3">
<h3 class="hasAnchor"><span class="header-section-number">8.0.3</span> Independence v.s. Multually Exclusive/Disjoint<a href="#independence-v.s.-multually-exclusivedisjoint" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall the definition of mutually exclusive</p>
<div class="definition">
<p><span id="def:unlabeled-div-19" class="definition"><strong>(#def:unlabeled-div-19) (Mutually Exclusive) </strong></span>Any two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are said to be <strong>mutually exclusive</strong> or <strong>disjoint</strong> if
<span class="math display">\[
  P(A \cap B) = 0.
\]</span></p>
</div>
<p>Note:</p>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are mutually exclusive, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> may NOT be independent!</p></li>
<li><p><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> CAN only be mutually exclusive and independent when either <span class="math inline">\(A\)</span>, <span class="math inline">\(A\)</span>, or both are the empty set <span class="math inline">\(\emptyset\)</span>.</p></li>
</ol>
<div class="lemma">
<p><span id="lem:unlabeled-div-20" class="lemma"><strong>(#lem:unlabeled-div-20) </strong></span>Let two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> such that NOT both events are trivial events (empty set). If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent and mutually exclusive/disjoint, then either <span class="math inline">\(P(A) = 0\)</span> or <span class="math inline">\(P(B) = 0\)</span>.</p>
</div>
<!--chapter:end:07-Lec7.Rmd-->
</div>
</div>
<div id="lecture-8-january-24-2024" class="section level1 hasAnchor" number="9">
<h1 class="hasAnchor"><span class="header-section-number">9</span> Lecture 8, January 24, 2024<a href="#lecture-8-january-24-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="definition">
<p><span id="def:unlabeled-div-21" class="definition"><strong>(#def:unlabeled-div-21) (Conditional Probability) </strong></span>The conditional probability of an event <span class="math inline">\(A\)</span> given an event <span class="math inline">\(B\)</span>, assuming <span class="math inline">\(P(B)&gt;0\)</span>, is</p>
<p><span class="math display">\[
  P(A \mid B) = \frac{P(A\cap B)}{P(B)}.
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-22" class="definition"><strong>(#def:unlabeled-div-22) (Equivalent definition of independence) </strong></span>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, if
<span class="math display">\[
P(A|B)=P(A),
\]</span>
provided <span class="math inline">\(P(B)&gt;0\)</span>.</p>
</div>
<div id="properties-of-conditional-probability" class="section level3 hasAnchor" number="9.0.1">
<h3 class="hasAnchor"><span class="header-section-number">9.0.1</span> Properties of Conditional Probability<a href="#properties-of-conditional-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(0 \le P(A \mid B) \le 1\)</span>.</li>
</ol>
<p>This follows from the fact that if <span class="math inline">\(A \subset B\)</span> then <span class="math inline">\(P(A) \le P(B)\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p><span class="math inline">\(P(A^c \mid B) = 1-P(A \mid B)\)</span>.</p></li>
<li><p>If <span class="math inline">\(A_1\)</span> and <span class="math inline">\(A_2\)</span> are disjoint (i.e. <span class="math inline">\(P(A_1\cap A_2)=\emptyset\)</span>: <span class="math inline">\(P(A_1 \cup A_2 \mid B) = P(A_1 \mid B) + P(A_2 \mid B)\)</span>.</p></li>
<li><p><span class="math inline">\(P(S \mid B)= 1 = P(B \mid B)\)</span>.</p></li>
</ol>
<div class="definition">
<p><span id="def:unlabeled-div-23" class="definition"><strong>(#def:unlabeled-div-23) (Product rule) </strong></span>For any events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, we have
<span class="math display">\[
P(A\cap B) = P(A\mid B) P(B)  = P(B \mid A) P(A).
\]</span></p>
</div>
<!--chapter:end:08-Lec8.Rmd-->
</div>
</div>
<div id="lecture-9-january-26-2024" class="section level1 hasAnchor" number="10">
<h1 class="hasAnchor"><span class="header-section-number">10</span> Lecture 9, January 26, 2024<a href="#lecture-9-january-26-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="law-of-total-probability" class="section level3 hasAnchor" number="10.0.1">
<h3 class="hasAnchor"><span class="header-section-number">10.0.1</span> Law of Total Probability<a href="#law-of-total-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-24" class="definition"><strong>(#def:unlabeled-div-24) (Partition) </strong></span>A sequence of sets <span class="math inline">\(B_1,B_2,...,B_k\)</span> are said to <strong>partition</strong> the sample space <span class="math inline">\(S\)</span> if <span class="math inline">\(B_i \cap B_j = \emptyset\)</span> for all <span class="math inline">\(i \ne j\)</span>, and <span class="math inline">\(\cup_{j=1}^k B_j = S\)</span>.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-25" class="theorem"><strong>(#thm:unlabeled-div-25) (Law of Total Probability) </strong></span>Suppose that <span class="math inline">\(B_1,B_2,...,B_k\)</span> partition <span class="math inline">\(S\)</span>. Then for any event <span class="math inline">\(A\)</span>,
<span class="math display">\[
P(A) = P(A | B_1) P(B_1) + P(A | B_2) P(B_2) + \cdots +P(A | B_k) P(B_k).
\]</span></p>
</div>
<p>Note: It is a simple usage of LTP such that
<span class="math display">\[
  P(A) = P(A\cap B) + P(A \cap B^c),
\]</span>
since <span class="math inline">\(B\)</span> and <span class="math inline">\(B^c\)</span> partition <span class="math inline">\(S\)</span> (i.e. <span class="math inline">\(B\cup B^c = S\)</span> and <span class="math inline">\(B\cap B^c = \emptyset\)</span>)</p>
<hr />
</div>
<div id="bayes-rule" class="section level3 hasAnchor" number="10.0.2">
<h3 class="hasAnchor"><span class="header-section-number">10.0.2</span> Bayes Rule<a href="#bayes-rule" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If we can calculate the conditional profitability, then we may calculate the desire probability by using 1) LTP, 2) definition of conditional probability, and 3) property of the sets (through the Venn diagrams). However, sometimes we have to <em>FLIP</em> the event and the conditioning event. This brings us to the <strong>Bayes Theorem</strong>.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-26" class="theorem"><strong>(#thm:unlabeled-div-26) (Bayes Theorem) </strong></span>Suppose that <span class="math inline">\(B_1,B_2,...,B_k\)</span> partition <span class="math inline">\(S\)</span>. Then for any event <span class="math inline">\(A\)</span>,
<span class="math display">\[
P(B_i \mid A ) =  \frac{P(A \mid B_i)P(B_i)}{\sum_{j=1}^k P(A \mid B_j) P(B_j) }.
\]</span></p>
</div>
<p>This concludes Chapter 4!.</p>
<!--chapter:end:09-Lec9.Rmd-->
</div>
</div>
<div id="lecture-10-january-29-2024" class="section level1 hasAnchor" number="11">
<h1 class="hasAnchor"><span class="header-section-number">11</span> Lecture 10, January 29, 2024<a href="#lecture-10-january-29-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>We begin <strong>Chapter 5</strong> in this lecture!</p>
<div id="chapter-5.-discrete-random-variable" class="section level3 hasAnchor" number="11.0.1">
<h3 class="hasAnchor"><span class="header-section-number">11.0.1</span> Chapter 5. Discrete Random Variable<a href="#chapter-5.-discrete-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-27" class="definition"><strong>(#def:unlabeled-div-27) (Random Variable) </strong></span>A <em>random variable</em> is a function that maps assigns a real number <span class="math inline">\(\mathbb{R}\)</span> to each point in a sample space <span class="math inline">\(S\)</span>. That is, <span class="math inline">\(X\)</span> is a random variable if
<span class="math display">\[X:S\to \mathbb{R}\]</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-28" class="definition"><strong>(#def:unlabeled-div-28) (Range) </strong></span>The values that a random variables takes is called the <em>range</em> of the random variable. We often denote the range of a random variable <span class="math inline">\(X\)</span> by <span class="math inline">\(X(S)\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-29" class="definition"><strong>(#def:unlabeled-div-29) (Discrete random variable) </strong></span>The <em>discrete</em> random variables take integer values, or more generally, values in a countable set (i.e. finite or countably infinite set). That is, its range is a discrete/countable subset of <span class="math inline">\(\mathbb{R}\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-30" class="definition"><strong>(#def:unlabeled-div-30) (Continuous random variable) </strong></span>A random variable is <em>continuous</em> if its range is an interval that is a subset of <span class="math inline">\({\mathbb R}\)</span> (e.g. $[0,1], (0,), {R} $).</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-31" class="definition"><strong>(#def:unlabeled-div-31) (Probability (mass) function) </strong></span>The <em>probability (mass) function</em> of a <em>discrete</em> random variable <span class="math inline">\(X\)</span> is the function
<span class="math display">\[
f_X(x) = P(X=x),\quad \text{ for } x\in \mathbb{R},
\]</span>
which is non-zero at at most countably many values.</p>
</div>
<p>Notation: We write <span class="math inline">\(P(X=x)\)</span> as the shorthanded notation for <span class="math inline">\(P(\{\omega \in S : X(\omega)=x\})\)</span>.</p>
<p>Notation: We can write <span class="math inline">\(f_X(x)=P(X^{-1}(x))=(P\circ X^{-1})(x)\)</span>. We call this as <em>push-forward probability measure</em>.</p>
<p>Note: The definition <span class="math inline">\(f_X\)</span> is valid for all <span class="math inline">\(x\)</span>, but the value is zero when <span class="math inline">\(x\)</span> is outside the range of the random variable <span class="math inline">\(X\)</span>. (This is called the null set).</p>
<hr />
<p>Properties of probability mass function <span class="math inline">\(f\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(f_X(x)\in[0,1]\)</span> for all <span class="math inline">\(x\)</span>, and</p></li>
<li><p><span class="math inline">\(\sum_{x\in X(\omega)}f_X(x) =1\)</span>. i.e. sum of the probability on ALL the events equal to <span class="math inline">\(1\)</span>.</p></li>
</ol>
<!--chapter:end:10-Lec10.Rmd-->
</div>
</div>
<div id="lecture-11-january-31-2024" class="section level1 hasAnchor" number="12">
<h1 class="hasAnchor"><span class="header-section-number">12</span> Lecture 11, January 31, 2024<a href="#lecture-11-january-31-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="distinction-of-the-definition-discrete-of-the-sample-space-and-the-random-variable" class="section level2 hasAnchor" number="12.1">
<h2 class="hasAnchor"><span class="header-section-number">12.1</span> Distinction of the definition “discrete” of the sample space and the random variable<a href="#distinction-of-the-definition-discrete-of-the-sample-space-and-the-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall the definition of the <em>Range of a random variable</em> from last lecture:
::: {.definition name=“Range”}
The values that a random variables takes is called the <em>range</em> of the random variable. We often denote the range of a random variable <span class="math inline">\(X\)</span> by <span class="math inline">\(X(S)\)</span>.
:::</p>
<ul>
<li><p>We say that, a <strong>random variable</strong> is discrete if its range <span class="math inline">\(X(\omega)\)</span> is discrete (finite or countable, in another word, we can say it is <em>at most countable</em>).</p></li>
<li><p>We say, a sample space <span class="math inline">\(S\)</span> is discrete if <span class="math inline">\(S\)</span> is finite or countable.</p></li>
</ul>
<p>The sample space <span class="math inline">\(S\)</span> and the range of the random variable are two different things, so do not get confused! We can have a discrete random variable while the sample space <span class="math inline">\(S\)</span> is continuous!</p>
</div>
<div id="cumulative-distribution-function" class="section level2 hasAnchor" number="12.2">
<h2 class="hasAnchor"><span class="header-section-number">12.2</span> Cumulative distribution function<a href="#cumulative-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-32" class="definition"><strong>(#def:unlabeled-div-32) (Range) </strong></span>The <strong>cumulative distribution function</strong> (cdf) of a random variable <span class="math inline">\(X\)</span> is
<span class="math display">\[
F_X(x) = P(X \le x),\;\; x \in {\mathbb{R}}.
\]</span></p>
</div>
<p>Note: The cumulative distribution function <span class="math inline">\(F_X\)</span> is <strong>always defined</strong> over the entire real line <span class="math inline">\(\mathbb{R}\)</span>, while the probability function may not always be defined! Hence, the cumulative distribution function is an useful tool! (but do not worry about it now.)</p>
<p>Notation: <span class="math inline">\(F_X(x) = P(X\le x) = P(\{\omega \in S : X(\omega)\in x\}).\)</span></p>
<p>If <span class="math inline">\(X\)</span> is <em>discrete with probability function <span class="math inline">\(f_X\)</span> (i.e. if <span class="math inline">\(f_X\)</span> exists)</em>, then we can calculate the cdf from summing up the pdf as
<span class="math display">\[
F_X(x)=P( X \le x ) = \sum_{y:\; y\le x}f_X(y).
\]</span></p>
<div id="properties-of-the-cumulative-distribution-function" class="section level3 hasAnchor" number="12.2.1">
<h3 class="hasAnchor"><span class="header-section-number">12.2.1</span> Properties of the cumulative distribution function<a href="#properties-of-the-cumulative-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(F_X(\cdot)\)</span> be a cdf. Then the following holds</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(F_X(x)\in [0,1]\)</span></p></li>
<li><p><span class="math inline">\(F_X(x) \le F_x(y)\)</span> whenever <span class="math inline">\(x&lt;y\)</span> (i.e. <span class="math inline">\(F_X(\cdot)\)</span> is a non-increasing function.)</p></li>
<li><p><span class="math inline">\(\lim\limits_{x \to - \infty } F_X(x)=0\)</span>, and <span class="math inline">\(\lim\limits_{x \to \infty } F_X(x) = 1\)</span>.</p></li>
<li><p><span class="math inline">\(F_X\)</span> is <em>right continuous</em>, i.e., <span class="math inline">\(F(x_0)=\lim_{x\downarrow x_0} F(x)\)</span> for all <span class="math inline">\(x_0\in\mathbb{R}\)</span>.</p></li>
</ol>
</div>
</div>
<div id="special-distributions-with-names" class="section level2 hasAnchor" number="12.3">
<h2 class="hasAnchor"><span class="header-section-number">12.3</span> Special distributions with names<a href="#special-distributions-with-names" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="discrete-uniform-distribution" class="section level3 hasAnchor" number="12.3.1">
<h3 class="hasAnchor"><span class="header-section-number">12.3.1</span> Discrete uniform distribution<a href="#discrete-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first named distribution we look at is the <em>discrete uniform distribution</em></p>
<div class="definition">
<p><span id="def:unlabeled-div-33" class="definition"><strong>(#def:unlabeled-div-33) (Discrete uniform distribution) </strong></span>uppose the range of the random variable <span class="math inline">\(X\)</span> is <span class="math inline">\(\{a,a+1,\dots, b\}\)</span>, where <span class="math inline">\(a,b\in\mathbb{Z}\)</span>, and suppose all values are equally likely. Then we say that <span class="math inline">\(X\)</span> has a <strong>discrete uniform distribution</strong> on <span class="math inline">\(\{a,a+1,\dots,b\}\)</span>, shorthand: <span class="math inline">\(X \sim U[a,b]\)</span>.</p>
</div>
<div id="probability-function-and-distribution-function" class="section level4 hasAnchor" number="12.3.1.1">
<h4 class="hasAnchor"><span class="header-section-number">12.3.1.1</span> Probability function and distribution function<a href="#probability-function-and-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If <span class="math inline">\(X \sim U[a,b]\)</span>, then its probability function is given by
<span class="math display">\[
   f_X(x)= P(X = x) = \begin{cases} \frac{1}{b - a + 1}, &amp;\quad\text{ if }x \in\{a,a+1,\dots,b\}, \\ 0, &amp;\quad\text{ otherwise} \end{cases}   \]</span>
and corresponding (cumulative) distribution function is
<span class="math display">\[
   F_X(x)= P(X \leq x) = \begin{cases} 0, &amp;\quad\text{ if } x&lt;a\\
   \frac{\lfloor x\rfloor - a + 1}{b - a + 1}, &amp;\quad\text{ if }x \in\{a,a+1,\dots,b\}, \\
   1, &amp;\quad\text{ if } x\geq b,\end{cases}   \]</span>
where <span class="math inline">\(\lfloor x\rfloor=\max\{z\in\mathbb{Z}: z\leq x\}\)</span> is the <em>floor function</em>.</p>
<!--chapter:end:11-Lec11.Rmd-->
</div>
</div>
</div>
</div>
<div id="lecture-12-feburary-02-2024" class="section level1 hasAnchor" number="13">
<h1 class="hasAnchor"><span class="header-section-number">13</span> Lecture 12, Feburary 02, 2024<a href="#lecture-12-feburary-02-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="hypergeometric-distribution" class="section level2 hasAnchor" number="13.1">
<h2 class="hasAnchor"><span class="header-section-number">13.1</span> Hypergeometric distribution<a href="#hypergeometric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-34" class="definition"><strong>(#def:unlabeled-div-34) (Hypergeometric distribution) </strong></span>Consider a population that consists of <span class="math inline">\(N\)</span> objects, of which <span class="math inline">\(r\)</span> are considered <em>successes</em> and the remaining <span class="math inline">\(N-r\)</span> are considered <em>failures</em>. Suppose that a subset of size <span class="math inline">\(n\)</span> (with <span class="math inline">\(n\leq N\)</span>) is drawn from the population WITHOUT REPLACEMENT. Let <span class="math inline">\(X\)</span>=Number of successes obtained, then we say <span class="math inline">\(X\)</span> follows a <strong>hypergeometric distribution</strong> with parameters <span class="math inline">\((N,r,n)\)</span>. We sometimes write <span class="math inline">\(X \sim hyp(N,r,n)\)</span> or <span class="math inline">\(X\sim HG(N,r,n)\)</span>.</p>
</div>
<div id="range-of-the-hypergeomnetric-distribution" class="section level3 hasAnchor" number="13.1.1">
<h3 class="hasAnchor"><span class="header-section-number">13.1.1</span> Range of the Hypergeomnetric distribution<a href="#range-of-the-hypergeomnetric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>We cannot have more successes than there are (<span class="math inline">\(r\)</span>) <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(x\leq r\)</span></li>
<li>We cannot have more successes than trials (<span class="math inline">\(n\)</span>) <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(x\leq n\)</span></li>
<li>When there are more trials than failures (<span class="math inline">\(n&gt;(N-r)\)</span>) we will for sure have at least <span class="math inline">\(n-(N-r)\)</span> successes <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(x\geq \max\{0, n-(N-r)\}\)</span>.</li>
<li>Overall, we have <strong><span class="math inline">\(\max\{0, n-(N-r)\} \leq x \leq \min\{r,n\}\)</span></strong>.</li>
</ul>
</div>
<div id="probability-function-of-hypergeometric-distribution" class="section level3 hasAnchor" number="13.1.2">
<h3 class="hasAnchor"><span class="header-section-number">13.1.2</span> Probability function of hypergeometric distribution<a href="#probability-function-of-hypergeometric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Total number of of subsets of size <span class="math inline">\(n\)</span>: <span class="math inline">\(\binom{N}{n}\)</span>.</li>
<li>Number of ways to select <span class="math inline">\(x\)</span> successes out of <span class="math inline">\(r\)</span> successes: <span class="math inline">\(\binom{r}{x}\)</span>.</li>
<li>Number of ways to choose remaining <span class="math inline">\(n-x\)</span> failures from <span class="math inline">\(N-r\)</span> failures: <span class="math inline">\(\binom{N-r}{n-x}\)</span>.</li>
<li>Thus,
<span class="math display">\[f(x) = \frac{\binom{r}{x}\binom{N-r}{n-x}}{\binom{N}{n}},\]</span>
where <span class="math inline">\(\max\{0, n-(N-r)\} \leq x \leq \min\{r,n\}\)</span> and 0 otherwise.</li>
</ul>
</div>
</div>
<div id="bernoulli-and-binimial-distributions" class="section level2 hasAnchor" number="13.2">
<h2 class="hasAnchor"><span class="header-section-number">13.2</span> Bernoulli and Binimial distributions<a href="#bernoulli-and-binimial-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-35" class="definition"><strong>(#def:unlabeled-div-35) (Bernoulli trail) </strong></span>A <strong>Bernoulli trial</strong> with probability of success <span class="math inline">\(p\)</span> is an experiment that results in either a success or failure, and the probability of success is <span class="math inline">\(p\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-36" class="definition"><strong>(#def:unlabeled-div-36) (Bernoulli distribution) </strong></span>If a random variable <span class="math inline">\(X\)</span> represents the number of successes in a Bernoulli trial with probability of success <span class="math inline">\(p\)</span>, it follows the <em>Bernoulli distribution</em>, and we denote it as
<span class="math display">\[
X \sim Bernoulli(p).
\]</span></p>
</div>
<div id="probability-function-and-cumulative-distribution-function" class="section level3 hasAnchor" number="13.2.1">
<h3 class="hasAnchor"><span class="header-section-number">13.2.1</span> Probability function and cumulative distribution function<a href="#probability-function-and-cumulative-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="bernoulli" class="section level3 hasAnchor" number="13.2.2">
<h3 class="hasAnchor"><span class="header-section-number">13.2.2</span> Bernoulli<a href="#bernoulli" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(X\sim Bern(p)\)</span>, the pf of <span class="math inline">\(X\)</span> is
<span class="math display">\[ f_X(0)=1-p,\quad f_X(1)=p,\quad f_X(x)=0\,\,\text{ for }x\not\in\{0,1\}\]</span>
and the cdf is
<span class="math display">\[ F_X(x)=\begin{cases} 0, &amp;\text{ if }x&lt;0, \\ 1-p, &amp;\text{ if } 0\leq x &lt; 1,\\ 1, &amp;\text{ if }x\geq 1\end{cases}\]</span></p>
</div>
<div id="binomial" class="section level3 hasAnchor" number="13.2.3">
<h3 class="hasAnchor"><span class="header-section-number">13.2.3</span> Binomial<a href="#binomial" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-37" class="definition"><strong>(#def:unlabeled-div-37) (Bernoulli distribution) </strong></span>If we have <span class="math inline">\(N\)</span> independent runs and record the numbers of successes obtained in these <span class="math inline">\(n\)</span> runs, then <span class="math inline">\(X\)</span> is said to have a binomial distribution, denoted by <span class="math inline">\(X\sim Bin(n,p)\)</span>.</p>
</div>
<p>Note: The probability function of <span class="math inline">\(X\sim Bin(n, p)\)</span> is
<span class="math display">\[ f(x) = \binom{n}{x} p^x(1-p)^{n-x},\quad x=0,1,2,\dots,n\]</span>
and 0 otherwise.</p>
</div>
</div>
<div id="relationship-and-difference-between-binomial-and-hypergeometric" class="section level2 hasAnchor" number="13.3">
<h2 class="hasAnchor"><span class="header-section-number">13.3</span> Relationship and difference between binomial and hypergeometric<a href="#relationship-and-difference-between-binomial-and-hypergeometric" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Binomial and hypergeometric distributions are fundamentally different!</li>
<li>In Binomial models, we pick <strong>WITH</strong> replacement, in the hypergeometric model <strong>WITHOUT</strong> replacement.</li>
<li>If <span class="math inline">\(N\)</span> is large and <span class="math inline">\(n\)</span> is small, the chance we pick the same object twice is small.</li>
<li>Thus, letting <span class="math inline">\(r/N=p\)</span>, <span class="math inline">\(X\sim Hyp(N,r,n)\)</span> and <span class="math inline">\(Y\sim Bin(n,p)\)</span>, then we can <strong>APPROXIMATE</strong>
<span class="math display">\[ P(X \leq k) \approx P(Y\leq k).\]</span>
(more precisely, in the limit as <span class="math inline">\(N\rightarrow\infty\)</span> with <span class="math inline">\(r/N\rightarrow p\)</span> (the ratio of successes converges to the success probability).</li>
<li>See pages 86/87 and later in the course for more.</li>
<li>We’ll see an example next time.</li>
</ul>
<!--chapter:end:12-Lec12.Rmd-->
</div>
</div>
<div id="lecture-13-feburary-05-2024" class="section level1 hasAnchor" number="14">
<h1 class="hasAnchor"><span class="header-section-number">14</span> Lecture 13, Feburary 05, 2024<a href="#lecture-13-feburary-05-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="binomial-v.s.-hypergeometric" class="section level2 hasAnchor" number="14.1">
<h2 class="hasAnchor"><span class="header-section-number">14.1</span> Binomial v.s. hypergeometric<a href="#binomial-v.s.-hypergeometric" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><span class="math inline">\(Bin(n,\frac{r}{N})\)</span> and <span class="math inline">\(hyp(N,r,n)\)</span> are fundamentally different!</li>
<li>If you have an urn with <span class="math inline">\(r\)</span> successes and <span class="math inline">\(N-r\)</span> failures…
<ul>
<li>… and you draw <span class="math inline">\(n\)</span> items, then the number of successes is…
<ul>
<li>… Binomial: drawn replacement, …</li>
<li>… Hypergeometric: drawn replacement.</li>
</ul></li>
</ul></li>
<li><span class="math inline">\(N\gg n \Rightarrow\)</span> chance we pick same object twice w/ replacement small.</li>
<li> For <span class="math inline">\(p=\frac{r}{N}\)</span>, <span class="math inline">\(X\sim Hyp(N,r,n)\)</span> and <span class="math inline">\(Y\sim Bin(n,p)\)</span>
<span class="math display">\[ P(X \leq k) \approx P(Y\leq k).\]</span>
(more precisely, in the limit as <span class="math inline">\(N\rightarrow\infty\)</span> with <span class="math inline">\(r/N\rightarrow p\)</span>).</li>
</ul>
</div>
<div id="negative-binomial-distribution" class="section level2 hasAnchor" number="14.2">
<h2 class="hasAnchor"><span class="header-section-number">14.2</span> Negative binomial distribution<a href="#negative-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-38" class="definition"><strong>(#def:unlabeled-div-38) (Negative Binomial) </strong></span>Consider an experiment with two possible outcomes <em>success</em> (Su) or <em>failure</em> (F). Without loss of generality, assume that the <span class="math inline">\(P(Su)=p\)</span> (so <span class="math inline">\(P(F)=1-P(Su)= 1-p\)</span>). Repeat the experiment <strong>independently</strong> until a specified number of <span class="math inline">\(k\)</span> successes have been observed. Denote <span class="math inline">\(X\)</span> the number of failures before the <span class="math inline">\(k\)</span>-th suceess, then <span class="math inline">\(X\sim NegBin(k,p)\)</span>.</p>
</div>
<div id="probability-function" class="section level3 hasAnchor" number="14.2.1">
<h3 class="hasAnchor"><span class="header-section-number">14.2.1</span> Probability function<a href="#probability-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The probability function of <span class="math inline">\(X\sim NegBin(k,p)\)</span> is
<span class="math display">\[ f(x)=P(X=x)=\binom{x+k-1}{x}p^k(1-p)^x,\quad x=0,1,2,\dots\]</span>
since there are <span class="math inline">\(\binom{x+k-1}{x}\)</span> to choose <span class="math inline">\(x\)</span> positions among the first <span class="math inline">\(x+k-1\)</span> positions to be a failure (and the remaining ones are automatically success), and each of these sequence of outcomes has probability <span class="math inline">\(p^k(1-p)^x\)</span>.</p>
</div>
</div>
<div id="binomial-v.s.-negative-binomial" class="section level2 hasAnchor" number="14.3">
<h2 class="hasAnchor"><span class="header-section-number">14.3</span> Binomial v.s. Negative Binomial<a href="#binomial-v.s.-negative-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>Binomial distribution: We know number of trials <span class="math inline">\(n\)</span>, but we do not know how many successes.</p></li>
<li><p>Negative Binomial distribution: We know the number of successes <span class="math inline">\(k\)</span>, but we do not know how many trials will be needed.</p></li>
</ul>
</div>
<div id="geometric-distribution" class="section level2 hasAnchor" number="14.4">
<h2 class="hasAnchor"><span class="header-section-number">14.4</span> Geometric distribution<a href="#geometric-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-39" class="definition"><strong>(#def:unlabeled-div-39) (Geometric) </strong></span>Consider an experiment with two possible outcomes <em>success</em> (Su) or <em>failure</em> (F). Without loss of generality, assume that the <span class="math inline">\(P(Su)=p\)</span> (so <span class="math inline">\(P(F)=1-P(Su)= 1-p\)</span>). Repeat the experiment <strong>independently</strong> <strong>before the 1st success</strong> has been observed. Denote <span class="math inline">\(X\)</span> the number of failures before 1st success, then <span class="math inline">\(X\sim Geo(p)\)</span>.</p>
</div>
<p>Note: The Geometric distribution is a special case of the negative binomial distribution: <span class="math inline">\(Geo(P) \sim Neg(k=1,p)\)</span>.</p>
<div id="probability-function-and-distribution-function-1" class="section level3 hasAnchor" number="14.4.1">
<h3 class="hasAnchor"><span class="header-section-number">14.4.1</span> Probability function and distribution function<a href="#probability-function-and-distribution-function-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>If <span class="math inline">\(X\sim Geo(p)\)</span>, then <span class="math inline">\(X\)</span> has probability function
<span class="math display">\[ f(x) = P(X=x)= (1-p)^x p,\quad x\in\{0,1,2,\dots\}.\]</span></li>
</ul>
<p><span class="math display">\[ F(x)=P(X\leq x)
= P(X\leq \floor{x}) =
\sum_{k=0}^{\floor{x}} (1-p)^k p =
1-(1-p)^{\floor{x}+1}\]</span>
if <span class="math inline">\(x\geq 0\)</span> and <span class="math inline">\(0\)</span> otherwise.</p>
<ul>
<li>Note that <span class="math inline">\(P(X&gt;x)=1-F(x)=(1-p)^{\floor{x}+1}\)</span> which is nice for computations!</li>
</ul>
</div>
<div id="memoryless-property" class="section level3 hasAnchor" number="14.4.2">
<h3 class="hasAnchor"><span class="header-section-number">14.4.2</span> Memoryless property<a href="#memoryless-property" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theroem" name="Memoryless property of $Geo(p)$">
<p>Let <span class="math inline">\(X \sim Geo(p)\)</span> and <span class="math inline">\(s,t\)</span> be non-negative integers. Then, the following equation holds.
<span class="math display">\[
P(X \geq s+t | X \geq s) = P(X \geq t).
\]</span></p>
</div>
</div>
<div id="aside-reason-to-call-the-negative-binomial-distribution" class="section level3 hasAnchor" number="14.4.3">
<h3 class="hasAnchor"><span class="header-section-number">14.4.3</span> Aside, Reason to call the Negative binomial distribution<a href="#aside-reason-to-call-the-negative-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>(This is contributed by Jeffery!)</p>
<p>Can extend binomial coefficients to <strong>negative or fractional</strong> ``top’’ part.</p>
<p><span class="math display">\[\binom{\theta}{x} := \frac{\theta^{(X)}}{x!} = \frac{\theta(\theta-1)(\theta-1)...(\theta-x+1)}{X!}\]</span></p>
<p>Note: ``bottom’’ part of coefficient still a non-negative integer.</p>
<p>Then
<span class="math display">\[\begin{align*}
    \binom{-k}{x}
        &amp; = \frac{-k(-k-1)...(-k-x+1)}{x!} \\
        &amp; = (-1)^x \frac{(x+k-1)(x+k-2)...((x+k-1)-x+1)}{x!}\\
        &amp; = (-1)^x \binom{x+k-1}{x}
\end{align*}\]</span>
So
<span class="math display">\[
    f_X(x) = \binom{x+k-1}{x}p^k(1-p)^x = (-1)^x \binom{-k}{x} p^k(1-p)^x
\]</span></p>
<!--chapter:end:13-Lec13.Rmd-->
</div>
</div>
</div>
<div id="lecture-14-feburary-07-2024" class="section level1 hasAnchor" number="15">
<h1 class="hasAnchor"><span class="header-section-number">15</span> Lecture 14, Feburary 07, 2024<a href="#lecture-14-feburary-07-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="poisson-distribution" class="section level2 hasAnchor" number="15.1">
<h2 class="hasAnchor"><span class="header-section-number">15.1</span> Poisson distribution<a href="#poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-40" class="definition"><strong>(#def:unlabeled-div-40) (Poisson distribution) </strong></span>We say the random variable <span class="math inline">\(X\)</span> has a {} distribution with parameter <span class="math inline">\(\mu &gt; 0\)</span> if
<span class="math display">\[
f(x) = e^{-\mu} \frac{ \mu^x}{x!},\;\;x=0,1,2,\dots\]</span></p>
</div>
<div id="notation" class="section level3 hasAnchor" number="15.1.1">
<h3 class="hasAnchor"><span class="header-section-number">15.1.1</span> Notation<a href="#notation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We write <span class="math inline">\(X\sim Poisson(\mu)\)</span> or <span class="math inline">\(Poi(\mu)\)</span>, where <span class="math inline">\(\mu\)</span> is called the <em>rate</em> parameter.</p>
</div>
<div id="interpreation-of-the-poisson-distribution" class="section level3 hasAnchor" number="15.1.2">
<h3 class="hasAnchor"><span class="header-section-number">15.1.2</span> Interpreation of the Poisson distribution<a href="#interpreation-of-the-poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p><em>Limiting case of binomial distribution</em>, where you fix <span class="math inline">\(\lambda = np\)</span> , and let <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(p \rightarrow 0\)</span> (This can be a consequence of b))</p></li>
<li><p><em>Poisson Process</em></p></li>
</ol>
</div>
</div>
<div id="poisson-as-the-limiting-distribution-of-the-binomial-distribution" class="section level2 hasAnchor" number="15.2">
<h2 class="hasAnchor"><span class="header-section-number">15.2</span> Poisson as the limiting distribution of the binomial distribution<a href="#poisson-as-the-limiting-distribution-of-the-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One way to view the Poisson distribution is to consider the limiting case of binomial distribution, where you fix <span class="math inline">\(\mu = np\)</span> , and let <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(p \rightarrow 0\)</span>.</p>
<p>One can show that if <span class="math inline">\(n\to \infty\)</span> and <span class="math inline">\(p=p_n \to 0\)</span> as <span class="math inline">\(n\to \infty\)</span> in such a way that <span class="math inline">\(n p_n \to \mu\)</span>, then
<span class="math display">\[
{n \choose x} p^x (1-p)^{n-x} \to e^{-\mu} \frac{ \mu^x}{x!},\;\;\;as\;\;\; n\to \infty.
\]</span>
Actually here, it is something called the *convergence in distribution**.</p>
</div>
<div id="poisson-process" class="section level2 hasAnchor" number="15.3">
<h2 class="hasAnchor"><span class="header-section-number">15.3</span> Poisson process<a href="#poisson-process" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider counting the number of occurrences of an event that happens at random points in time (or space). Poisson process is the <em>counting process</em> that satisfies the following</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independence</strong>: the number of occurrences in non-overlapping
intervals are independent.</p></li>
<li><p><strong>Individuality</strong>: for sufficiently short time periods of length
<span class="math inline">\(\Delta t,\)</span> the probability of 2 or more events occurring in the interval is
close to zero
<span class="math display">\[
\frac{P\left(  \text{2 or more events in }(t,t+\Delta_t)\right)}{\Delta_t}  \rightarrow 0,\;\; \Delta_t \to 0
\]</span></p></li>
<li><p><strong>Homogeneity or Uniformity</strong>: events occur at a uniform or
homogeneous rate <span class="math inline">\(\lambda\)</span> and proportional to time interval <span class="math inline">\(\Delta_t\)</span>, i.e.
<span class="math display">\[
\frac{P\left(  \text{one event in }(t,t+\Delta_t)\right) - \lambda\Delta_t }{\Delta_t}  \to 0.
\]</span></p></li>
</ol>
<p>If <span class="math inline">\(X=\)</span> occurrences in a time period of length <span class="math inline">\(t\)</span>, then
<span class="math display">\[X\sim Poi(\lambda t).\]</span></p>
<div class="definition">
<p><span id="def:unlabeled-div-41" class="definition"><strong>(#def:unlabeled-div-41) (Poisson process) </strong></span>A process that satisfies the prior conditions on the occurrence of events is often called a <strong>Poisson process</strong>. More precisely, if <span class="math inline">\(X_t, \; \text{for } t\ge0,\)</span> (a random variable for each <span class="math inline">\(t\)</span>) denotes the number of events that have occurred up to time <span class="math inline">\(t\)</span>, then <span class="math inline">\(X_t\)</span> is called a Poisson process.</p>
</div>
</div>
<div id="side-notes-rigorous-definition-of-convergence-in-distribution" class="section level2 hasAnchor" number="15.4">
<h2 class="hasAnchor"><span class="header-section-number">15.4</span> Side notes – Rigorous definition of convergence in distribution<a href="#side-notes-rigorous-definition-of-convergence-in-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section is just served as a reference for those of you who are interested in the rigorous definition of <em>convergence in distribution</em>. Do not worry too much if you are not interested in knowing those.</p>
<div class="definition">
<p><span id="def:unlabeled-div-42" class="definition"><strong>(#def:unlabeled-div-42) (Convergence in distribution) </strong></span>Let <span class="math inline">\((F_n)_{n\in\mathbb{N}}\)</span> and <span class="math inline">\(G\)</span> be CDFs. Let <span class="math inline">\(c(G) = \{x\in\mathbb{R} : G \text{ is cts. at }x\}\)</span> be the set of continuity points of <span class="math inline">\(G\)</span>. <span class="math inline">\(F_n\)</span> <em>converges in distribution</em> to <span class="math inline">\(G\)</span> if
<span class="math display">\[\begin{align}
    \forall x\in c(G) \quad F_n(x)\to G(x)  (\#eq:convdist)
\end{align}\]</span>
If <span class="math inline">\(X_n\)</span> has CDF <span class="math inline">\(F_n\)</span> for each <span class="math inline">\(n\)</span> and <span class="math inline">\(Y\)</span> has CDF <span class="math inline">\(G\)</span> and @ref(eq:convdist) holds then <span class="math inline">\(X_n\)</span> converges in distribution to <span class="math inline">\(Y\)</span>. Denoted <span class="math inline">\(X_n \stackrel{d}{\to} Y\)</span>, or <span class="math inline">\(F_n \stackrel{d}{\to} G\)</span></p>
</div>
<!--chapter:end:14-Lec14.Rmd-->
</div>
</div>
<div id="lecture-15-feburary-09-2024" class="section level1 hasAnchor" number="16">
<h1 class="hasAnchor"><span class="header-section-number">16</span> Lecture 15, Feburary 09, 2024<a href="#lecture-15-feburary-09-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="review-of-the-distributions-we-covered-before" class="section level2 hasAnchor" number="16.1">
<h2 class="hasAnchor"><span class="header-section-number">16.1</span> Review of the distributions we covered before<a href="#review-of-the-distributions-we-covered-before" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<table>
<colgroup>
<col width="7%" />
<col width="48%" />
<col width="43%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Distribution</th>
<th align="center"><span class="math inline">\(f(x)=P(X=x)\)</span></th>
<th align="center">Interpretation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(U[a,b]\)</span></td>
<td align="center"><span class="math inline">\(\frac{1}{b-a+1},\, x=a,a+1,\dots,b\)</span></td>
<td align="center">Sample from <span class="math inline">\(\{a,a+1,\dots,b\}\)</span> once uniformly at random\</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(Bin (n,p)\)</span></td>
<td align="center"><span class="math inline">\(\binom{n}{x}p^x(1-p)^{n-x},\,x=0,1,\dots,n\)</span></td>
<td align="center"><span class="math inline">\(\#\)</span> of successes in <span class="math inline">\(n\)</span> indep. trials with success prob. <span class="math inline">\(p\)</span>.</td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(Hyp(N,r,n)\)</span></td>
<td align="center"><span class="math inline">\(\frac{\binom{r}{x}\binom{N-r}{n-x}}{\binom{N}{n}},\)</span> <span class="math inline">\(\max\{0, n-(N-r)\} \leq x \leq \min\{r,n\}\)</span></td>
<td align="center"><span class="math inline">\(\#\)</span> of successes in <span class="math inline">\(n\)</span> draws without replacement from <span class="math inline">\(N\)</span> objects with <span class="math inline">\(r\)</span> successes.</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(NegBin(k,p)\)</span></td>
<td align="center"><span class="math inline">\(\binom{x+k-1}{x}p^k(1-p)^x,\, x=0,1,\dots,\)</span></td>
<td align="center"><span class="math inline">\(\#\)</span> of failures until <span class="math inline">\(k\)</span> successes in indep. trials with success prob. <span class="math inline">\(p\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(Geo(p)\)</span></td>
<td align="center"><span class="math display">\[p(1-p)^x,~ x=0,1,\dots \]</span></td>
<td align="center"><span class="math inline">\(\#\)</span> of failures until first success in indep. trials with success prob. <span class="math inline">\(p\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(Poi(\mu)\)</span></td>
<td align="center"><span class="math display">\[\exp(-\mu) \mu^x/x!,~ x=0,1,\dots \]</span></td>
<td align="center"><span class="math inline">\(\#\)</span> of occurrences in Poi process.</td>
</tr>
</tbody>
</table>
<!--chapter:end:15-Lec15.Rmd-->
</div>
</div>
<div id="lecture-16-feburary-12-2024" class="section level1 hasAnchor" number="17">
<h1 class="hasAnchor"><span class="header-section-number">17</span> Lecture 16, Feburary 12, 2024<a href="#lecture-16-feburary-12-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="chapter-7-expectation-and-variance" class="section level2 hasAnchor" number="17.1">
<h2 class="hasAnchor"><span class="header-section-number">17.1</span> Chapter 7 Expectation and Variance<a href="#chapter-7-expectation-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Probability and statistics are closely related to data; we often try to ``extract” additional information from data.</p>
<p>Some simple way to analyse and visualize data are:</p>
<ol style="list-style-type: decimal">
<li>frequency table</li>
<li>frequency histogram</li>
</ol>
<p>However, sometimes it is unclear how to define the groups in the frequency or in the histogram. Hence, we often would like to have more concise defined ways to analyse the random variable and its associated distribution. We call those numerical values as the (summary) <em>statistics</em>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-43" class="definition"><strong>(#def:unlabeled-div-43) (Sample mean) </strong></span>Let <span class="math inline">\(x_1, \ x_2, \ldots, \ x_n\)</span> be <span class="math inline">\(n\)</span> realizations of a random variable <span class="math inline">\(X\)</span> (such a set is called a <strong>sample</strong>). The <em>sample mean</em> is defined as
<span class="math display">\[
\bar{x} = \frac{1}{n}  \sum_{i=1}^n x_i
\]</span></p>
</div>
<p>Note: there are other summary</p>
<ol style="list-style-type: decimal">
<li>Sample median: a value such that half of the results are below it and the other half above it, when the sample is arranged in numerical order.</li>
</ol>
<ul>
<li>Median is more <em>robust</em> against some abnormally big/small observations, or recording errors.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Sample mode: The most frequently-occuring value in a sample.</li>
</ol>
<ul>
<li>We may have more than one mode.</li>
</ul>
</div>
<div id="theoretical-mean-and-the-sample-mean" class="section level2 hasAnchor" number="17.2">
<h2 class="hasAnchor"><span class="header-section-number">17.2</span> Theoretical mean and the sample mean<a href="#theoretical-mean-and-the-sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can compute <em>statistics</em> for a random variable <span class="math inline">\(X\)</span> directly if we know its distribution. Such a mean would be <em>theoretical</em>, as we are working from its probability distribution rather than an actual sample.</p>
<p>That means, we can do experiment to get sample, then compute the sample mean, while if we know the distribution, we can compute the theoretical mean without any samples.</p>
<div id="definition" class="section level3 hasAnchor" number="17.2.1">
<h3 class="hasAnchor"><span class="header-section-number">17.2.1</span> Definition<a href="#definition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-44" class="definition"><strong>(#def:unlabeled-div-44) ((Theoretical) Mean/Expectation/First moment) </strong></span>Suppose <span class="math inline">\(X\)</span> is a discrete random variable with probability function <span class="math inline">\(f_X(x)\)</span>. The expected value of <span class="math inline">\(X\)</span>, denoted by <span class="math inline">\(E[X]\)</span>, is then the number
<span class="math display">\[
E[X] = \sum_{x\in X(S) } x \ f_X(x) = \sum_{x\in X(S) } x \ P(X=x),
\]</span>
provided the sum converges absolutely (i.e., if <span class="math inline">\(\sum_{x\in X(S) } |x| \ f_X(x)&lt;\infty\)</span>)</p>
</div>
</div>
<div id="interpretation" class="section level3 hasAnchor" number="17.2.2">
<h3 class="hasAnchor"><span class="header-section-number">17.2.2</span> Interpretation<a href="#interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>Geometrical interpretation: <span class="math inline">\(E[X]\)</span> is the <em>balancing point</em> of the probability function <span class="math inline">\(f(x)\)</span>.</li>
<li><span class="math inline">\(E[X]\)</span> is what the average of many, many independent realizations of the random variable <span class="math inline">\(X\)</span> would approach (<strong>Law of large numbers</strong>).</li>
</ol>
<!--chapter:end:16-Lec16.Rmd-->
</div>
</div>
</div>
<div id="lecture-17-feburary-14-2024" class="section level1 hasAnchor" number="18">
<h1 class="hasAnchor"><span class="header-section-number">18</span> Lecture 17, Feburary 14, 2024<a href="#lecture-17-feburary-14-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="more-about-expectation" class="section level2 hasAnchor" number="18.1">
<h2 class="hasAnchor"><span class="header-section-number">18.1</span> More about expectation<a href="#more-about-expectation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Some observations we made from the example we went through in class.</p>
<ol style="list-style-type: decimal">
<li><p>Suppose <span class="math inline">\(X\)</span> is a random variable satisfying <span class="math inline">\(a \le X(\omega) \le b\)</span> for all <span class="math inline">\(\omega \in S\)</span>. Then <span class="math inline">\(a&lt;E[X]&lt;b\)</span>.</p></li>
<li><p>We may think that the expectation is a <strong>weighting</strong> of the values <span class="math inline">\(x\in X(S)\)</span> by its probability function (which is always positive, and sum up to one).</p></li>
</ol>
</div>
<div id="law-of-unconscious-statistician" class="section level2 hasAnchor" number="18.2">
<h2 class="hasAnchor"><span class="header-section-number">18.2</span> Law of Unconscious Statistician<a href="#law-of-unconscious-statistician" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If
<span class="math display">\[
g: {\mathbb R} \to {\mathbb R},
\]</span>
and <span class="math inline">\(X\)</span> is a random variable with probability function <span class="math inline">\(f\)</span>, then <span class="math inline">\(g(X)\)</span> is a random variable taking values <span class="math inline">\(g(X(S))\)</span> and</p>
<p><span class="math display">\[
E[g(X)] = \sum_{x \in X(S)} g(x) f(x)
\]</span></p>
<p>NOTE: In general, <span class="math inline">\(E[g(X)]\ne g(E[X])\)</span>!</p>
<p>e.g. For an arbitrary <em>convex</em> function <span class="math inline">\(g(X)\)</span>, <span class="math inline">\(g\{E(X)\} \le E\{g(x)\}\)</span>. This is a famous theorem called <strong>Jansen’s inequality</strong>.</p>
</div>
<div id="tricks" class="section level2 hasAnchor" number="18.3">
<h2 class="hasAnchor"><span class="header-section-number">18.3</span> Tricks<a href="#tricks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Sometimes, in order to calculate the expectation, some terminologies may help</p>
<ol style="list-style-type: decimal">
<li>For <span class="math inline">\(x\in X(S)\)</span>, <span class="math inline">\(\sum_{x\in X(S)} f_X(x)=1\)</span>.</li>
</ol>
</div>
<div id="property-of-expectation---linearity" class="section level2 hasAnchor" number="18.4">
<h2 class="hasAnchor"><span class="header-section-number">18.4</span> Property of expectation - linearity<a href="#property-of-expectation---linearity" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Suppose the random variable <span class="math inline">\(X\)</span> has <span class="math inline">\(E[X]=\mu\)</span>. Then for any constants <span class="math inline">\(a,b\in\mathbb{R}\)</span>,
<span class="math display">\[ E[aX+b]= a \mu + b = a E[X]+b\]</span></p>
<p>If we have 2 random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and three constants, <span class="math inline">\(a,b,c\in \mathbb{R}\)</span>, then
<span class="math display">\[
  E[aX+bY + c] = aE[X] + bE[Y] +c.
\]</span></p>
<div id="proof." class="section level3 hasAnchor" number="18.4.1">
<h3 class="hasAnchor"><span class="header-section-number">18.4.1</span> Proof.<a href="#proof." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="proof">
<p><span id="unlabeled-div-45" class="proof"><em>Proof</em>. </span>By the <em>Law of unconscious statistician</em> with <span class="math inline">\(g(x)=ax+b\)</span>, we find
<span class="math display">\[\begin{align*}
E[aX+b] &amp;=\sum_{x\in X(S)} (ax+b) f(x)\\
&amp; = a \cdot  \sum_{x\in X(S)} x f(x) + b  \cdot \sum_{x\in X(S)} f(x) \\
&amp;= a\cdot  E[X]  + b \cdot 1
\end{align*}\]</span></p>
</div>
</div>
</div>
<div id="mean-of-binomial-distribution" class="section level2 hasAnchor" number="18.5">
<h2 class="hasAnchor"><span class="header-section-number">18.5</span> Mean of binomial distribution<a href="#mean-of-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If <span class="math inline">\(X \sim Binomial(n,p)\)</span>, then <span class="math inline">\(E[X] = np\)</span></p>
<div class="proof">
<p><span id="unlabeled-div-46" class="proof"><em>Proof</em>. </span>Suppose <span class="math inline">\(X \sim Bin(n,p)\)</span>. Then we have
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum_{x=0}^n x\cdot \binom{n}{x}  p^x (1-p)^{n-x}\\
&amp;= \sum_{x=1}^n x\cdot \frac{n!}{(n-x)!x!} \cdot p^x (1-p)^{n-x}\\
&amp;= \sum_{x=1}^n \frac{x}{x(x-1)!} \frac{n(n-1)!}{(n-x)!} p p^{x-1} (1-p)^{n-x}\\
&amp;= \sum_{x=1}^n \frac{1}{(x-1)!}\frac{n(n-1)!}{(n-x-1+1)!} p p^{x-1} (1-p)^{n-x-1+1}\\
&amp;= np \sum_{x=1}^n \frac{1}{(x-1)!}\frac{(n-1)!}{((n-1)-(x-1))! (x-1)!} p\cdot p^{x-1} (1-p)^{(n-1)-(x-1)}\\
&amp;= np \; \underbrace{\sum_{y=0}^{n-1} \frac{(n-1)!}{((n-1)-y)! y!} \cdot p^{y} (1-p)^{(n-1)-y}}_{=1\text{b/c sum of PF of bin(n-1,p). is 1}}\\
&amp;= np
\end{align*}\]</span></p>
</div>
<!--chapter:end:17-Lec17.Rmd-->
</div>
</div>
<div id="lecture-18-feburary-16-2024" class="section level1 hasAnchor" number="19">
<h1 class="hasAnchor"><span class="header-section-number">19</span> Lecture 18, Feburary 16, 2024<a href="#lecture-18-feburary-16-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="mean-of-poisson-distribution" class="section level2 hasAnchor" number="19.1">
<h2 class="hasAnchor"><span class="header-section-number">19.1</span> Mean of Poisson distribution<a href="#mean-of-poisson-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(Z\sim Poi(\mu)\)</span>. Then the expectation of <span class="math inline">\(Z\)</span> is <span class="math inline">\(E[Z] = \mu\)</span>.</p>
<div class="proof">
<p><span id="unlabeled-div-47" class="proof"><em>Proof</em>. </span>Suppose <span class="math inline">\(X \sim Poi(\mu)\)</span>. Then we have
<span class="math display">\[\begin{align*}
E[X] &amp;= \sum\limits_\text{x \in X(S)} x\cdot f(x) &amp;&amp; \text{definition}\\
&amp;= \sum_{x=0}^\infty x \cdot e^{-\mu} \frac{\mu^x}{x!} &amp;&amp; \text{P.F.}\\
&amp;= \sum_{x=1}^\infty x \cdot e^{-\mu} \frac{\mu^x}{x!}\\
&amp;= \mu \sum_{x=1}^\infty e^{-\mu} \frac{\mu^{x-1}}{(x-1)!} &amp;&amp; \text{let $Y=x-1$}\\
&amp;= \mu \underbrace{\sum_{y=0}^\infty e^{-\mu} \frac{\mu^{y}}{y!}}_{=1\text{ as sum of p.f.}}\\
\end{align*}\]</span></p>
</div>
</div>
<div id="mean-of-hypergeometric-and-negative-binomial" class="section level2 hasAnchor" number="19.2">
<h2 class="hasAnchor"><span class="header-section-number">19.2</span> Mean of Hypergeometric and Negative binomial<a href="#mean-of-hypergeometric-and-negative-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="hypergeometric" class="section level3 hasAnchor" number="19.2.1">
<h3 class="hasAnchor"><span class="header-section-number">19.2.1</span> Hypergeometric<a href="#hypergeometric" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(X \sim hyp(N,r,n)\)</span>, then
<span class="math display">\[E[X]= n \frac{r}{N}.\]</span></p>
<p>Intuitively, <span class="math inline">\(n \cdot r/N\)</span> is the number of trials <span class="math inline">\(n\)</span> times the success probability in the</p>
</div>
<div id="negative-binomial" class="section level3 hasAnchor" number="19.2.2">
<h3 class="hasAnchor"><span class="header-section-number">19.2.2</span> Negative binomial<a href="#negative-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(Y \sim NB(k,p)\)</span>, then
<span class="math display">\[E[Y] = \frac{k(1-p)}{p}.\]</span></p>
</div>
</div>
<div id="note-that-the-expectation-does-not-always-exist" class="section level2 hasAnchor" number="19.3">
<h2 class="hasAnchor"><span class="header-section-number">19.3</span> Note that the expectation does not always exist<a href="#note-that-the-expectation-does-not-always-exist" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="example">
<p><span id="exm:unlabeled-div-48" class="example"><strong>(#exm:unlabeled-div-48) </strong></span>Consider a random variable <span class="math inline">\(X\)</span> with probability function <span class="math inline">\(f(x)=\frac{1}{x}\)</span> for <span class="math inline">\(x=2, 4, 8, 16,\dots\)</span> and 0 otherwise.</p>
<p>The rv <span class="math inline">\(X\)</span> can only take values <span class="math inline">\(x\)</span> of the form <span class="math inline">\(x=2^n\)</span>. Note that we can write <span class="math inline">\(P(X=2^n) = 2^{-n}\)</span> for <span class="math inline">\(n=1,2,\dots\)</span>. Thus
<span class="math display">\[\sum\limits_{\text{ all x}} f(x) = \sum_{n=1}^\infty P(X=2^n)= \sum_{n=1}^\infty \left(\frac{1}{2}\right)^n=\frac{1}{1-1/2} -1 = 1.\]</span>
Then
<span class="math display">\[ E[X] = \sum_{n=1}^\infty 2^n \left(\frac{1}{2^n}\right)=\sum_{n=1}^\infty 1 = \infty,\]</span></p>
</div>
<!--chapter:end:18-Lec18.Rmd-->
</div>
</div>
<div id="lecture-19-feburary-26-2024" class="section level1 hasAnchor" number="20">
<h1 class="hasAnchor"><span class="header-section-number">20</span> Lecture 19, Feburary 26, 2024<a href="#lecture-19-feburary-26-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>After discussing about the first moment/expectation/expected value, we want to see how other summary statistics we may want in order to describe a distribution/model.</p>
<div id="motivation-to-have-higher-moments" class="section level2 hasAnchor" number="20.1">
<h2 class="hasAnchor"><span class="header-section-number">20.1</span> Motivation to have higher moments<a href="#motivation-to-have-higher-moments" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Note: Does look at one summary statistics, the expectation, enough for describing and fully characterize a distribution?</p>
<div class="example">
<p><span id="exm:unlabeled-div-49" class="example"><strong>(#exm:unlabeled-div-49) </strong></span>Suppose we have two random variables</p>
<ul>
<li><span class="math inline">\(X\)</span> is a r.v. representing the outcome of one fair 6-sided die roll</li>
<li><span class="math inline">\(Y\)</span> is a r.v. representing the number of phone calls over 1 minute at Lenovo call centre, with the rate of 3.5 calls per minute</li>
</ul>
<p>By looking at their expectations, we have
<span class="math display">\[
  E X = 3.5 = E Y.
\]</span>
So if we only look at the expectation, we CANNOT DISTINGUISH those two, but clearly those two RV are very different!</p>
</div>
<p>Hence, we may need <em>other quantities</em> to describe the random variables. One key thing to study is how the RVs <em>deviate from its mean/expectation.</em></p>
<div id="deviations-from-the-mean" class="section level3 hasAnchor" number="20.1.1">
<h3 class="hasAnchor"><span class="header-section-number">20.1.1</span> Deviations from the mean<a href="#deviations-from-the-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Some common used ones are</p>
<ol style="list-style-type: decimal">
<li>Deviation</li>
</ol>
<p><span class="math display">\[\mbox{E}((X- \mu)) = \mbox{E}(X)- \mu = 0\]</span>
2. Absolute deviation
<span class="math display">\[ \mbox{E} \left(| X - \mu |\right)\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>Squared deviation
<span class="math display">\[ \mbox{E} \left((X-\mu)^2\right) \]</span>.</li>
</ol>
<p>The squared deviation turns out to be particular useful measure of variability, which we coin the term as the <strong>Variance.</strong></p>
</div>
</div>
<div id="the-defintion-of-variance" class="section level2 hasAnchor" number="20.2">
<h2 class="hasAnchor"><span class="header-section-number">20.2</span> The defintion of variance<a href="#the-defintion-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-50" class="definition"><strong>(#def:unlabeled-div-50) (variance) </strong></span>The variance of a random variable <span class="math inline">\(X\)</span> is denoted by <span class="math inline">\(Var(X)\)</span>, and is defined by
<span class="math display">\[
  Var(X) := E[(X-EX)^2].
\]</span></p>
</div>
<div id="properties-of-variance" class="section level3 hasAnchor" number="20.2.1">
<h3 class="hasAnchor"><span class="header-section-number">20.2.1</span> Properties of variance<a href="#properties-of-variance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>Shortcut definition
<span class="math display">\[
  Var(X) = E[X^2] - (E X)^2.
\]</span></p></li>
<li><p>Variance is always positive.</p></li>
</ol>
<p>This can be easily seen from the definition of the variance, that it is the expectation of the square of <span class="math inline">\(X-EX\)</span>. Everything squared is a non-negative number. And expectation can be thought as the weighted average of a random variable. Thus, it is <strong>Always Nonnegative</strong>!</p>
<ol start="3" style="list-style-type: decimal">
<li>Variance is not linear</li>
</ol>
<p>Let <span class="math inline">\(a,b\in \mathbb{R}\)</span>, and <span class="math inline">\(X\)</span> be a RV. Then
<span class="math display">\[
  Var(aX +b ) = a^2 Var(X).
\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li>The case when variance is zero</li>
</ol>
<p>Suppose a random variable <span class="math inline">\(X\)</span> has <span class="math inline">\(E(X)=\mu\)</span> and <span class="math inline">\(Var(X)=0\)</span>. This means, <span class="math inline">\(X\)</span> does not ``vary’’ or deviate from its mean at all, and is (with probability 1) always the same value <span class="math inline">\(\mu\)</span>, as we show in the following</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-51" class="theorem"><strong>(#thm:unlabeled-div-51) </strong></span><span class="math inline">\(Var(X) = 0\)</span> if and only if <span class="math inline">\(P(X = \mbox{E}(X)) = 1\)</span>.</p>
</div>
<ol start="5" style="list-style-type: decimal">
<li>Alternative way to write the variance</li>
</ol>
<p>We can write the variance of X as
<span class="math display">\[
  Var(X) = E[X(X-1)] + E[X] - (EX)^2.
\]</span></p>
<div class="proof">
<p><span id="unlabeled-div-52" class="proof"><em>Proof</em>. </span>Look at the right-hand-side,
<span class="math display">\[\begin{align*}
  &amp;E[X(X-1)]+ E[X] - E^2[X]\\
  &amp;=E[X^2-X] + E[X] - E^2[X]\\
  &amp;= E[X^2 -X + X - E^2[X]] \\
  &amp;= E[X^2 -\mu^2] = E[X^2] - \mu^2,
\end{align*}\]</span>
as in our shortcut formula.</p>
</div>
</div>
</div>
<div id="variance-of-a-binomial-distribution" class="section level2 hasAnchor" number="20.3">
<h2 class="hasAnchor"><span class="header-section-number">20.3</span> Variance of a binomial distribution<a href="#variance-of-a-binomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="theorem">
<p><span id="thm:unlabeled-div-53" class="theorem"><strong>(#thm:unlabeled-div-53) </strong></span>Suppose that <span class="math inline">\(X\sim Binomial(n,p)\)</span>, then</p>
<p><span class="math display">\[
Var(X) = np(1-p).
\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-54" class="proof"><em>Proof</em>. </span>We use the formula <span class="math display">\[Var(X) = E(X(X-1)) + E(X) - (E(X))^2\]</span> and note we already know <span class="math inline">\(E(X)=np\)</span>.
Then we tweek the sum for <span class="math inline">\(E(X(X-1))\)</span> similarly as before:</p>
<p><span class="math display">\[\begin{align*}
E&amp;(X(X-1)) =\sum_{x=0}^n x(x-1) \frac{n!}{(n-x)! x!} p^x (1-p)^{n-x} \\
&amp;= n(n-1) p^2 \sum_{x=2}^n \frac{(n-2)!}{(n-2-(x-2))!(x-2)!} p^{x-2} (1-p)^{n-2-(x-2)}\\
&amp;= n(n-1) p^2 \underbrace{\sum_{y=0}^{n-2} \frac{(n-2)!}{(n-2-y)!y!} p^{y} (1-p)^{n-2-y}}_{=1\text{ as sum of $Bin(n-2,p)$ p.f.}}\\
&amp;= n(n-1) p^2
\end{align*}\]</span></p>
</div>
<!--chapter:end:19-Lec19.Rmd-->
</div>
</div>
<div id="lecture-20-feburary-28-2024" class="section level1 hasAnchor" number="21">
<h1 class="hasAnchor"><span class="header-section-number">21</span> Lecture 20, Feburary 28, 2024<a href="#lecture-20-feburary-28-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="variance-of-poisson-hypergeometric-and-negative-binomial" class="section level2 hasAnchor" number="21.1">
<h2 class="hasAnchor"><span class="header-section-number">21.1</span> Variance of Poisson, Hypergeometric and Negative Binomial<a href="#variance-of-poisson-hypergeometric-and-negative-binomial" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Last time, we saw that if <span class="math inline">\(X \sim Bin(m,p)\)</span>, then <span class="math inline">\(\mathbb{V}ar(X) = np(1-p)\)</span>. We proved this using the definition of the expectation and with the <strong>summation trick</strong>.</p>
<p>Similarly, one can show that</p>
<ul>
<li>If <span class="math inline">\(X\sim Poi(\lambda)\)</span>, then</li>
</ul>
<p><span class="math display">\[
\mathbb{V}ar(X) = \lambda.
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(Y \sim hyp(N,r,n)\)</span>, then
<span class="math display">\[
\mathbb{V}ar(Y) = n \frac{r}{N} \left(1-\frac{r}{N}\right)\left(\frac{N-n}{N-1}\right).
\]</span></p></li>
<li><p>If <span class="math inline">\(Z \sim NB(k,p)\)</span>, then
<span class="math display">\[
\mathbb{V}ar(Z) = \frac{k(1-p)}{p^2}.
\]</span></p></li>
</ul>
</div>
<div id="standard-deviation" class="section level2 hasAnchor" number="21.2">
<h2 class="hasAnchor"><span class="header-section-number">21.2</span> Standard Deviation<a href="#standard-deviation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Note that <span class="math inline">\(\mathbb{V}ar(X)\)</span> is in the squared unit (e.g., <span class="math inline">\(X\)</span> in <span class="math inline">\(meters\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\mathbb{V}ar(X)\)</span> is in <span class="math inline">\(meters^2\)</span>). To recover the original unit, we take the square root of variance.\</p>
<div class="definition">
<p><span id="def:unlabeled-div-55" class="definition"><strong>(#def:unlabeled-div-55) (Standard Deviation) </strong></span>The <strong>standard deviation</strong> of a random variable <span class="math inline">\(X\)</span> is denoted <span class="math inline">\(SD(X)\)</span>, and defined by
<span class="math display">\[
SD(X) = \sqrt{\mathbb{V}ar(X)}.
\]</span></p>
</div>
</div>
<div id="last-note-of-the-chapter" class="section level2 hasAnchor" number="21.3">
<h2 class="hasAnchor"><span class="header-section-number">21.3</span> Last note of the chapter<a href="#last-note-of-the-chapter" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li><p>The expectation and the variance give a simple giving the center and variability of the distribution</p></li>
<li><p>We call <span class="math inline">\(E[X]\)</span> and <span class="math inline">\(E[X^2]\)</span> the first and second moment of <span class="math inline">\(X\)</span></p></li>
<li><p>In general, <span class="math inline">\(E[X^k]\)</span> is <strong>the <span class="math inline">\(k\)</span>th moment of the distribution of <span class="math inline">\(X\)</span></strong>, while <span class="math inline">\(E[ (X-E(X))^k]\)</span> is <strong>the <span class="math inline">\(k\)</span>th central moment of the distribution of <span class="math inline">\(X\)</span></strong></p></li>
<li><p>You’ll see other statistics later in STAT 231 and onwards, such as</p>
<ul>
<li><p><strong>Skewness</strong> (measures asymmetry)
<span class="math display">\[
E\left[\left( \frac{(X - E(X))}{\sqrt{\mathbb{V}ar(X)}} \right)^3\right].
\]</span></p></li>
<li><p><strong>Kurtosis</strong> (measures heavy tailedness)
<span class="math display">\[
E\left[\left( \frac{(X - E(X))}{\sqrt{\mathbb{V}ar(X)}} \right)^4\right].
\]</span></p></li>
</ul></li>
</ul>
</div>
<div id="chapter-8-continuous-random-variables" class="section level2 hasAnchor" number="21.4">
<h2 class="hasAnchor"><span class="header-section-number">21.4</span> Chapter 8 Continuous Random Variables<a href="#chapter-8-continuous-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="continuous-random-variable" class="section level3 hasAnchor" number="21.4.1">
<h3 class="hasAnchor"><span class="header-section-number">21.4.1</span> Continuous random variable<a href="#continuous-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(X\)</span> be a random variable and <span class="math inline">\(F_X(x) = P(X\leq x) = P(\{\omega\in S : X(\omega)\leq x\})\)</span> for <span class="math inline">\(x\in\mathbb{R}\)</span> be its cumulative distribution function (cdf).</p>
<p>We say that the random variable <span class="math inline">\(X\)</span> is</p>
<ul>
<li><p><strong>discrete</strong> if <span class="math inline">\(F_X\)</span> is <em>piecewise constant</em>.</p>
<ul>
<li>The jumps of <span class="math inline">\(F\)</span> are exactly the range of <span class="math inline">\(X\)</span>, <span class="math inline">\(X(S)\)</span>. For <span class="math inline">\(x\in X(S)\)</span> (at the jumps of <span class="math inline">\(F\)</span>),</li>
<li>the probability function is <span class="math inline">\(f(x)=P(X=x)=\lim_{h\downarrow 0} F(x+h)-F(x)=\text{size of jump at $x$}\)</span>.</li>
</ul></li>
<li><p><strong>continuous</strong> if <span class="math inline">\(F_X\)</span> is a <em>continuous function</em>.</p></li>
<li><p><em>absolutely continuous</em> if
<span class="math display">\[ F_X(x) = \int_{-\infty}^x f(t) dt\]</span></p></li>
</ul>
<p>In this course, when talking about continuous random variables, we mean absolutely continuous.
### Probability density function</p>
<div class="definition">
<p><span id="def:unlabeled-div-56" class="definition"><strong>(#def:unlabeled-div-56) (Probability Density Function) </strong></span>We say that an continuous random variable <span class="math inline">\(X\)</span> with distribution function <span class="math inline">\(F\)</span> admits probability function (PDF) <span class="math inline">\(f(x)\)</span>, if</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(f(x)\geq 0\)</span> for all <span class="math inline">\(x\in\mathbb{R}\)</span>;</li>
<li><span class="math inline">\(\int_{-\infty}^\infty f(x)dx = 1\)</span>;</li>
<li><span class="math inline">\(F(x)=P(X\leq x)= \int_{-\infty}^x f(t)\;d t\)</span>.</li>
</ol>
<p>In other words, <span class="math inline">\(F\)</span> is an antiderivative of <span class="math inline">\(f\)</span>, of <span class="math inline">\(f\)</span> is the derivative of <span class="math inline">\(F\)</span>,
<span class="math display">\[ f(x) = F&#39;(x) = \frac{d}{dx} F(x)\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-57" class="definition"><strong>(#def:unlabeled-div-57) (Support) </strong></span>The <strong>support</strong> of a r.v. <span class="math inline">\(X\)</span> with density <span class="math inline">\(F\)</span> is the set
<span class="math display">\[
supp(f) = \{x \in \mathbb{R}: f(x) \neq 0\}.
\]</span></p>
</div>
<p>If <span class="math inline">\(X\)</span> was a discrete random variable instead, these 4 probabilities could all be different.
If <span class="math inline">\(X\)</span> is a continuous rv with probability density function (pdf) <span class="math inline">\(f_X(x)\)</span>, then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(P(X=x)=0\)</span></li>
<li><span class="math inline">\(F(x) = \int_{-\infty}^x f_X(t)dt\)</span></li>
<li><span class="math inline">\(P(a &lt; X \leq b) = \int_a^b f_X(t)dt\)</span></li>
</ol>
<p>We highlight: For a continuous random variable <span class="math inline">\(X\)</span>, <span class="math inline">\(f(x)\)</span> is <em>not</em> <span class="math inline">\(P(X=x)\)</span>, which is always zero.</p>
</div>
<div id="equality-does-not-matter-in-the-continous-case" class="section level3 hasAnchor" number="21.4.2">
<h3 class="hasAnchor"><span class="header-section-number">21.4.2</span> Equality does not matter in the continous case<a href="#equality-does-not-matter-in-the-continous-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(X\)</span> is a continuous random variable, then
<span class="math display">\[ P(a&lt;X\leq b) = F(b)-F(a)\]</span>
<span class="math display">\[ P(a\leq X \leq b) =  P(a&lt;X\leq b) +P(X=a)=[F(b)-F(a)]+0\]</span>
<span class="math display">\[ P(a&lt;X&lt;b)=P(a&lt;X\leq b) -P(X=b)=[F(b)-F(a)]-0\]</span>
<span class="math display">\[ P(a\leq X&lt;b) = P(a&lt;X\leq b) +P(X=a)-P(X=b)=[F(b)-F(a)]\]</span>
so if <span class="math inline">\(X\)</span> is <strong>continuous</strong>, all these probabilities coincide!</p>
<!--chapter:end:20-Lec20.Rmd-->
</div>
</div>
</div>
<div id="lecture-21-march-01-2024" class="section level1 hasAnchor" number="22">
<h1 class="hasAnchor"><span class="header-section-number">22</span> Lecture 21, March 01, 2024<a href="#lecture-21-march-01-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="law-of-unconciousness-of-statistician-continuous-version" class="section level2 hasAnchor" number="22.1">
<h2 class="hasAnchor"><span class="header-section-number">22.1</span> Law of unconciousness of statistician, continuous version<a href="#law-of-unconciousness-of-statistician-continuous-version" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-58" class="definition"><strong>(#def:unlabeled-div-58) (LOTUS) </strong></span>If <span class="math inline">\(X\)</span> is a continuous random variable with pdf <span class="math inline">\(f(x)\)</span>, and <span class="math inline">\(g:\mathbb{R}\to \mathbb{R}\)</span> is a function, then
<span class="math display">\[
  \mathbb{E}g(x) = \int_{-\infty}^\infty g(x)f(x)dx
\]</span>
provided the expression exists.</p>
</div>
<p>By above, we can calculate the expectation and the variance as follows</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\mathbb{E}X = \int_{-\infty}^\infty x f(x) dx\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{V}ar(X) = \mathbb{E}[(X-\mathbb{E}X)^2] = \int_{-\infty}^\infty (x-\mathbb{E}X)^2 f(x)dx\)</span>.</p></li>
</ol>
<p>Similar to the discrete random variable case, we have the shortcut formula to calculate the variance:
<span class="math display">\[
  \mathbb{V}ar(X) = \mathbb{E}[X^2] - (\mathbb{E}X)^2.
\]</span></p>
</div>
<div id="function-of-random-variable" class="section level2 hasAnchor" number="22.2">
<h2 class="hasAnchor"><span class="header-section-number">22.2</span> function of random variable<a href="#function-of-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>If <span class="math inline">\(X\)</span> is a random variable and <span class="math inline">\(g\)</span> is a function, then <span class="math inline">\(Y=g(X)\)</span> is also a random variable</li>
<li>By the law of the unconscious statistician,</li>
</ul>
<p><span class="math display">\[ \mathbb{E}(Y)=\mathbb{E}(g(X)) = \begin{cases} \sum_{\text{all }x} g(x)\cdot f(x),\quad&amp;\text{if $X$ discrete with pf }f,\\
\int_{\mathbb{R}} g(x)\cdot f(x)dx,\quad&amp;\text{if $X$ continuous with pdf }f\end{cases}
\]</span></p>
<ul>
<li>Next, we are studying how to find the distribution of <span class="math inline">\(Y=g(X)\)</span>.</li>
</ul>
<!--chapter:end:21-Lec21.Rmd-->
</div>
</div>
<div id="lecture-22-march-04-2024" class="section level1 hasAnchor" number="23">
<h1 class="hasAnchor"><span class="header-section-number">23</span> Lecture 22, March 04, 2024<a href="#lecture-22-march-04-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="receipt-to-find-the-distribution-of-the-transformed-random-varaible-ygx." class="section level2 hasAnchor" number="23.1">
<h2 class="hasAnchor"><span class="header-section-number">23.1</span> Receipt to find the distribution of the transformed random varaible <span class="math inline">\(Y=g(X)\)</span>.<a href="#receipt-to-find-the-distribution-of-the-transformed-random-varaible-ygx." class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In class, we have an easy three steps receipt:</p>
<p>Let <span class="math inline">\(Y=g(X)\)</span>. In general, we can use the following steps to find the pdf of <span class="math inline">\(Y=g(X)\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Using the support of <span class="math inline">\(X\)</span>, find the support of <span class="math inline">\(Y=g(X)\)</span> denoted by <span class="math inline">\(\text{supp}(Y)=\{y\in\mathbb{R}:f_Y(y)&gt;0\}\)</span></p></li>
<li><p>Express the cdf of <span class="math inline">\(Y=g(X)\)</span> using the cdf of <span class="math inline">\(X\)</span>: $F_Y(y)=P(g(X)y)=$.</p></li>
<li><p>Compute the pdf of <span class="math inline">\(Y=g(X)\)</span> by differentiating <span class="math inline">\(f_y(y)=F_y&#39;(y)\)</span></p></li>
</ol>
<p>Notes: If the function <span class="math inline">\(g\)</span> is <em>invertible</em> and <em>differentiable</em> with inverse <span class="math inline">\(g^{-1}\)</span> on the support of <span class="math inline">\(Y\)</span>, then
<span class="math display">\[ f_Y(y)= |(g^{-1})&#39;(y)| f_X(g^{-1}(y)),\quad y\in\text{supp}(Y)\]</span></p>
<p>Question:
When the function <span class="math inline">\(g\)</span> is <em>not strictly increasing (or decreasing)</em> over the support of <span class="math inline">\(X\)</span>, then we <em>must be careful</em> when rewriting the inequality <span class="math inline">\(P(g(X)\leq y)\)</span>.</p>
</div>
<div id="quantile" class="section level2 hasAnchor" number="23.2">
<h2 class="hasAnchor"><span class="header-section-number">23.2</span> Quantile<a href="#quantile" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-59" class="definition"><strong>(#def:unlabeled-div-59) (Quantile) </strong></span>Let <span class="math inline">\(p\in[0,1]\)</span>. The <span class="math inline">\(100\times p\)</span>th percentile (or <span class="math inline">\(100\times p\%\)</span> quantile) of the distribution of <span class="math inline">\(X\)</span> with cdf <span class="math inline">\(F_X\)</span> is the value <span class="math inline">\(c_p\)</span> given by
<span class="math display">\[ c_p = \inf\{x\in\mathbb{R}: F_X(x) \geq p \}\]</span></p>
</div>
<ul>
<li>The infimum of a set <span class="math inline">\(A\)</span> is the largest lower bound of <span class="math inline">\(A\)</span> (e.g., <span class="math inline">\(\inf\{x\in\mathbb{R}: 0&lt;x&lt;1\}=0\)</span>)</li>
<li>The quantile function <span class="math inline">\(p\mapsto c_p\)</span> is also called generalized invere function.</li>
<li>The probability that <span class="math inline">\(X\)</span> is at most <span class="math inline">\(c_p\)</span> is at least <span class="math inline">\(100\times p\)</span>%. More precisely, <span class="math inline">\(c_p\)</span> is the smallest value <span class="math inline">\(c\)</span> so <span class="math inline">\(P(X\leq c)\)</span> is at least <span class="math inline">\(p\)</span>.
*The of a distribution is its 50% quantile. </li>
<li>If the distribution function <span class="math inline">\(F_X\)</span> is continuous and strictly increasing, it has an inverse <span class="math inline">\(F^{-1}\)</span>, and we get
<span class="math display">\[ c_p = F^{-1}(p)\]</span></li>
<li>In the previous clicker question, we computed the 95% quantile.</li>
</ul>
<!--chapter:end:22-Lec22.Rmd-->
</div>
</div>
<div id="lecture-23-march-06-2024" class="section level1 hasAnchor" number="24">
<h1 class="hasAnchor"><span class="header-section-number">24</span> Lecture 23, March 06, 2024<a href="#lecture-23-march-06-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="recap-the-quantile-function" class="section level2 hasAnchor" number="24.1">
<h2 class="hasAnchor"><span class="header-section-number">24.1</span> Recap the quantile function<a href="#recap-the-quantile-function" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:unlabeled-div-60" class="definition"><strong>(#def:unlabeled-div-60) (Quantile) </strong></span>Let <span class="math inline">\(p\in[0,1]\)</span>. The <span class="math inline">\(100\times p\)</span>th percentile (or <span class="math inline">\(100\times p\%\)</span> quantile) of the distribution of <span class="math inline">\(X\)</span> with cdf <span class="math inline">\(F_X\)</span> is the value <span class="math inline">\(c_p\)</span> given by
<span class="math display">\[ c_p = \inf\{x\in\mathbb{R}: F_X(x) \geq p \}\]</span></p>
</div>
<ul>
<li>The infimum of a set <span class="math inline">\(A\)</span> is the largest lower bound of <span class="math inline">\(A\)</span> (e.g., <span class="math inline">\(\inf\{x\in\mathbb{R}: 0&lt;x&lt;1\}=0\)</span>)</li>
<li>The quantile function <span class="math inline">\(p\mapsto c_p\)</span> is also called generalized invere function.</li>
<li>The probability that <span class="math inline">\(X\)</span> is at most <span class="math inline">\(c_p\)</span> is at least <span class="math inline">\(100\times p\)</span>%. More precisely, <span class="math inline">\(c_p\)</span> is the smallest value <span class="math inline">\(c\)</span> so <span class="math inline">\(P(X\leq c)\)</span> is at least <span class="math inline">\(p\)</span>.</li>
<li>The <strong>median</strong> of a distribution is its 50% quantile.</li>
<li>If the distribution function <span class="math inline">\(F_X\)</span> is continuous and strictly increasing, it has an inverse <span class="math inline">\(F^{-1}\)</span>, and we get
<span class="math display">\[ c_p = F^{-1}(p)\]</span></li>
<li>In the previous clicker question, we computed the 95% quantile.</li>
</ul>
</div>
<div id="quantiles-for-discrete-distributions" class="section level2 hasAnchor" number="24.2">
<h2 class="hasAnchor"><span class="header-section-number">24.2</span> Quantiles for discrete distributions<a href="#quantiles-for-discrete-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If <span class="math inline">\(F\)</span> is strictly increasing and continuous, the <span class="math inline">\(p\)</span>-quantile <span class="math inline">\(c_p\)</span> satisfying <span class="math inline">\(F(c_p)=p\)</span> is just <span class="math inline">\(c_p=F^{-1}(p)\)</span>, the (ordinary) inverse of <span class="math inline">\(F\)</span> at <span class="math inline">\(p\)</span> found by solving <span class="math inline">\(F(c_p)=p\)</span> for <span class="math inline">\(c_p\)</span>.</p>
<p>If <span class="math inline">\(F\)</span> has jumps or flat parts, then <span class="math inline">\(F(c_p)=p\)</span> may not have any solution or infinitely many! In this case, we use
<span class="math display">\[ F^{-1}(p)=\inf_{x\in\mathbb{R}}\{F(x)\geq p\},\]</span>
though <span class="math inline">\(F^{-1}\)</span> is an abuse of notation here and does not mean the ordinary inverse.</p>
</div>
<div id="special-named-distributions" class="section level2 hasAnchor" number="24.3">
<h2 class="hasAnchor"><span class="header-section-number">24.3</span> Special named distributions<a href="#special-named-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="continuous-uniform-distribution" class="section level3 hasAnchor" number="24.3.1">
<h3 class="hasAnchor"><span class="header-section-number">24.3.1</span> Continuous uniform distribution<a href="#continuous-uniform-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-61" class="definition"><strong>(#def:unlabeled-div-61) (Continuous uniform distribution) </strong></span>We say that <span class="math inline">\(X\)</span> has a continuous uniform distribution on <span class="math inline">\((a,b)\)</span> if <span class="math inline">\(X\)</span> has pdf
<span class="math display">\[ f(x) =\begin{cases}
      \frac{1}{b-a} &amp; \mbox{ $x \in (a,b)$,} \\
      0 &amp; \mbox{ otherwise }
   \end{cases}
\]</span>
This is abbreviated <span class="math inline">\(X \sim U(a,b)\)</span>.</p>
</div>
<ul>
<li>For continuous random variables, <span class="math inline">\(P(X=x)=0\)</span>, so it does not matter mathematically if we think of the uniform distribution as sampling uniformly on <span class="math inline">\((a,b)\)</span> or <span class="math inline">\([a,b]\)</span> or <span class="math inline">\((a,b]\)</span> or <span class="math inline">\([a,b)\)</span></li>
<li>Examples:
<ul>
<li>Cutting a stick of length 1 at a random position (<em>motivating example</em>!)</li>
<li>Spinning a wheel in a game show</li>
</ul></li>
</ul>
<div id="expectation-and-variance" class="section level4 hasAnchor" number="24.3.1.1">
<h4 class="hasAnchor"><span class="header-section-number">24.3.1.1</span> Expectation and variance<a href="#expectation-and-variance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="definition">
<p><span id="def:unlabeled-div-62" class="definition"><strong>(#def:unlabeled-div-62) (Expectation of continuous uniform distribution) </strong></span>Let <span class="math inline">\(X\sim U(a,b)\)</span>. Then
<span class="math display">\[E(X) = \frac{a + b}{2}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-63" class="proof"><em>Proof</em>. </span>Recall that <span class="math inline">\(X\sim U(a,b)\)</span> has density <span class="math inline">\(f(x)=\frac{1}{b-a}\)</span> if <span class="math inline">\(x\in(a,b)\)</span> and 0 otherwise. Thus,
<span class="math display">\[\begin{align*}
E(X) &amp;= \int x f(x)\; d x = \int_a^b x \cdot \frac{1}{b-a}\;d x \\
&amp;= \frac{1}{2} \frac{b^2-a^2}{b-a}=\frac{(b-a)(b+a)}{2(b-a)}=\frac{a+b}{2}.\end{align*}\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-64" class="definition"><strong>(#def:unlabeled-div-64) (Variance of continuous uniform distribution) </strong></span>Let <span class="math inline">\(X\sim U(a,b)\)</span>. Then
<span class="math display">\[Var(X) = \frac{(b-a)^2}{12}\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-65" class="proof"><em>Proof</em>. </span>Recall that <span class="math inline">\(X\sim U(a,b)\)</span> has density <span class="math inline">\(f(x)=\frac{1}{b-a}\)</span> if <span class="math inline">\(x\in(a,b)\)</span> and 0 otherwise. ALso from above, we have <span class="math inline">\(E X =\frac{a + b}{2}\)</span>. Then
<span class="math display">\[\begin{align*}
E(X^2) &amp;= \int x^2 f(x)\;d x = \int_a^b x^2 \frac{1}{b-a}\;d x \\
&amp;= \frac{b^3-a^3}{3(b-a)}=\frac{(b-a)(b^2+ab+a^2)}{3(b-a)}=\frac{b^2+ab+a^2}{3}.\end{align*}\]</span></p>
<p>Combining and simplifying gives
<span class="math display">\[ Var(X) = E(X^2)-E(X)^2 = \frac{(b-a)^2}{12}.\]</span></p>
</div>
</div>
</div>
<div id="exponential-distribution" class="section level3 hasAnchor" number="24.3.2">
<h3 class="hasAnchor"><span class="header-section-number">24.3.2</span> Exponential distribution<a href="#exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-66" class="definition"><strong>(#def:unlabeled-div-66) (Rate-parametrization of exponential distribution) </strong></span>We say that <span class="math inline">\(X\)</span> has an exponential distribution with parameter <span class="math inline">\(\lambda\)</span>, denoted by <span class="math inline">\(X\sim Exp(\lambda)\)</span>, if the density of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[ f(x) =\begin{cases}
      \lambda e ^{- \lambda x} &amp; \mbox{ $x &gt;0$,} \\
      0 &amp; \mbox{ $x \le 0$ }.
   \end{cases}
\]</span></p>
</div>
<ul>
<li>Since <span class="math inline">\(\int_{\mathbb{R}} f(x) dx = \int_0^\infty \lambda e^{-\lambda x}dx =1\)</span> and <span class="math inline">\(f(x)\geq 0\)</span> for all <span class="math inline">\(x\in\mathbb{R}\)</span>, this is a valid pdf.</li>
</ul>
<div id="moments" class="section level4 hasAnchor" number="24.3.2.1">
<h4 class="hasAnchor"><span class="header-section-number">24.3.2.1</span> Moments<a href="#moments" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When computing <span class="math inline">\(E(X)\)</span> and <span class="math inline">\(Var(X)\)</span>, we need to solve integrals
<span class="math display">\[ E(X) = \int_0^\infty x\cdot \frac{1}{\theta} e^{-\frac{x}{\theta}}\;d x\]</span>
and
<span class="math display">\[ E(X^2) = \int_0^\infty x^2 \cdot \frac{1}{\theta} e^{-\frac{x}{\theta}}\;d x\]</span>
which can be done using integration by parts.</p>
<ul>
<li>Alternatively, we can use the <strong>gamma function</strong></li>
</ul>
<div class="definition">
<p><span id="def:unlabeled-div-67" class="definition"><strong>(#def:unlabeled-div-67) (Gamma function) </strong></span>The integral
<span class="math display">\[
        \Gamma(\alpha) = \int_0^\infty y^{\alpha - 1} e^{-y} dy, \ \alpha &gt; 0
        \]</span>
is called the gamma function of <span class="math inline">\(\alpha\)</span>.</p>
</div>
<div id="properties-of-gamma-function" class="section level5 hasAnchor" number="24.3.2.1.1">
<h5 class="hasAnchor"><span class="header-section-number">24.3.2.1.1</span> Properties of Gamma function<a href="#properties-of-gamma-function" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Some useful properties of <span class="math inline">\(\Gamma(\alpha)\)</span> are</p>
<ul>
<li><span class="math inline">\(\Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1)\)</span> for <span class="math inline">\(\alpha &gt; 1\)</span></li>
<li><span class="math inline">\(\Gamma(\alpha) = (\alpha - 1)!\)</span> for <span class="math inline">\(\alpha \in \mathbb{N}\)</span></li>
<li><span class="math inline">\(\Gamma(1/2) = \sqrt{\pi}\)</span></li>
<li>The Gamma function is a continuous function that interpolates the factorial function.</li>
<li>Gamma function is used to derive the Gamma distribution (<span class="math inline">\(\Rightarrow\)</span> STAT 330), which is extremely important in non-life insurance pricing, and it can be used to model certain brain signals in neuroscience.</li>
</ul>
</div>
</div>
<div id="expectation-and-variance-1" class="section level4 hasAnchor" number="24.3.2.2">
<h4 class="hasAnchor"><span class="header-section-number">24.3.2.2</span> Expectation and variance<a href="#expectation-and-variance-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>With the Gamma function at hand, show that if <span class="math inline">\(X\sim Exp(\theta)\)</span>, then
<span class="math display">\[ E(X)=\theta\]</span>
and
<span class="math display">\[Var(X)=\theta^2.\]</span></p>
<p>There are other paramaterizations for exponential distribution.
It is sometimes more convenient to express the parameter as <span class="math inline">\(\frac{1}{\theta}=\lambda\)</span>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-68" class="definition"><strong>(#def:unlabeled-div-68) (theta-parametrisation of exponential distribution) </strong></span>We say that <span class="math inline">\(X\)</span> has an exponential distribution with parameter <span class="math inline">\(\theta\)</span> <span class="math inline">\((X\sim Exp(\theta))\)</span> if the density of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[ f(x) =\begin{cases}
      \frac{1}{\theta} e ^{- \frac{x}{\theta}} &amp; \mbox{ $x &gt;0$,} \\
      0 &amp; \mbox{ $x \le 0$ }.
   \end{cases}
\]</span></p>
</div>
<p>If <span class="math inline">\(\lambda\)</span> denotes the <em>rate</em> of event occurrence in a Poisson process, then <span class="math inline">\(\theta = 1/\lambda\)</span> denotes the <em>waiting time</em> until the first occurrence.</p>
<!--chapter:end:23-Lec23.Rmd-->
</div>
</div>
</div>
</div>
<div id="lecture-24-march-08-2024" class="section level1 hasAnchor" number="25">
<h1 class="hasAnchor"><span class="header-section-number">25</span> Lecture 24, March 08, 2024<a href="#lecture-24-march-08-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="proof-of-the-moments-of-exponential-distribution" class="section level2 hasAnchor" number="25.1">
<h2 class="hasAnchor"><span class="header-section-number">25.1</span> Proof of the moments of exponential distribution<a href="#proof-of-the-moments-of-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall that we can have many ways to show the expectation and the variance of <span class="math inline">\(X\sim Exp(\theta)\)</span> using the Gamma function. Then we have
<span class="math display">\[ E(X)=\theta\]</span>
and
<span class="math display">\[Var(X)=\theta^2.\]</span></p>
<p>Let <span class="math inline">\(X\sim Exp(\theta)\)</span>. We use the change of variable <span class="math inline">\(y=x\theta\)</span> with <span class="math inline">\(dx =\theta dy\)</span>
<span class="math display">\[\begin{align*}
        E[X]&amp;= \int_0^\infty x \cdot \frac{1}{\theta} e^{-\frac{x}{\theta}}dx \overset{y=x/\theta}{=}  \int_0^\infty y e^{-y} \theta dy\\
        &amp;= \theta \underbrace{\int_0^\infty y e^{-y}   dy}_{=\Gamma(2)}= \theta \Gamma(2) = \theta \cdot (1!) =\theta
    \end{align*}\]</span>
and similarly
<span class="math display">\[\begin{align*}
        E[X^2]&amp;= \int_0^\infty x^2 \cdot \frac{1}{\theta} e^{-\frac{x}{\theta}}dx\overset{y=x/\theta}{=}  \int_0^\infty \theta y^2 e^{-y} \theta dy\\
        &amp;= \theta^2 \underbrace{\int_0^\infty y^{3-1} e^{-y}   dy}_{=\Gamma(3)}= \theta^2 \Gamma(3) = \theta \cdot (2!) =2\theta^2
    \end{align*}\]</span>
so that
<span class="math display">\[ Var(X) = E[X^2]-E[X]^2=2\theta^2-\theta^2 =\theta^2\]</span></p>
<div id="memoryless-property-of-exponential-distribution" class="section level3 hasAnchor" number="25.1.1">
<h3 class="hasAnchor"><span class="header-section-number">25.1.1</span> Memoryless property of exponential distribution<a href="#memoryless-property-of-exponential-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:unlabeled-div-69" class="theorem"><strong>(#thm:unlabeled-div-69) (Memoryless property) </strong></span>If <span class="math inline">\(X \sim Exp(\theta)\)</span>, then
<span class="math display">\[
        P(X &gt; s + t | X &gt; s) = P(X &gt; t).
        \]</span></p>
</div>
<ul>
<li>We’ve seen the memoryless property for the <span class="math inline">\(Geo(p)\)</span> earlier (and the geometric distribution is the only discrete distribution with this property)</li>
<li>If a continuous random variable has memoryless property, it must follow exponential distribution.</li>
<li>Intuitively, both the geometric and exponential distributions measure waiting time until first success</li>
</ul>
<div class="proof">
<p><span id="unlabeled-div-70" class="proof"><em>Proof</em>. </span>Recall the cdf of <span class="math inline">\(X\sim Exp(\theta)\)</span> is
<span class="math display">\[ F(x)=P(X\leq x) =\int_{-\infty}^x f(t)dt = \int_0^x \theta^{-1} e^{-t/\theta}dt = 1-\exp(-x/\theta)\]</span>
for <span class="math inline">\(x&gt;0\)</span> and 0 otherwise. Hence,
<span class="math display">\[\begin{align*}
       P(X &gt; s + t | X &gt; s) &amp;= \frac{P( X&gt;s+t \text{ and }X&gt;s)}{P(X&gt;s)}\\
       &amp;= \frac{P(X&gt;s+t)}{P(X&gt;s)} = \frac{e^{-(s+t)/\theta}}{e^{-s/\theta}}\\
       &amp;= e^{-t/\theta} = 1-F(t) = P(X&gt;t)
   \end{align*}\]</span>
as desired.</p>
</div>
<!--chapter:end:24-Lec24.Rmd-->
</div>
</div>
</div>
<div id="lecture-24-march-11-2024" class="section level1 hasAnchor" number="26">
<h1 class="hasAnchor"><span class="header-section-number">26</span> Lecture 24, March 11, 2024<a href="#lecture-24-march-11-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="sampling-realizations-of-random-variables" class="section level2 hasAnchor" number="26.1">
<h2 class="hasAnchor"><span class="header-section-number">26.1</span> Sampling realizations of random variables<a href="#sampling-realizations-of-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>In practice, we may often want to sample realizations of random variables, and use those realizations to conduct some estimation of inference (<span class="math inline">\(\Rightarrow\)</span> Monte Carlo Simulation)</li>
<li>For instance, suppose we wish to estimate (approximate) the probability <span class="math inline">\(P(X&gt;2)\)</span> for <span class="math inline">\(X\sim Exp(1)\)</span> by simulation
<ul>
<li>of course, we can compute <span class="math inline">\(P(X&gt;2)\)</span> by hand, but let’s pretend we can’t.</li>
<li>In this case, could sample independent realizations <span class="math inline">\(x_1,\dots,x_n\)</span> from <span class="math inline">\(Exp(1)\)</span> (numbers that look like realizations <span class="math inline">\(X(\omega_1),\dots,X(\omega_n)\)</span>), and then estimate
<span class="math display">\[P(X&gt;2)\approx \frac{|\{x_1,\dots,x_n: x_i&gt;2\}|}{n}\]</span>
that is, we estimate <span class="math inline">\(P(X&gt;2)\)</span> by the relative frequency of observations larger than 2.</li>
</ul></li>
</ul>
</div>
<div id="inversion-method" class="section level2 hasAnchor" number="26.2">
<h2 class="hasAnchor"><span class="header-section-number">26.2</span> Inversion method<a href="#inversion-method" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="with-strictly-increasing-and-continuous-assumption" class="section level3 hasAnchor" number="26.2.1">
<h3 class="hasAnchor"><span class="header-section-number">26.2.1</span> With strictly increasing and continuous assumption<a href="#with-strictly-increasing-and-continuous-assumption" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="lemma">
<p><span id="lem:unlabeled-div-71" class="lemma"><strong>(#lem:unlabeled-div-71) </strong></span>If <span class="math inline">\(F\)</span> is a continuous and strictly increasing cdf of some random variable <span class="math inline">\(X\)</span> and if <span class="math inline">\(U\sim Unif(0,1)\)</span>, then the random variable <span class="math inline">\(Y=F^{-1}(U)\)</span> has cdf <span class="math inline">\(F\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-72" class="proof"><em>Proof</em>. </span>Denote by <span class="math inline">\(F_Y\)</span> the cdf of the random variable <span class="math inline">\(Y = F^{-1}(U)\)</span>. Then,
<span class="math display">\[ F_Y(y) = P(F^{-1}(U)\leq x) = P(F(F^{-1}(U))\leq F(y))=P(U\leq F(y)).\]</span>
But for any <span class="math inline">\(u\in[0,1]\)</span>, we know that <span class="math inline">\(P(U\leq u)=u\)</span>, since <span class="math inline">\(U\sim U(0,1)\)</span>. Hence,
<span class="math display">\[ F_Y(x) = P(U\leq F(y))=F(y)\]</span>
Hence, the random variable <span class="math inline">\(Y = F^{-1}(U)\)</span> has the cdf <span class="math inline">\(F\)</span>, as desired.</p>
</div>
</div>
<div id="more-general-case-using-the-quantile" class="section level3 hasAnchor" number="26.2.2">
<h3 class="hasAnchor"><span class="header-section-number">26.2.2</span> More general case using the quantile<a href="#more-general-case-using-the-quantile" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By using the more general definition of the quantile function
<span class="math display">\[F^{-1}(y) = \inf\{x\in\mathbb{R}: F(x)\geq y\}\]</span>
one can show the following generalization:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-73" class="theorem"><strong>(#thm:unlabeled-div-73) </strong></span>Let <span class="math inline">\(F\)</span> be any cumulative distribution function of some random variable <span class="math inline">\(X\)</span> and <span class="math inline">\(U\sim U(0,1)\)</span>. Then the random variable <span class="math inline">\(F^{-1}(U)\)</span> has cdf <span class="math inline">\(F\)</span>.</p>
</div>
<ul>
<li>No matter what cdf <span class="math inline">\(F\)</span> (discrete or continuous), we can sample observations as follows:
<ol style="list-style-type: lower-alpha">
<li>Sample <span class="math inline">\(U\sim U(0,1)\)</span> (eg via )</li>
<li>Return <span class="math inline">\(X=F^{-1}(U)\)</span>.</li>
</ol></li>
<li>Repeating this <span class="math inline">\(n\)</span> times independently gives <span class="math inline">\(n\)</span> realizations from <span class="math inline">\(F\)</span>.</li>
</ul>
</div>
</div>
<div id="normalgaussian-distribution" class="section level2 hasAnchor" number="26.3">
<h2 class="hasAnchor"><span class="header-section-number">26.3</span> Normal/Gaussian distribution<a href="#normalgaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Gaussian distribution is named as Gauss, and is perhaps one of the most important distribution, if not the most important one.</p>
<div class="definition">
<p><span id="def:unlabeled-div-74" class="definition"><strong>(#def:unlabeled-div-74) </strong></span><span class="math inline">\(X\)</span> is said to have a (or Gaussian distribution) with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span> if the density of <span class="math inline">\(X\)</span> is
<span class="math display">\[
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{\frac{-(x-\mu)^2}{2\sigma^2}}, \;\;\; x\in {\mathbb R}.
\]</span>
We denote it by <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span>.</p>
</div>
<div id="properties-of-gaussian-distribution" class="section level3 hasAnchor" number="26.3.1">
<h3 class="hasAnchor"><span class="header-section-number">26.3.1</span> Properties of Gaussian distribution<a href="#properties-of-gaussian-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>1/ Symmetric about its mean: If <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span></p>
<p><span class="math display">\[
P( X \le \mu - t) = P(X \ge \mu + t).
\]</span>
2. Density is unimodal: Peak is at <span class="math inline">\(\mu\)</span>.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Mean and Variance are the parameters:
<span class="math display">\[E(X)= \mu\]</span>
and
<span class="math display">\[Var(X)=\sigma^2.\]</span></p></li>
<li><p><span class="math inline">\(N(\mu, \sigma^2)\)</span> is sometimes (STAT 231) also parametrised as Gaussian distribution using <span class="math inline">\(\sigma\)</span> instead of <span class="math inline">\(\sigma^2\)</span>, where
<span class="math display">\[
X \sim G(\mu, \sigma).
\]</span>
That is, <span class="math inline">\(X\sim N(1, 4)\)</span> and <span class="math inline">\(X\sim G(1, 2)\)</span> mean the same thing.</p></li>
<li><p>Median = mean = mode = first moment.</p></li>
</ol>
</div>
<div id="problem" class="section level3 hasAnchor" number="26.3.2">
<h3 class="hasAnchor"><span class="header-section-number">26.3.2</span> Problem<a href="#problem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is difficult to calculate the CDF! If <span class="math inline">\(X\sim N(\mu, \sigma^2)\)</span>, then
<span class="math display">\[
P(a \le X \le b ) = \int_a^b \frac{1}{\sqrt{2\pi \sigma^2}} e^{\frac{-(x-\mu)^2}{2\sigma^2}} dx = ???
\]</span>
functions of the form <span class="math inline">\(e^{-x^2}\)</span> do not have elementary anti-derivatives… :(</p>
<!--chapter:end:25-Lec25.Rmd-->
</div>
</div>
</div>
<div id="lecture-26-march-13-2024" class="section level1 hasAnchor" number="27">
<h1 class="hasAnchor"><span class="header-section-number">27</span> Lecture 26, March 13, 2024<a href="#lecture-26-march-13-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="normal-distribution-continue" class="section level2 hasAnchor" number="27.1">
<h2 class="hasAnchor"><span class="header-section-number">27.1</span> Normal distribution continue<a href="#normal-distribution-continue" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If we have a random variable follows an arbitrary Gaussian distribution, i.e. <span class="math inline">\(X\sim \mathcal{N}(\mu,\sigma^2)\)</span>. How do we obtain the CDF and quantile? It turns out that we can standardize/transform the RV <span class="math inline">\(X\)</span> to <span class="math inline">\(Z\sim\mathcal{N}(0,1)\)</span>.</p>
<div id="standard-normal-distribution" class="section level3 hasAnchor" number="27.1.1">
<h3 class="hasAnchor"><span class="header-section-number">27.1.1</span> Standard normal distribution<a href="#standard-normal-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="defintion" name="Standard normal">
<p>We say that <span class="math inline">\(Z\)</span> follows the standard normal distribution if <span class="math inline">\(Z \sim \mathcal{N}(0,1)\)</span>.</p>
</div>
<p>Frequently in probability and statistics literature, the density of the standard normal random variable is denoted</p>
<p><span class="math display">\[
\varphi(x)= \frac{1}{\sqrt{2\pi}}e^{\frac{-x^2}{2}},
\]</span></p>
<p>and the cdf of a standard normal random variable is denoted</p>
<p><span class="math display">\[
\Phi(x)= \int_{-\infty}^x  \frac{1}{\sqrt{2\pi}}e^{\frac{-y^2}{2}}dy.
\]</span></p>
<p>Aside: Why is there <span class="math inline">\(1/\sqrt{2 \pi}\)</span> in the pdf? That’s because
<span class="math display">\[
    \int_{-\infty}^\infty e^{\frac{-x^2}{2}} dx = \sqrt{2\pi},
    \]</span>
so if we divide <span class="math inline">\(e^{\frac{-x^2}{2}}\)</span> by the integral by <span class="math inline">\(\sqrt{2 \pi}\)</span> we obtain a valid pdf (non-negative, integrates to 1).</p>
<p>Values of <span class="math inline">\(\Phi(z)=P(Z\leq z)=\int_{-\infty}^z \varphi(t)dt\)</span> can be approximated numerically with high accuracy. Here we can either use a function in R, <code>pnorm()</code>, or use the <strong>z-table</strong>.</p>
<p>Now we know how to computer the CDF for <span class="math inline">\(\mathcal{N}(0,1)\)</span>, how to link it to <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>?</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-75" class="theorem"><strong>(#thm:unlabeled-div-75) (Standardising normal random variable) </strong></span></p>
<pre><code> If $X \sim N(\mu, \sigma^2)$, then 
    $$
    Z = \frac{X - \mu}{\sigma} \sim \N(0,1),
    $$
    and $P(X \leq x) = P\left(Z \leq \dfrac{x - \mu}{\sigma}\right)$.</code></pre>
</div>
</div>
<div id="procedure" class="section level3 hasAnchor" number="27.1.2">
<h3 class="hasAnchor"><span class="header-section-number">27.1.2</span> Procedure<a href="#procedure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>for computing <span class="math inline">\(P(X\leq x)\)</span> for <span class="math inline">\(X\sim N(\mu,\sigma^2)\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Compute <span class="math inline">\(z=\frac{x-\mu}{\sigma}\)</span> (``z-score’’).</li>
<li>Find <span class="math inline">\(\Phi(z)=P(Z\leq z)\)</span> in the table where <span class="math inline">\(Z\sim N(0,1)\)</span>.</li>
<li>Return <span class="math inline">\(P(X\leq x)=\Phi(z)\)</span>.</li>
</ol>
</div>
<div id="quantile-1" class="section level3 hasAnchor" number="27.1.3">
<h3 class="hasAnchor"><span class="header-section-number">27.1.3</span> Quantile<a href="#quantile-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Z-table can also be used to obtain percentiles and quantiles.</p>
<ul>
<li>Let <span class="math inline">\(Z\sim N(0,1)\)</span> and <span class="math inline">\(p\in(0,1)\)</span>. Then we can find the value <span class="math inline">\(z_p\)</span> so that <span class="math inline">\(P(Z\leq z_q)=\Phi(z_p)=p\)</span> either by
<ul>
<li><span class="math inline">\(\dots\)</span> looking at the top of the <span class="math inline">\(z\)</span>-table and selecting the value <span class="math inline">\(z_p\)</span> so that <span class="math inline">\(\Phi(z_p)\)</span> is closest to <span class="math inline">\(p\)</span></li>
<li><span class="math inline">\(\dots\)</span> looking at the bottom of the table, which directly gives the quantile for selected <span class="math inline">\(p\geq 0.5\)</span> (for <span class="math inline">\(p&lt;0.5\)</span>, use <span class="math inline">\(\Phi^{-1}(p)=-\Phi^{-1}(1-p)\)</span>). This is .</li>
</ul></li>
<li>If <span class="math inline">\(X \sim \mathcal{N}(\mu,\sigma^2)\)</span> and we want <span class="math inline">\(x_p\)</span> so that <span class="math inline">\(P(X\leq x_p)=p\)</span>
<ul>
<li>First find <span class="math inline">\(z_p\)</span> for the standard normal distribution <span class="math inline">\(N(0,1)\)</span>.</li>
<li>Second, set <span class="math inline">\(x_p= \mu + \sigma z_p\)</span>. Then
<span class="math display">\[P(X\leq x_p) = P(X\leq \mu+\sigma z_p)=P\left( \underbrace{(X-\mu)/\sigma}_{\sim N(0,1)} \leq z_p\right)=\Phi(z_p)=p\]</span></li>
</ul></li>
</ul>
</div>
<div id="rule-for-1-2-3-standard-deviations" class="section level3 hasAnchor" number="27.1.4">
<h3 class="hasAnchor"><span class="header-section-number">27.1.4</span> 68-95-99.7 rule for 1-2-3 standard deviation(s)<a href="#rule-for-1-2-3-standard-deviations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An interesting empirical rule about normal distribution is the <strong>68-95-99.7</strong> rule, which states:</p>
<p>If <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, then
<span class="math display">\[
P( \mu - \sigma \le X \le \mu + \sigma) \approx 0.68
\]</span>
<span class="math display">\[
P( \mu - 2\sigma \le X \le \mu + 2\sigma) \approx 0.95
\]</span>
<span class="math display">\[
P( \mu - 3\sigma \le X \le \mu + 3\sigma) \approx  0.997.
\]</span></p>
<!--chapter:end:26-Lec26.Rmd-->
</div>
</div>
</div>
<div id="lecture-27-march-15-2024" class="section level1 hasAnchor" number="28">
<h1 class="hasAnchor"><span class="header-section-number">28</span> Lecture 27, March 15, 2024<a href="#lecture-27-march-15-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Recap:</p>
<p>In previous lecture, we discussed about the relationship between a standard normal <span class="math inline">\(\mathcal{N}(0,1)\)</span> and an arbitrary normal <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>. The steps are as fellows.</p>
<ol style="list-style-type: decimal">
<li><p>Transform/standardize <span class="math inline">\(X\sim\mathcal{N}(\mu,\sigma^2)\)</span> to <span class="math inline">\(Z\sim\mathcal{N}(0,1)\)</span> by let <span class="math inline">\(Z=\frac{x-\mu}{\sigma}\)</span>.</p></li>
<li><p>Use the <code>z-table</code> or the <em>R</em> <code>pronrm()</code> and <code>qnorm()</code> functions.</p></li>
<li><p>If you use the <code>z-table</code> in this class, we only provide the standard normal CDF for <span class="math inline">\(Z&gt;0\)</span>, so you may want to use the symmetric property of the normal distribution when you are asked to calculate the CDF for <span class="math inline">\(P(Z\le z)\)</span> where <span class="math inline">\(z&lt;0\)</span>.</p></li>
</ol>
<p>The other things to remember is the 68-95-99.7 rule for <span class="math inline">\(\pm\)</span> 1,2, and 3 standard deviation away from the mean <span class="math inline">\(\mu\)</span>.</p>
<div id="chapter-9-multivariate-distributions" class="section level2 hasAnchor" number="28.1">
<h2 class="hasAnchor"><span class="header-section-number">28.1</span> Chapter 9: Multivariate distributions<a href="#chapter-9-multivariate-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Often, we may face problems that may have have multiple factors/covariates/features/causes. In those situations, only using <strong>one</strong> random variable is not sufficient to model the outcome. Hence, we need two or more random variables in those situations. When we have more than one random variables, we say we have a <strong>multivariate distribution</strong>.</p>
<p>Q: How to extend what we have learned from univariate (i.e. one variable) to the multivariate case?</p>
<p>A: We can extend the definitions we have before from the univariate case to the multivariate case!</p>
<div class="definition">
<p><span id="def:unlabeled-div-76" class="definition"><strong>(#def:unlabeled-div-76) (Random Vector) </strong></span>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be random variables defined on a common probability space. The vector <span class="math inline">\((X_1,\dots,X_n)\)</span> is called a <strong>random vector</strong>.</p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-77" class="definition"><strong>(#def:unlabeled-div-77) (Joint Distribution) </strong></span>Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <em>discrete</em> random variables defined on the same sample space (in general, when we consider two or more random variables it is assumed they are defined on the same sample space.)</p>
<p>The <strong>joint probability function</strong> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is
<span class="math display">\[
f(x,y) = P( \{\omega\in S:X(\omega)=x\} \cap \{\omega\in S:Y(\omega)=y\} )
\]</span>
for <span class="math inline">\(x\in X(S),y\in Y(S)\)</span> and 0 otherwise.</p>
<p>As in the univariate case, a shorthand notation for this is
<span class="math display">\[
f(x,y) = P(X=x,Y=y).
\]</span></p>
</div>
<p>Q: What if we have more than two random variables?</p>
<p>For a collection of <span class="math inline">\(n\)</span> discrete random variables, <span class="math inline">\(X_1,...,X_n\)</span>, the joint probability function is defined as</p>
<p><span class="math display">\[
f(x_1,x_2,...,x_n)=P(X_1=x_1,X_2=x_2,...,X_n=x_n).
\]</span></p>
<p>and we call the vector <span class="math inline">\((X_1,\dots,X_n)\)</span> a random vector.</p>
<div id="properties-of-the-joint-probability-function" class="section level3 hasAnchor" number="28.1.1">
<h3 class="hasAnchor"><span class="header-section-number">28.1.1</span> Properties of the joint probability function<a href="#properties-of-the-joint-probability-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let <span class="math inline">\(f(x,y)\)</span> be a joint probability function. Then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(0\leq f(x,y) \le 1\)</span></li>
<li><span class="math inline">\(\sum_{x,y} f(x,y) = 1\)</span>.</li>
</ol>
</div>
<div id="marginal-distribution-of-the-joint-distribution-function" class="section level3 hasAnchor" number="28.1.2">
<h3 class="hasAnchor"><span class="header-section-number">28.1.2</span> Marginal distribution of the joint distribution function<a href="#marginal-distribution-of-the-joint-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The joint probability function <span class="math inline">\(f(x,y)\)</span> gives us probabilities <span class="math inline">\(P(X=x, Y=y)\)</span>. What if we just care about the probability <span class="math inline">\(P(X=x)\)</span> (without caring about <span class="math inline">\(Y\)</span>?)</p>
<div class="definition">
<p><span id="def:unlabeled-div-78" class="definition"><strong>(#def:unlabeled-div-78) (Marginal probability function) </strong></span>Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are {} random variables with joint probability function <span class="math inline">\(f(x,y)\)</span>.</p>
<p>The <strong>marginal probability function</strong> of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[
f_X(x) = P( X=x )= \sum_{y \in Y(S)} f(x,y).
\]</span>
Similarly, the marginal probability function of <span class="math inline">\(Y\)</span> is
<span class="math display">\[
f_Y(y) = P( Y=y )= \sum_{x \in X(S)} f(x,y).
\]</span></p>
</div>
</div>
<div id="comments" class="section level3 hasAnchor" number="28.1.3">
<h3 class="hasAnchor"><span class="header-section-number">28.1.3</span> Comments<a href="#comments" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><p>A common mistake is to think that there is a difference between the marginal distribution of <span class="math inline">\(X\)</span> and the probability function of <span class="math inline">\(X\)</span>. They are the same!</p></li>
<li><p>When we only have one random variable, say <span class="math inline">\(X\)</span>, you may write the CDF and PF as <span class="math inline">\(f\)</span>, <span class="math inline">\(F\)</span> to represent <span class="math inline">\(f_X\)</span> and <span class="math inline">\(F_X\)</span>. However, if you have two or more random variables, say if two, denote by <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Then we <strong>NEED</strong> to write the subscript, <span class="math inline">\(f_X\)</span>, <span class="math inline">\(f_Y\)</span>, <span class="math inline">\(F_X\)</span> and <span class="math inline">\(F_Y\)</span> to distinguish which random variable you are mentioning.</p></li>
</ul>
<!--chapter:end:27-Lec27.Rmd-->
</div>
</div>
</div>
<div id="lecture-28-march-18-2024" class="section level1 hasAnchor" number="29">
<h1 class="hasAnchor"><span class="header-section-number">29</span> Lecture 28, March 18, 2024<a href="#lecture-28-march-18-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In the previous lecture, we introduced the <strong>multivariate</strong> case (i.e. to have more than one random variables). This lecture, we want to see how some concepts we saw before can be used here.</p>
<div class="definition">
<p><span id="def:unlabeled-div-79" class="definition"><strong>(#def:unlabeled-div-79) (Independence between random variables) </strong></span><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent</strong> random variables if
<span class="math display">\[
f(x,y) = f_X(x) f_Y(y)
\]</span>
for all values of <span class="math inline">\((x,y)\)</span>.</p>
<p>More generally, <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> are <strong>independent</strong> if
<span class="math display">\[
f(x_1, x_2, \ldots, x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n)
\]</span>
for all values of <span class="math inline">\((x_1, \ldots, x_n)\)</span>.</p>
</div>
<p><strong>Not</strong>e: This means that if you find a <strong>single realization</strong> of <span class="math inline">\((x_1, \ldots, x_n)\)</span> values that doesn’t satisfy the above equation, then <span class="math inline">\(X_1, \ldots, X_n\)</span> are not independent.</p>
<div id="conditional-probability-function-for-multivaraite-random-variable" class="section level3 hasAnchor" number="29.0.1">
<h3 class="hasAnchor"><span class="header-section-number">29.0.1</span> Conditional probability function for multivaraite random variable<a href="#conditional-probability-function-for-multivaraite-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="definition">
<p><span id="def:unlabeled-div-80" class="definition"><strong>(#def:unlabeled-div-80) (conditional probability function for bivariate case) </strong></span>The <strong>conditional probability function</strong> of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y=y\)</span> is denoted <span class="math inline">\(f_X(x|y)\)</span>, and is defined to be
<span class="math display">\[
f_X(x|y)= P(X=x|Y=y) = \frac{P(X=x,Y=y)}{P(Y=y)} = \frac{f(x,y)}{f_Y(y)},
\]</span>
Given that <span class="math inline">\(f_Y(y) &gt; 0\)</span>.
<span class="math inline">\(f_Y(y|x)\)</span> can be defined similarly.</p>
</div>
</div>
<div id="probably-function-of-ugx_1cdotsx_n" class="section level3 hasAnchor" number="29.0.2">
<h3 class="hasAnchor"><span class="header-section-number">29.0.2</span> Probably function of <span class="math inline">\(U=g(X_1,\cdots,X_n)\)</span><a href="#probably-function-of-ugx_1cdotsx_n" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With multiple random variables, we are even more interested in <em>functions</em> of such variables. For example, Let <span class="math inline">\(A, M, F\)</span> be the random variables for your assignment, midterm and final grades respectively. Then, it’s natural to consider the overall grade <span class="math inline">\(G = g(A,M,F)\)</span> as a function of random variables <span class="math inline">\(A, M, F\)</span>.</p>
<p>In general, we have the following formula for the <strong>probability function of <span class="math inline">\(U = g(X_1, X_2, \ldots, X_n).\)</span></strong>
<span class="math display">\[
P(U = u) = \sum_{\substack{(x_1, \ldots x_n) \text{ such that }\\ g(x_1, \ldots, x_n) = u}} f(x_1, \ldots, x_n)
\]</span>
Of course, we restrict ourselves to <span class="math inline">\((x_1,\dots,x_n)\)</span> in the range of <span class="math inline">\((X_1,\dots,X_n)\)</span> and omit terms with <span class="math inline">\(f(x_1, \ldots, x_n)=0\)</span></p>
</div>
<div id="known-results-for-named-distributions" class="section level3 hasAnchor" number="29.0.3">
<h3 class="hasAnchor"><span class="header-section-number">29.0.3</span> Known results for named distributions<a href="#known-results-for-named-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:unlabeled-div-81" class="theorem"><strong>(#thm:unlabeled-div-81) (Sum of independent Poisson is Poisson) </strong></span>If <span class="math inline">\(X \sim Poi(\lambda_1)\)</span> and <span class="math inline">\(Y \sim Poi(\lambda_2)\)</span> independently, then <span class="math inline">\(T = X + Y \sim Poi(\lambda_1 + \lambda_2)\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-82" class="proof"><em>Proof</em>. </span><span class="math display">\[\begin{align*}
    P(X+Y=t) &amp;= \sum_{(x,y):x+y= t} P(X=x, Y=y) \\
    &amp;= \sum_{x=0}^t P(X=x, Y=t-x)\\
    &amp;=   \sum_{x=0}^t  \underbrace{f(x,t-x)}_{=f_X(x)f_Y(t-x)}\\
    &amp; =\sum_{x=0}^t e^{-\lambda_1} \frac{\lambda_1^x}{x!}e^{-\lambda_2} \frac{\lambda_2^{t-x}}{(t-x)!}  \dots\end{align*}\]</span>
rest is <em>exercise</em> and in the course notes.</p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-83" class="theorem"><strong>(#thm:unlabeled-div-83) (Sum of 2 independent binomial is binomial) </strong></span>If <span class="math inline">\(X \sim Bin(n, p)\)</span> and <span class="math inline">\(Y \sim Bin(m, p)\)</span> independently, then <span class="math inline">\(T = X + Y \sim Bin(n + m, p)\)</span></p>
</div>
<p>Proof is left for an exercise.</p>
<p>This can be easily extended to the <span class="math inline">\(n\)</span>-case.</p>
<div class="theorem" names="Sum of n independent binomial is binomial">
<p><span id="thm:unlabeled-div-84" class="theorem"><strong>(#thm:unlabeled-div-84) </strong></span>Let <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span> each follow <span class="math inline">\(Bernoulli(p)\)</span> independently. Then,
<span class="math display">\[
X_1 + X_2 + \ldots + X_n \sim Bin(n,p).
\]</span></p>
</div>
<p>This should not come as a surprise as it is the meaning of binomial distribution – run <span class="math inline">\(n\)</span> independent Bernoulli trials with the same probability of success <span class="math inline">\(p\)</span>. Similar, we may have the same relationship between the Negative binomial distribution and the geometric distribution.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-85" class="theorem"><strong>(#thm:unlabeled-div-85) (Sum of n independent geometric is Negative binomial) </strong></span>Let <span class="math inline">\(X_1, X_2, \ldots, X_k\)</span> each follow <span class="math inline">\(Geo(p)\)</span> independently. Then,
<span class="math display">\[
X_1 + X_2 + \ldots + X_k \sim NB(k,p).
\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-86" class="theorem"><strong>(#thm:unlabeled-div-86) (Conditional distribution of two independent poisson) </strong></span>Let <span class="math inline">\(X \sim Poi(\lambda_1)\)</span> and <span class="math inline">\(Y \sim Poi(\lambda_2)\)</span> independently. Then, given <span class="math inline">\(X + Y = n\)</span>, <span class="math inline">\(X\)</span> follows binomial distribution. That is,
<span class="math display">\[
X|X+Y = n \sim Bin\left(n, \frac{\lambda_1}{\lambda_1 + \lambda_2}\right).
\]</span>
Similarly, for <span class="math inline">\(Y\)</span>, we have
<span class="math display">\[
Y|X+Y = n \sim Bin\left(n, \frac{\lambda_2}{\lambda_1 + \lambda_2}\right).
\]</span></p>
</div>
<p>The proof is left as an exercise. Hint is to use <span class="math inline">\(X+Y\sim Poi(\lambda_1+\lambda_2)\)</span>.</p>
<!--chapter:end:28-Lec28.Rmd-->
</div>
</div>
<div id="lecture-29-march-20-2024" class="section level1 hasAnchor" number="30">
<h1 class="hasAnchor"><span class="header-section-number">30</span> Lecture 29, March 20, 2024<a href="#lecture-29-march-20-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this lecture, we are going to introduce a named multivariate distribution – the <em>multinomial distribution</em>.</p>
<div id="multinomial-distribution" class="section level2 hasAnchor" number="30.1">
<h2 class="hasAnchor"><span class="header-section-number">30.1</span> Multinomial distribution<a href="#multinomial-distribution" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The multinomial distribution is a generalization of the binomial distribution.</p>
<div class="definition">
<p><span id="def:unlabeled-div-87" class="definition"><strong>(#def:unlabeled-div-87) (Multinomial distribution) </strong></span>Consider an experiment in which:</p>
<p>In this case we say <span class="math inline">\(X_1,...,X_k\)</span> have a <strong>Multinomial distribution</strong> with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p_1,...,p_k\)</span>. We use the notation <span class="math inline">\((X_1,...,X_k) \sim Mult(n,p_1,...,p_k)\)</span>.</p>
</div>
<p><strong>Note</strong>: The binomial distribution has only 2 class/categories, i.e. the success/failture, head/tail…with the associated probability <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2=(1-p_1)\)</span> Whereas in the multinomial distribution, we have <span class="math inline">\(n\)</span>-different classes/categories, <span class="math inline">\(1,2,\cdots,n\)</span>, with the associated probabilities <span class="math inline">\(p_1,p_2,\cdots,p_n=1-\sum_{i=1}^{n-1}p_i\)</span>.</p>
<div id="the-probability-distribution-function" class="section level3 hasAnchor" number="30.1.1">
<h3 class="hasAnchor"><span class="header-section-number">30.1.1</span> The probability distribution function<a href="#the-probability-distribution-function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(X_1,...,X_k\)</span> have a joint multinomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p_1,...,p_k\)</span>, then their <strong>joint probability function</strong> is</p>
<p><span class="math display">\[
f(x_1,...,x_k) = \frac{n!}{x_1!x_2!\cdots x_k!} p_1^{x_1}\cdots p_k^{x_k},
\]</span></p>
<p>where <span class="math inline">\(x_1,...,x_k\)</span> satisfy <span class="math inline">\(x_1+\cdots+x_k = n\)</span>, <span class="math inline">\(x_i \ge 0\)</span>. \
</p>
<p>The terms</p>
<p><span class="math display">\[
\binom{n}{x_1, x_2, \cdots,x_k} := \frac{n!}{x_1!x_2!\cdots x_k!}, \quad\text{where } x_1+\cdots+x_k = n,
\]</span>
are called the <strong>multinomial coefficients</strong></p>
<p>Just like the binomial coefficient where we can use one variable to two outcomes/class/categories, we can write the multinomial with <span class="math inline">\(k\)</span>-class as <span class="math inline">\(k-1\)</span> random variables:</p>
<p>Multinomial distribution over <span class="math inline">\((X_1, \ldots, X_k)\)</span> can also be written in terms of <span class="math inline">\(k-1\)</span> variables.</p>
<p>If <span class="math inline">\(x_1 + \ldots + x_n = n\)</span>, and we know <span class="math inline">\(x_1, \ldots x_{k-1}\)</span>, then we can let
<span class="math display">\[
x_k = n - x_1 - x_2 - \ldots - x_{k-1}.
\]</span></p>
<p>Similarly, we can let
<span class="math display">\[
p_k = 1 - p_1 - p_2 - \ldots - p_{k-1}.
\]</span></p>
<p>Thus, we can write the probability function of <span class="math inline">\(Mult(n, p_1, \ldots, p_k)\)</span> as
<span class="math display">\[
f(x_1,...,x_{k-1}) = \frac{n! p_1^{x_1}\cdots p_{k-1}^{x_{k-1}} \left(1 - \sum_{i=1}^{k-1}p_i\right)^{n - \sum_{i=1}^{k-1} x_i}}{x_1!x_2!\cdots x_{k-1}! (n - \sum_{i=1}^{k-1} x_i)!}
\]</span>
### Marginal ditribution and associated probability</p>
<p>We often wonder what is the marginal distribution of a multivariate joint distribution. So what’s the marginal distribution of the multinomial random variable <span class="math inline">\((X_1, X_2, \ldots, X_k)\)</span>?</p>
<p>::: {.theorem name=Marginal distribution of multinomial”}
Let <span class="math inline">\((X_1, X_2, \ldots, X_k) \sim Mult(n, p_1, \ldots, p_k)\)</span>.
Then,
<span class="math display">\[
        X_j \sim Bin(n, p_j),
        \]</span>
for <span class="math inline">\(j = 1, 2, \ldots, k\)</span>.
:::
This follows either by construction (since <span class="math inline">\(X_j\)</span> counts the number of successes in <span class="math inline">\(n\)</span> independent trials with constant success prob <span class="math inline">\(p_j\)</span>, or by computing the sum
<span class="math display">\[ P(X_j=x_j)=f_j(x_j)= \sum\limits_{\text{all }x_1,x_2,\dots,x_{j-1},x_{j+1},\dots,x_k} f(x_1,\dots,x_{j-1},x_j,\x_{j+1},\dots, x_k)\]</span>
which is illustrated in the case <span class="math inline">\(k=3\)</span> in the course notes.</p>
<p>We can also write out the marginal distribution for 2 random variables in a multinomial distribution:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-88" class="theorem"><strong>(#thm:unlabeled-div-88) (Sum of individual rvs in multinomial) </strong></span>Let <span class="math inline">\((X_1, X_2, \ldots, X_k) \sim Mult(n, p_1, \ldots, p_k)\)</span>. Then,
<span class="math display">\[
        X_i + X_j \sim Bin(n, p_i + p_j),
        \]</span>
for <span class="math inline">\(i \neq j\)</span>.</p>
</div>
<p>Again, the rationale behind it is either by construction (since <span class="math inline">\(X_i+X_j\)</span> counts the number of successes in <span class="math inline">\(n\)</span> independent trials with constant success prob <span class="math inline">\(p_i+p_j\)</span>,
or by computing the sum
<span class="math display">\[ P(X_i + X_j=t)= \sum\limits_{\text{all }x_1,x_2,\dots,x_k:x_i+x_j=t} f(x_1,\dots, x_k)\]</span>
### Conditional distribution of multinomial</p>
<p>The other things that we are often interested in, when we have multivariate distribution, is the conditional distribution. For multinomial distribution, we have the following theorem:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-89" class="theorem"><strong>(#thm:unlabeled-div-89) (Conditional distribution of multinomial) </strong></span>Let <span class="math inline">\((X_1, X_2, \ldots, X_k) \sim Mult(n, p_1, \ldots, p_k)\)</span>.
Then,
<span class="math display">\[
X_i|X_i + X_j = t \sim Bin\left(t, \frac{p_i}{p_i + p_j}\right),
\]</span>
for <span class="math inline">\(i \neq j\)</span>.</p>
</div>
<!--chapter:end:29-Lec29.Rmd-->
</div>
</div>
</div>
<div id="lecture-30-march-22-2024" class="section level1 hasAnchor" number="31">
<h1 class="hasAnchor"><span class="header-section-number">31</span> Lecture 30, March 22, 2024<a href="#lecture-30-march-22-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this lecture, we will finish up where we left over – the properties of multinomial distribution.</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-90" class="theorem"><strong>(#thm:unlabeled-div-90) (Conditional distribution of multinomial) </strong></span>Let <span class="math inline">\((X_1, X_2, \ldots, X_k) \sim Mult(n, p_1, \ldots, p_k)\)</span>.
Then,
<span class="math display">\[
X_i|X_i + X_j = t \sim Bin\left(t, \frac{p_i}{p_i + p_j}\right),
\]</span>
for <span class="math inline">\(i \neq j\)</span>.</p>
</div>
<div id="summary-statistics-of-multivariate-distributions" class="section level3 hasAnchor" number="31.0.1">
<h3 class="hasAnchor"><span class="header-section-number">31.0.1</span> Summary statistics of multivariate distributions<a href="#summary-statistics-of-multivariate-distributions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Akin the univaraite distributions, we want to use the moments to <em>summaries</em> the multivariate distributions. First, we define the Mulvariate Law of Unconciouss Statistician (mLOTUS).</p>
<div class="definition">
<p><span id="def:unlabeled-div-91" class="definition"><strong>(#def:unlabeled-div-91) (Bivariate LOTUS) </strong></span>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are discrete random variables with joint probability function <span class="math inline">\(f(x,y)\)</span>. Then for a function <span class="math inline">\(g: {\mathbb R}^2 \to {\mathbb R}\)</span>,</p>
<p><span class="math display">\[
\mbox{E} \left[g(X,Y) \right] = \sum_{(x,y)} g(x,y)f(x,y).
\]</span></p>
</div>
<div class="definition">
<p><span id="def:unlabeled-div-92" class="definition"><strong>(#def:unlabeled-div-92) (Multivariate LOTUS) </strong></span>More generally, if <span class="math inline">\(g: {\mathbb R}^n \to {\mathbb R}\)</span>, and <span class="math inline">\(X_1,...,X_n\)</span> are discrete random variables with joint probability function <span class="math inline">\(f(x_1,...,x_n)\)</span>, then</p>
<p><span class="math display">\[
\mbox{E} \left[g(X_1,...,X_n)\right] = \sum_{(x_1,...,x_n)} g(x_1,...,x_n)f(x_1,...,x_n).
\]</span></p>
</div>
<p>With mLOTUS, we may define the expectation for multivariate distributions. In addition, the expectation has the linearity property as in the univariate case.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\mbox{E}[ a g_1(X,Y) + b g_2(X,Y) ] = a \cdot \mbox{E}[g_1(X,Y)] + b \cdot \mbox{E}[g_2(X,Y)].\)</span></li>
<li><span class="math inline">\(\mbox{E}[X+Y] = \mbox{E}[X] + \mbox{E}[Y]\)</span></li>
</ol>
<p>Observation: Regardless of the relationship between the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we HAVE the linearity property!</p>
<p>Proofs are left as exercises.</p>
<div class="lemma">
<p><span id="lem:unlabeled-div-93" class="lemma"><strong>(#lem:unlabeled-div-93) (Expectation for trivariate multinomial) </strong></span>Let <span class="math inline">\((X_1, X_2, X_3) \sim Mult(n, p_1, p_2, p_3)\)</span>. Then
<span class="math display">\[
E[X_1 X_2] = n(n-1) p_1 p_2.
\]</span></p>
</div>
</div>
<div id="relationship-betwen-the-random-variables" class="section level3 hasAnchor" number="31.0.2">
<h3 class="hasAnchor"><span class="header-section-number">31.0.2</span> Relationship betwen the random variables<a href="#relationship-betwen-the-random-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we only discuss about the independence between the ranodm variables. E.g. if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, we have <span class="math inline">\(f(x,y) = f_X(x)f_Y(y)\)</span>. However, what if they are not independent? In this case, we say they are dependent. We want to first determine if two random variables are dependent, and then measure how strong the <em>correlation</em> is. To do so, we use the terminology <strong>covariance</strong>.</p>
<div class="definition">
<p><span id="def:unlabeled-div-94" class="definition"><strong>(#def:unlabeled-div-94) (Covariance) </strong></span>For two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, we define
<span class="math display">\[
Cov(X,Y) = \mathbb{E}\left[ (X - E(X))(Y-E(Y)) \right].
\]</span>
as the covariance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, provided the expression exists.</p>
</div>
<p>Similarly to <span class="math inline">\(Var(X)=E(X^2)-E(X)^2\)</span>, we have a <em>shortcut formula</em> for the covariance:</p>
<p><span class="math display">\[
Cov(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y].
\]</span></p>
<p>Actually, we can write the variance as a covariance as follows:</p>
<ul>
<li><span class="math inline">\(\mathbb{V}ar(X) = cov(X,X) = \mathbb{E}[(X-\mathbb{E}X)(Y-\mathbb{E}Y)] = \mathbb{E}[XX] - \mathbb{E}X \mathbb{E}X\)</span>, which can be simplified as what we have above.</li>
</ul>
<p><strong>Intuition</strong>:
Why do we look at <span class="math inline">\(E \left[ (X - E(X))(Y-E(Y)) \right]\)</span>?</p>
<p>Positive correlation: In this case, <span class="math inline">\(Cov(X,Y)&gt;0\)</span></p>
<ul>
<li>Suppose <span class="math inline">\(X,Y\)</span> are positively related (when <span class="math inline">\(X\)</span> large, <span class="math inline">\(Y\)</span> likely large; when <span class="math inline">\(X\)</span> small, <span class="math inline">\(Y\)</span> likely small)</li>
<li>If <span class="math inline">\((X-E(X))&gt;0\)</span> (so <span class="math inline">\(X\)</span> large), then likely <span class="math inline">\((Y-E(Y))&gt;0\)</span> (so <span class="math inline">\(Y\)</span> also large), hence the product <span class="math inline">\((X-E(X))(Y-E(Y))&gt;0\)</span></li>
<li>If conversely <span class="math inline">\((X-E(X))&lt;0\)</span> (so <span class="math inline">\(X\)</span> small), then likely <span class="math inline">\((Y-E(Y))&lt;0\)</span> (so <span class="math inline">\(Y\)</span> also small), hence the product is likely <span class="math inline">\((X-E(X))(Y-E(Y))&gt;0\)</span></li>
</ul>
<p>Negative correlation:</p>
<ul>
<li>Conversely, suppose <span class="math inline">\(X,Y\)</span> are negatively related (when <span class="math inline">\(X\)</span> large, <span class="math inline">\(Y\)</span> likely small; when <span class="math inline">\(X\)</span> small, <span class="math inline">\(Y\)</span> likely large). Then <span class="math inline">\(Cov(X,Y)&lt;0\)</span>.</li>
</ul>
</div>
<div id="relationship-between-independence-and-covariance" class="section level3 hasAnchor" number="31.0.3">
<h3 class="hasAnchor"><span class="header-section-number">31.0.3</span> Relationship between independence and covariance<a href="#relationship-between-independence-and-covariance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One may wonder that does indpendence and covariance relate, if so, in what way?</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-95" class="theorem"><strong>(#thm:unlabeled-div-95) </strong></span>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then <span class="math inline">\(Cov(X,Y)=0\)</span>.</p>
</div>
<p><strong>NOTE</strong>: <strong>The converse statement is FALSE!!!</strong> That is, if <span class="math inline">\(Cov(X,Y)=0\)</span> then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not necessarily independent. For instance, let <span class="math inline">\(X\sim U(-1,1)\)</span>, and let <span class="math inline">\(Y =X^2\)</span>. Then <span class="math inline">\(Cov(X,Y)=0\)</span> but <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are not independent.</p>
<!--chapter:end:30-Lec30.Rmd-->
</div>
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
