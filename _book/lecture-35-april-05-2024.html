<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>36 Lecture 35, April 05, 2024 | Stat 230 Introduction to Probability</title>
  <meta name="description" content="This is an example site for UW Stat 230, 2024 Winter" />
  <meta name="generator" content="bookdown 0.38 and GitBook 2.6.7" />

  <meta property="og:title" content="36 Lecture 35, April 05, 2024 | Stat 230 Introduction to Probability" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This is an example site for UW Stat 230, 2024 Winter" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="36 Lecture 35, April 05, 2024 | Stat 230 Introduction to Probability" />
  
  <meta name="twitter:description" content="This is an example site for UW Stat 230, 2024 Winter" />
  



<meta name="date" content="2024-04-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="lecture-34-april-03-2024.html"/>
<link rel="next" href="lecture-36-april-08-2024.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Statistics 230 Introduction to Probability</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Information of the course</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#course-description"><i class="fa fa-check"></i><b>1.1</b> Course description</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#instructor"><i class="fa fa-check"></i><b>1.1.1</b> Instructor</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#course-coordinator"><i class="fa fa-check"></i><b>1.1.2</b> Course Coordinator</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#logistic-issue"><i class="fa fa-check"></i><b>1.1.3</b> Logistic Issue</a></li>
<li class="chapter" data-level="1.1.4" data-path="index.html"><a href="index.html#assessments-and-dates"><i class="fa fa-check"></i><b>1.1.4</b> Assessments and Dates</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#chapters-and-associated-lectures"><i class="fa fa-check"></i><b>1.2</b> Chapters and associated Lectures</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="lecture-1-january-08-2024.html"><a href="lecture-1-january-08-2024.html"><i class="fa fa-check"></i><b>2</b> Lecture 1, January 08, 2024</a></li>
<li class="chapter" data-level="3" data-path="lecture-2-january-10-2024.html"><a href="lecture-2-january-10-2024.html"><i class="fa fa-check"></i><b>3</b> Lecture 2, January 10, 2024</a>
<ul>
<li class="chapter" data-level="3.1" data-path="lecture-2-january-10-2024.html"><a href="lecture-2-january-10-2024.html#questions-from-the-class"><i class="fa fa-check"></i><b>3.1</b> Questions from the class</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lecture-3-january-12-2024.html"><a href="lecture-3-january-12-2024.html"><i class="fa fa-check"></i><b>4</b> Lecture 3, January 12, 2024</a>
<ul>
<li class="chapter" data-level="4.1" data-path="lecture-3-january-12-2024.html"><a href="lecture-3-january-12-2024.html#questions-from-the-class-1"><i class="fa fa-check"></i><b>4.1</b> Questions from the class</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lecture-4-january-15-2024.html"><a href="lecture-4-january-15-2024.html"><i class="fa fa-check"></i><b>5</b> Lecture 4, January 15, 2024</a></li>
<li class="chapter" data-level="6" data-path="lecture-5-january-17-2024.html"><a href="lecture-5-january-17-2024.html"><i class="fa fa-check"></i><b>6</b> Lecture 5, January 17, 2024</a>
<ul>
<li class="chapter" data-level="6.1" data-path="lecture-5-january-17-2024.html"><a href="lecture-5-january-17-2024.html#example-in-class"><i class="fa fa-check"></i><b>6.1</b> Example in class</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="lecture-6-january-19-2024.html"><a href="lecture-6-january-19-2024.html"><i class="fa fa-check"></i><b>7</b> Lecture 6, January 19, 2024</a>
<ul>
<li class="chapter" data-level="7.0.1" data-path="lecture-6-january-19-2024.html"><a href="lecture-6-january-19-2024.html#multinomial-coefficient"><i class="fa fa-check"></i><b>7.0.1</b> Multinomial Coefficient</a></li>
<li class="chapter" data-level="7.0.2" data-path="lecture-6-january-19-2024.html"><a href="lecture-6-january-19-2024.html#the-birthday-problem"><i class="fa fa-check"></i><b>7.0.2</b> The Birthday Problem</a></li>
<li class="chapter" data-level="7.0.3" data-path="lecture-6-january-19-2024.html"><a href="lecture-6-january-19-2024.html#chapter-4-probbility-rules-and-conditional-probability"><i class="fa fa-check"></i><b>7.0.3</b> Chapter 4 Probbility Rules and Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="lecture-7-january-22-2024.html"><a href="lecture-7-january-22-2024.html"><i class="fa fa-check"></i><b>8</b> Lecture 7, January 22, 2024</a>
<ul>
<li class="chapter" data-level="8.0.1" data-path="lecture-7-january-22-2024.html"><a href="lecture-7-january-22-2024.html#some-terminology-about-the-set-thoery."><i class="fa fa-check"></i><b>8.0.1</b> Some terminology about the set thoery.</a></li>
<li class="chapter" data-level="8.0.2" data-path="lecture-7-january-22-2024.html"><a href="lecture-7-january-22-2024.html#independence"><i class="fa fa-check"></i><b>8.0.2</b> Independence</a></li>
<li class="chapter" data-level="8.0.3" data-path="lecture-7-january-22-2024.html"><a href="lecture-7-january-22-2024.html#independence-v.s.-multually-exclusivedisjoint"><i class="fa fa-check"></i><b>8.0.3</b> Independence v.s. Multually Exclusive/Disjoint</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="lecture-8-january-24-2024.html"><a href="lecture-8-january-24-2024.html"><i class="fa fa-check"></i><b>9</b> Lecture 8, January 24, 2024</a>
<ul>
<li class="chapter" data-level="9.0.1" data-path="lecture-8-january-24-2024.html"><a href="lecture-8-january-24-2024.html#properties-of-conditional-probability"><i class="fa fa-check"></i><b>9.0.1</b> Properties of Conditional Probability</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="lecture-9-january-26-2024.html"><a href="lecture-9-january-26-2024.html"><i class="fa fa-check"></i><b>10</b> Lecture 9, January 26, 2024</a>
<ul>
<li class="chapter" data-level="10.0.1" data-path="lecture-9-january-26-2024.html"><a href="lecture-9-january-26-2024.html#law-of-total-probability"><i class="fa fa-check"></i><b>10.0.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="10.0.2" data-path="lecture-9-january-26-2024.html"><a href="lecture-9-january-26-2024.html#bayes-rule"><i class="fa fa-check"></i><b>10.0.2</b> Bayes Rule</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="lecture-10-january-29-2024.html"><a href="lecture-10-january-29-2024.html"><i class="fa fa-check"></i><b>11</b> Lecture 10, January 29, 2024</a>
<ul>
<li class="chapter" data-level="11.0.1" data-path="lecture-10-january-29-2024.html"><a href="lecture-10-january-29-2024.html#chapter-5.-discrete-random-variable"><i class="fa fa-check"></i><b>11.0.1</b> Chapter 5. Discrete Random Variable</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="lecture-11-january-31-2024.html"><a href="lecture-11-january-31-2024.html"><i class="fa fa-check"></i><b>12</b> Lecture 11, January 31, 2024</a>
<ul>
<li class="chapter" data-level="12.1" data-path="lecture-11-january-31-2024.html"><a href="lecture-11-january-31-2024.html#distinction-of-the-definition-discrete-of-the-sample-space-and-the-random-variable"><i class="fa fa-check"></i><b>12.1</b> Distinction of the definition “discrete” of the sample space and the random variable</a></li>
<li class="chapter" data-level="12.2" data-path="lecture-11-january-31-2024.html"><a href="lecture-11-january-31-2024.html#cumulative-distribution-function"><i class="fa fa-check"></i><b>12.2</b> Cumulative distribution function</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="lecture-11-january-31-2024.html"><a href="lecture-11-january-31-2024.html#properties-of-the-cumulative-distribution-function"><i class="fa fa-check"></i><b>12.2.1</b> Properties of the cumulative distribution function</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="lecture-11-january-31-2024.html"><a href="lecture-11-january-31-2024.html#special-distributions-with-names"><i class="fa fa-check"></i><b>12.3</b> Special distributions with names</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="lecture-11-january-31-2024.html"><a href="lecture-11-january-31-2024.html#discrete-uniform-distribution"><i class="fa fa-check"></i><b>12.3.1</b> Discrete uniform distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="lecture-12-feburary-02-2024.html"><a href="lecture-12-feburary-02-2024.html"><i class="fa fa-check"></i><b>13</b> Lecture 12, Feburary 02, 2024</a>
<ul>
<li class="chapter" data-level="13.1" data-path="lecture-12-feburary-02-2024.html"><a href="lecture-12-feburary-02-2024.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>13.1</b> Hypergeometric distribution</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="lecture-12-feburary-02-2024.html"><a href="lecture-12-feburary-02-2024.html#range-of-the-hypergeomnetric-distribution"><i class="fa fa-check"></i><b>13.1.1</b> Range of the Hypergeomnetric distribution</a></li>
<li class="chapter" data-level="13.1.2" data-path="lecture-12-feburary-02-2024.html"><a href="lecture-12-feburary-02-2024.html#probability-function-of-hypergeometric-distribution"><i class="fa fa-check"></i><b>13.1.2</b> Probability function of hypergeometric distribution</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="lecture-12-feburary-02-2024.html"><a href="lecture-12-feburary-02-2024.html#bernoulli-and-binimial-distributions"><i class="fa fa-check"></i><b>13.2</b> Bernoulli and Binimial distributions</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="lecture-12-feburary-02-2024.html"><a href="lecture-12-feburary-02-2024.html#probability-function-and-cumulative-distribution-function"><i class="fa fa-check"></i><b>13.2.1</b> Probability function and cumulative distribution function</a></li>
<li class="chapter" data-level="13.2.2" data-path="lecture-12-feburary-02-2024.html"><a href="lecture-12-feburary-02-2024.html#bernoulli"><i class="fa fa-check"></i><b>13.2.2</b> Bernoulli</a></li>
<li class="chapter" data-level="13.2.3" data-path="lecture-12-feburary-02-2024.html"><a href="lecture-12-feburary-02-2024.html#binomial"><i class="fa fa-check"></i><b>13.2.3</b> Binomial</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="lecture-12-feburary-02-2024.html"><a href="lecture-12-feburary-02-2024.html#relationship-and-difference-between-binomial-and-hypergeometric"><i class="fa fa-check"></i><b>13.3</b> Relationship and difference between binomial and hypergeometric</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="lecture-13-feburary-05-2024.html"><a href="lecture-13-feburary-05-2024.html"><i class="fa fa-check"></i><b>14</b> Lecture 13, Feburary 05, 2024</a>
<ul>
<li class="chapter" data-level="14.1" data-path="lecture-13-feburary-05-2024.html"><a href="lecture-13-feburary-05-2024.html#binomial-v.s.-hypergeometric"><i class="fa fa-check"></i><b>14.1</b> Binomial v.s. hypergeometric</a></li>
<li class="chapter" data-level="14.2" data-path="lecture-13-feburary-05-2024.html"><a href="lecture-13-feburary-05-2024.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>14.2</b> Negative binomial distribution</a>
<ul>
<li class="chapter" data-level="14.2.1" data-path="lecture-13-feburary-05-2024.html"><a href="lecture-13-feburary-05-2024.html#probability-function"><i class="fa fa-check"></i><b>14.2.1</b> Probability function</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="lecture-13-feburary-05-2024.html"><a href="lecture-13-feburary-05-2024.html#binomial-v.s.-negative-binomial"><i class="fa fa-check"></i><b>14.3</b> Binomial v.s. Negative Binomial</a></li>
<li class="chapter" data-level="14.4" data-path="lecture-13-feburary-05-2024.html"><a href="lecture-13-feburary-05-2024.html#geometric-distribution"><i class="fa fa-check"></i><b>14.4</b> Geometric distribution</a>
<ul>
<li class="chapter" data-level="14.4.1" data-path="lecture-13-feburary-05-2024.html"><a href="lecture-13-feburary-05-2024.html#probability-function-and-distribution-function-1"><i class="fa fa-check"></i><b>14.4.1</b> Probability function and distribution function</a></li>
<li class="chapter" data-level="14.4.2" data-path="lecture-13-feburary-05-2024.html"><a href="lecture-13-feburary-05-2024.html#memoryless-property"><i class="fa fa-check"></i><b>14.4.2</b> Memoryless property</a></li>
<li class="chapter" data-level="14.4.3" data-path="lecture-13-feburary-05-2024.html"><a href="lecture-13-feburary-05-2024.html#aside-reason-to-call-the-negative-binomial-distribution"><i class="fa fa-check"></i><b>14.4.3</b> Aside, Reason to call the Negative binomial distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="lecture-14-feburary-07-2024.html"><a href="lecture-14-feburary-07-2024.html"><i class="fa fa-check"></i><b>15</b> Lecture 14, Feburary 07, 2024</a>
<ul>
<li class="chapter" data-level="15.1" data-path="lecture-14-feburary-07-2024.html"><a href="lecture-14-feburary-07-2024.html#poisson-distribution"><i class="fa fa-check"></i><b>15.1</b> Poisson distribution</a>
<ul>
<li class="chapter" data-level="15.1.1" data-path="lecture-14-feburary-07-2024.html"><a href="lecture-14-feburary-07-2024.html#notation"><i class="fa fa-check"></i><b>15.1.1</b> Notation</a></li>
<li class="chapter" data-level="15.1.2" data-path="lecture-14-feburary-07-2024.html"><a href="lecture-14-feburary-07-2024.html#interpreation-of-the-poisson-distribution"><i class="fa fa-check"></i><b>15.1.2</b> Interpreation of the Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="lecture-14-feburary-07-2024.html"><a href="lecture-14-feburary-07-2024.html#poisson-as-the-limiting-distribution-of-the-binomial-distribution"><i class="fa fa-check"></i><b>15.2</b> Poisson as the limiting distribution of the binomial distribution</a></li>
<li class="chapter" data-level="15.3" data-path="lecture-14-feburary-07-2024.html"><a href="lecture-14-feburary-07-2024.html#poisson-process"><i class="fa fa-check"></i><b>15.3</b> Poisson process</a></li>
<li class="chapter" data-level="15.4" data-path="lecture-14-feburary-07-2024.html"><a href="lecture-14-feburary-07-2024.html#side-notes-rigorous-definition-of-convergence-in-distribution"><i class="fa fa-check"></i><b>15.4</b> Side notes – Rigorous definition of convergence in distribution</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="lecture-15-feburary-09-2024.html"><a href="lecture-15-feburary-09-2024.html"><i class="fa fa-check"></i><b>16</b> Lecture 15, Feburary 09, 2024</a>
<ul>
<li class="chapter" data-level="16.1" data-path="lecture-15-feburary-09-2024.html"><a href="lecture-15-feburary-09-2024.html#review-of-the-distributions-we-covered-before"><i class="fa fa-check"></i><b>16.1</b> Review of the distributions we covered before</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="lecture-16-feburary-12-2024.html"><a href="lecture-16-feburary-12-2024.html"><i class="fa fa-check"></i><b>17</b> Lecture 16, Feburary 12, 2024</a>
<ul>
<li class="chapter" data-level="17.1" data-path="lecture-16-feburary-12-2024.html"><a href="lecture-16-feburary-12-2024.html#chapter-7-expectation-and-variance"><i class="fa fa-check"></i><b>17.1</b> Chapter 7 Expectation and Variance</a></li>
<li class="chapter" data-level="17.2" data-path="lecture-16-feburary-12-2024.html"><a href="lecture-16-feburary-12-2024.html#theoretical-mean-and-the-sample-mean"><i class="fa fa-check"></i><b>17.2</b> Theoretical mean and the sample mean</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="lecture-16-feburary-12-2024.html"><a href="lecture-16-feburary-12-2024.html#definition"><i class="fa fa-check"></i><b>17.2.1</b> Definition</a></li>
<li class="chapter" data-level="17.2.2" data-path="lecture-16-feburary-12-2024.html"><a href="lecture-16-feburary-12-2024.html#interpretation"><i class="fa fa-check"></i><b>17.2.2</b> Interpretation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="lecture-17-feburary-14-2024.html"><a href="lecture-17-feburary-14-2024.html"><i class="fa fa-check"></i><b>18</b> Lecture 17, Feburary 14, 2024</a>
<ul>
<li class="chapter" data-level="18.1" data-path="lecture-17-feburary-14-2024.html"><a href="lecture-17-feburary-14-2024.html#more-about-expectation"><i class="fa fa-check"></i><b>18.1</b> More about expectation</a></li>
<li class="chapter" data-level="18.2" data-path="lecture-17-feburary-14-2024.html"><a href="lecture-17-feburary-14-2024.html#law-of-unconscious-statistician"><i class="fa fa-check"></i><b>18.2</b> Law of Unconscious Statistician</a></li>
<li class="chapter" data-level="18.3" data-path="lecture-17-feburary-14-2024.html"><a href="lecture-17-feburary-14-2024.html#tricks"><i class="fa fa-check"></i><b>18.3</b> Tricks</a></li>
<li class="chapter" data-level="18.4" data-path="lecture-17-feburary-14-2024.html"><a href="lecture-17-feburary-14-2024.html#property-of-expectation---linearity"><i class="fa fa-check"></i><b>18.4</b> Property of expectation - linearity</a>
<ul>
<li class="chapter" data-level="18.4.1" data-path="lecture-17-feburary-14-2024.html"><a href="lecture-17-feburary-14-2024.html#proof."><i class="fa fa-check"></i><b>18.4.1</b> Proof.</a></li>
</ul></li>
<li class="chapter" data-level="18.5" data-path="lecture-17-feburary-14-2024.html"><a href="lecture-17-feburary-14-2024.html#mean-of-binomial-distribution"><i class="fa fa-check"></i><b>18.5</b> Mean of binomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="lecture-18-feburary-16-2024.html"><a href="lecture-18-feburary-16-2024.html"><i class="fa fa-check"></i><b>19</b> Lecture 18, Feburary 16, 2024</a>
<ul>
<li class="chapter" data-level="19.1" data-path="lecture-18-feburary-16-2024.html"><a href="lecture-18-feburary-16-2024.html#mean-of-poisson-distribution"><i class="fa fa-check"></i><b>19.1</b> Mean of Poisson distribution</a></li>
<li class="chapter" data-level="19.2" data-path="lecture-18-feburary-16-2024.html"><a href="lecture-18-feburary-16-2024.html#mean-of-hypergeometric-and-negative-binomial"><i class="fa fa-check"></i><b>19.2</b> Mean of Hypergeometric and Negative binomial</a>
<ul>
<li class="chapter" data-level="19.2.1" data-path="lecture-18-feburary-16-2024.html"><a href="lecture-18-feburary-16-2024.html#hypergeometric"><i class="fa fa-check"></i><b>19.2.1</b> Hypergeometric</a></li>
<li class="chapter" data-level="19.2.2" data-path="lecture-18-feburary-16-2024.html"><a href="lecture-18-feburary-16-2024.html#negative-binomial"><i class="fa fa-check"></i><b>19.2.2</b> Negative binomial</a></li>
</ul></li>
<li class="chapter" data-level="19.3" data-path="lecture-18-feburary-16-2024.html"><a href="lecture-18-feburary-16-2024.html#note-that-the-expectation-does-not-always-exist"><i class="fa fa-check"></i><b>19.3</b> Note that the expectation does not always exist</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="lecture-19-feburary-26-2024.html"><a href="lecture-19-feburary-26-2024.html"><i class="fa fa-check"></i><b>20</b> Lecture 19, Feburary 26, 2024</a>
<ul>
<li class="chapter" data-level="20.1" data-path="lecture-19-feburary-26-2024.html"><a href="lecture-19-feburary-26-2024.html#motivation-to-have-higher-moments"><i class="fa fa-check"></i><b>20.1</b> Motivation to have higher moments</a>
<ul>
<li class="chapter" data-level="20.1.1" data-path="lecture-19-feburary-26-2024.html"><a href="lecture-19-feburary-26-2024.html#deviations-from-the-mean"><i class="fa fa-check"></i><b>20.1.1</b> Deviations from the mean</a></li>
</ul></li>
<li class="chapter" data-level="20.2" data-path="lecture-19-feburary-26-2024.html"><a href="lecture-19-feburary-26-2024.html#the-defintion-of-variance"><i class="fa fa-check"></i><b>20.2</b> The defintion of variance</a>
<ul>
<li class="chapter" data-level="20.2.1" data-path="lecture-19-feburary-26-2024.html"><a href="lecture-19-feburary-26-2024.html#properties-of-variance"><i class="fa fa-check"></i><b>20.2.1</b> Properties of variance</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="lecture-19-feburary-26-2024.html"><a href="lecture-19-feburary-26-2024.html#variance-of-a-binomial-distribution"><i class="fa fa-check"></i><b>20.3</b> Variance of a binomial distribution</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="lecture-20-feburary-28-2024.html"><a href="lecture-20-feburary-28-2024.html"><i class="fa fa-check"></i><b>21</b> Lecture 20, Feburary 28, 2024</a>
<ul>
<li class="chapter" data-level="21.1" data-path="lecture-20-feburary-28-2024.html"><a href="lecture-20-feburary-28-2024.html#variance-of-poisson-hypergeometric-and-negative-binomial"><i class="fa fa-check"></i><b>21.1</b> Variance of Poisson, Hypergeometric and Negative Binomial</a></li>
<li class="chapter" data-level="21.2" data-path="lecture-20-feburary-28-2024.html"><a href="lecture-20-feburary-28-2024.html#standard-deviation"><i class="fa fa-check"></i><b>21.2</b> Standard Deviation</a></li>
<li class="chapter" data-level="21.3" data-path="lecture-20-feburary-28-2024.html"><a href="lecture-20-feburary-28-2024.html#last-note-of-the-chapter"><i class="fa fa-check"></i><b>21.3</b> Last note of the chapter</a></li>
<li class="chapter" data-level="21.4" data-path="lecture-20-feburary-28-2024.html"><a href="lecture-20-feburary-28-2024.html#chapter-8-continuous-random-variables"><i class="fa fa-check"></i><b>21.4</b> Chapter 8 Continuous Random Variables</a>
<ul>
<li class="chapter" data-level="21.4.1" data-path="lecture-20-feburary-28-2024.html"><a href="lecture-20-feburary-28-2024.html#continuous-random-variable"><i class="fa fa-check"></i><b>21.4.1</b> Continuous random variable</a></li>
<li class="chapter" data-level="21.4.2" data-path="lecture-20-feburary-28-2024.html"><a href="lecture-20-feburary-28-2024.html#equality-does-not-matter-in-the-continous-case"><i class="fa fa-check"></i><b>21.4.2</b> Equality does not matter in the continous case</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="22" data-path="lecture-21-march-01-2024.html"><a href="lecture-21-march-01-2024.html"><i class="fa fa-check"></i><b>22</b> Lecture 21, March 01, 2024</a>
<ul>
<li class="chapter" data-level="22.1" data-path="lecture-21-march-01-2024.html"><a href="lecture-21-march-01-2024.html#law-of-unconciousness-of-statistician-continuous-version"><i class="fa fa-check"></i><b>22.1</b> Law of unconciousness of statistician, continuous version</a></li>
<li class="chapter" data-level="22.2" data-path="lecture-21-march-01-2024.html"><a href="lecture-21-march-01-2024.html#function-of-random-variable"><i class="fa fa-check"></i><b>22.2</b> function of random variable</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="lecture-22-march-04-2024.html"><a href="lecture-22-march-04-2024.html"><i class="fa fa-check"></i><b>23</b> Lecture 22, March 04, 2024</a>
<ul>
<li class="chapter" data-level="23.1" data-path="lecture-22-march-04-2024.html"><a href="lecture-22-march-04-2024.html#receipt-to-find-the-distribution-of-the-transformed-random-varaible-ygx."><i class="fa fa-check"></i><b>23.1</b> Receipt to find the distribution of the transformed random varaible <span class="math inline">\(Y=g(X)\)</span>.</a></li>
<li class="chapter" data-level="23.2" data-path="lecture-22-march-04-2024.html"><a href="lecture-22-march-04-2024.html#quantile"><i class="fa fa-check"></i><b>23.2</b> Quantile</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="lecture-23-march-06-2024.html"><a href="lecture-23-march-06-2024.html"><i class="fa fa-check"></i><b>24</b> Lecture 23, March 06, 2024</a>
<ul>
<li class="chapter" data-level="24.1" data-path="lecture-23-march-06-2024.html"><a href="lecture-23-march-06-2024.html#recap-the-quantile-function"><i class="fa fa-check"></i><b>24.1</b> Recap the quantile function</a></li>
<li class="chapter" data-level="24.2" data-path="lecture-23-march-06-2024.html"><a href="lecture-23-march-06-2024.html#quantiles-for-discrete-distributions"><i class="fa fa-check"></i><b>24.2</b> Quantiles for discrete distributions</a></li>
<li class="chapter" data-level="24.3" data-path="lecture-23-march-06-2024.html"><a href="lecture-23-march-06-2024.html#special-named-distributions"><i class="fa fa-check"></i><b>24.3</b> Special named distributions</a>
<ul>
<li class="chapter" data-level="24.3.1" data-path="lecture-23-march-06-2024.html"><a href="lecture-23-march-06-2024.html#continuous-uniform-distribution"><i class="fa fa-check"></i><b>24.3.1</b> Continuous uniform distribution</a></li>
<li class="chapter" data-level="24.3.2" data-path="lecture-23-march-06-2024.html"><a href="lecture-23-march-06-2024.html#exponential-distribution"><i class="fa fa-check"></i><b>24.3.2</b> Exponential distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="lecture-24-march-08-2024.html"><a href="lecture-24-march-08-2024.html"><i class="fa fa-check"></i><b>25</b> Lecture 24, March 08, 2024</a>
<ul>
<li class="chapter" data-level="25.1" data-path="lecture-24-march-08-2024.html"><a href="lecture-24-march-08-2024.html#proof-of-the-moments-of-exponential-distribution"><i class="fa fa-check"></i><b>25.1</b> Proof of the moments of exponential distribution</a>
<ul>
<li class="chapter" data-level="25.1.1" data-path="lecture-24-march-08-2024.html"><a href="lecture-24-march-08-2024.html#memoryless-property-of-exponential-distribution"><i class="fa fa-check"></i><b>25.1.1</b> Memoryless property of exponential distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="lecture-24-march-11-2024.html"><a href="lecture-24-march-11-2024.html"><i class="fa fa-check"></i><b>26</b> Lecture 24, March 11, 2024</a>
<ul>
<li class="chapter" data-level="26.1" data-path="lecture-24-march-11-2024.html"><a href="lecture-24-march-11-2024.html#sampling-realizations-of-random-variables"><i class="fa fa-check"></i><b>26.1</b> Sampling realizations of random variables</a></li>
<li class="chapter" data-level="26.2" data-path="lecture-24-march-11-2024.html"><a href="lecture-24-march-11-2024.html#inversion-method"><i class="fa fa-check"></i><b>26.2</b> Inversion method</a>
<ul>
<li class="chapter" data-level="26.2.1" data-path="lecture-24-march-11-2024.html"><a href="lecture-24-march-11-2024.html#with-strictly-increasing-and-continuous-assumption"><i class="fa fa-check"></i><b>26.2.1</b> With strictly increasing and continuous assumption</a></li>
<li class="chapter" data-level="26.2.2" data-path="lecture-24-march-11-2024.html"><a href="lecture-24-march-11-2024.html#more-general-case-using-the-quantile"><i class="fa fa-check"></i><b>26.2.2</b> More general case using the quantile</a></li>
</ul></li>
<li class="chapter" data-level="26.3" data-path="lecture-24-march-11-2024.html"><a href="lecture-24-march-11-2024.html#normalgaussian-distribution"><i class="fa fa-check"></i><b>26.3</b> Normal/Gaussian distribution</a>
<ul>
<li class="chapter" data-level="26.3.1" data-path="lecture-24-march-11-2024.html"><a href="lecture-24-march-11-2024.html#properties-of-gaussian-distribution"><i class="fa fa-check"></i><b>26.3.1</b> Properties of Gaussian distribution</a></li>
<li class="chapter" data-level="26.3.2" data-path="lecture-24-march-11-2024.html"><a href="lecture-24-march-11-2024.html#problem"><i class="fa fa-check"></i><b>26.3.2</b> Problem</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="27" data-path="lecture-26-march-13-2024.html"><a href="lecture-26-march-13-2024.html"><i class="fa fa-check"></i><b>27</b> Lecture 26, March 13, 2024</a>
<ul>
<li class="chapter" data-level="27.1" data-path="lecture-26-march-13-2024.html"><a href="lecture-26-march-13-2024.html#normal-distribution-continue"><i class="fa fa-check"></i><b>27.1</b> Normal distribution continue</a>
<ul>
<li class="chapter" data-level="27.1.1" data-path="lecture-26-march-13-2024.html"><a href="lecture-26-march-13-2024.html#standard-normal-distribution"><i class="fa fa-check"></i><b>27.1.1</b> Standard normal distribution</a></li>
<li class="chapter" data-level="27.1.2" data-path="lecture-26-march-13-2024.html"><a href="lecture-26-march-13-2024.html#procedure"><i class="fa fa-check"></i><b>27.1.2</b> Procedure</a></li>
<li class="chapter" data-level="27.1.3" data-path="lecture-26-march-13-2024.html"><a href="lecture-26-march-13-2024.html#quantile-1"><i class="fa fa-check"></i><b>27.1.3</b> Quantile</a></li>
<li class="chapter" data-level="27.1.4" data-path="lecture-26-march-13-2024.html"><a href="lecture-26-march-13-2024.html#rule-for-1-2-3-standard-deviations"><i class="fa fa-check"></i><b>27.1.4</b> 68-95-99.7 rule for 1-2-3 standard deviation(s)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="28" data-path="lecture-27-march-15-2024.html"><a href="lecture-27-march-15-2024.html"><i class="fa fa-check"></i><b>28</b> Lecture 27, March 15, 2024</a>
<ul>
<li class="chapter" data-level="28.1" data-path="lecture-27-march-15-2024.html"><a href="lecture-27-march-15-2024.html#chapter-9-multivariate-distributions"><i class="fa fa-check"></i><b>28.1</b> Chapter 9: Multivariate distributions</a>
<ul>
<li class="chapter" data-level="28.1.1" data-path="lecture-27-march-15-2024.html"><a href="lecture-27-march-15-2024.html#properties-of-the-joint-probability-function"><i class="fa fa-check"></i><b>28.1.1</b> Properties of the joint probability function</a></li>
<li class="chapter" data-level="28.1.2" data-path="lecture-27-march-15-2024.html"><a href="lecture-27-march-15-2024.html#marginal-distribution-of-the-joint-distribution-function"><i class="fa fa-check"></i><b>28.1.2</b> Marginal distribution of the joint distribution function</a></li>
<li class="chapter" data-level="28.1.3" data-path="lecture-27-march-15-2024.html"><a href="lecture-27-march-15-2024.html#comments"><i class="fa fa-check"></i><b>28.1.3</b> Comments</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="29" data-path="lecture-28-march-18-2024.html"><a href="lecture-28-march-18-2024.html"><i class="fa fa-check"></i><b>29</b> Lecture 28, March 18, 2024</a>
<ul>
<li class="chapter" data-level="29.0.1" data-path="lecture-28-march-18-2024.html"><a href="lecture-28-march-18-2024.html#conditional-probability-function-for-multivaraite-random-variable"><i class="fa fa-check"></i><b>29.0.1</b> Conditional probability function for multivaraite random variable</a></li>
<li class="chapter" data-level="29.0.2" data-path="lecture-28-march-18-2024.html"><a href="lecture-28-march-18-2024.html#probably-function-of-ugx_1cdotsx_n"><i class="fa fa-check"></i><b>29.0.2</b> Probably function of <span class="math inline">\(U=g(X_1,\cdots,X_n)\)</span></a></li>
<li class="chapter" data-level="29.0.3" data-path="lecture-28-march-18-2024.html"><a href="lecture-28-march-18-2024.html#known-results-for-named-distributions"><i class="fa fa-check"></i><b>29.0.3</b> Known results for named distributions</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="lecture-29-march-20-2024.html"><a href="lecture-29-march-20-2024.html"><i class="fa fa-check"></i><b>30</b> Lecture 29, March 20, 2024</a>
<ul>
<li class="chapter" data-level="30.1" data-path="lecture-29-march-20-2024.html"><a href="lecture-29-march-20-2024.html#multinomial-distribution"><i class="fa fa-check"></i><b>30.1</b> Multinomial distribution</a>
<ul>
<li class="chapter" data-level="30.1.1" data-path="lecture-29-march-20-2024.html"><a href="lecture-29-march-20-2024.html#the-probability-distribution-function"><i class="fa fa-check"></i><b>30.1.1</b> The probability distribution function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="31" data-path="lecture-30-march-22-2024.html"><a href="lecture-30-march-22-2024.html"><i class="fa fa-check"></i><b>31</b> Lecture 30, March 22, 2024</a>
<ul>
<li class="chapter" data-level="31.0.1" data-path="lecture-30-march-22-2024.html"><a href="lecture-30-march-22-2024.html#summary-statistics-of-multivariate-distributions"><i class="fa fa-check"></i><b>31.0.1</b> Summary statistics of multivariate distributions</a></li>
<li class="chapter" data-level="31.0.2" data-path="lecture-30-march-22-2024.html"><a href="lecture-30-march-22-2024.html#relationship-betwen-the-random-variables"><i class="fa fa-check"></i><b>31.0.2</b> Relationship betwen the random variables</a></li>
<li class="chapter" data-level="31.0.3" data-path="lecture-30-march-22-2024.html"><a href="lecture-30-march-22-2024.html#relationship-between-independence-and-covariance"><i class="fa fa-check"></i><b>31.0.3</b> Relationship between independence and covariance</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="lecture-31-march-25-2024.html"><a href="lecture-31-march-25-2024.html"><i class="fa fa-check"></i><b>32</b> Lecture 31, March 25, 2024</a>
<ul>
<li class="chapter" data-level="32.1" data-path="lecture-31-march-25-2024.html"><a href="lecture-31-march-25-2024.html#covariance-and-correlation"><i class="fa fa-check"></i><b>32.1</b> Covariance and Correlation</a></li>
<li class="chapter" data-level="32.2" data-path="lecture-31-march-25-2024.html"><a href="lecture-31-march-25-2024.html#linear-combination-of-random-variables"><i class="fa fa-check"></i><b>32.2</b> Linear combination of random variables</a>
<ul>
<li class="chapter" data-level="32.2.1" data-path="lecture-31-march-25-2024.html"><a href="lecture-31-march-25-2024.html#some-examples"><i class="fa fa-check"></i><b>32.2.1</b> Some examples</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="33" data-path="lecture-32-march-27-2024.html"><a href="lecture-32-march-27-2024.html"><i class="fa fa-check"></i><b>33</b> Lecture 32, March 27, 2024</a>
<ul>
<li class="chapter" data-level="33.1" data-path="lecture-32-march-27-2024.html"><a href="lecture-32-march-27-2024.html#linear-combination-continue"><i class="fa fa-check"></i><b>33.1</b> Linear combination (continue)</a>
<ul>
<li class="chapter" data-level="33.1.1" data-path="lecture-32-march-27-2024.html"><a href="lecture-32-march-27-2024.html#linear-combination-of-gaussian-random-variables"><i class="fa fa-check"></i><b>33.1.1</b> Linear Combination of Gaussian random variables</a></li>
</ul></li>
<li class="chapter" data-level="33.2" data-path="lecture-32-march-27-2024.html"><a href="lecture-32-march-27-2024.html#indicator-variable"><i class="fa fa-check"></i><b>33.2</b> Indicator variable</a>
<ul>
<li class="chapter" data-level="33.2.1" data-path="lecture-32-march-27-2024.html"><a href="lecture-32-march-27-2024.html#properties-of-the-indicator-variable"><i class="fa fa-check"></i><b>33.2.1</b> Properties of the indicator variable</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="34" data-path="lecture-33-april-01-2024.html"><a href="lecture-33-april-01-2024.html"><i class="fa fa-check"></i><b>34</b> Lecture 33, April 01, 2024</a>
<ul>
<li class="chapter" data-level="34.1" data-path="lecture-33-april-01-2024.html"><a href="lecture-33-april-01-2024.html#indicator-funcrions"><i class="fa fa-check"></i><b>34.1</b> Indicator funcrions</a></li>
<li class="chapter" data-level="34.2" data-path="lecture-33-april-01-2024.html"><a href="lecture-33-april-01-2024.html#chapter-10-central-limit-theorem-and-moment-generating-functions"><i class="fa fa-check"></i><b>34.2</b> Chapter 10: Central Limit Theorem and Moment Generating Functions</a></li>
</ul></li>
<li class="chapter" data-level="35" data-path="lecture-34-april-03-2024.html"><a href="lecture-34-april-03-2024.html"><i class="fa fa-check"></i><b>35</b> Lecture 34, April 03, 2024</a>
<ul>
<li class="chapter" data-level="35.0.1" data-path="lecture-34-april-03-2024.html"><a href="lecture-34-april-03-2024.html#rule-of-continuity-correction"><i class="fa fa-check"></i><b>35.0.1</b> Rule of continuity correction</a></li>
<li class="chapter" data-level="35.0.2" data-path="lecture-34-april-03-2024.html"><a href="lecture-34-april-03-2024.html#normal-approximation-to-binomial"><i class="fa fa-check"></i><b>35.0.2</b> Normal approximation to Binomial</a></li>
<li class="chapter" data-level="35.0.3" data-path="lecture-34-april-03-2024.html"><a href="lecture-34-april-03-2024.html#rule-of-thumb-for-using-the-central-limit-theorem"><i class="fa fa-check"></i><b>35.0.3</b> Rule of thumb for using the central limit theorem!!!</a></li>
<li class="chapter" data-level="35.0.4" data-path="lecture-34-april-03-2024.html"><a href="lecture-34-april-03-2024.html#normal-approximation-to-poisson"><i class="fa fa-check"></i><b>35.0.4</b> Normal approximation to Poisson</a></li>
<li class="chapter" data-level="35.1" data-path="lecture-34-april-03-2024.html"><a href="lecture-34-april-03-2024.html#moment-generating-functions"><i class="fa fa-check"></i><b>35.1</b> Moment Generating functions</a>
<ul>
<li class="chapter" data-level="35.1.1" data-path="lecture-34-april-03-2024.html"><a href="lecture-34-april-03-2024.html#properties-of-the-mgf"><i class="fa fa-check"></i><b>35.1.1</b> Properties of the MGF</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="36" data-path="lecture-35-april-05-2024.html"><a href="lecture-35-april-05-2024.html"><i class="fa fa-check"></i><b>36</b> Lecture 35, April 05, 2024</a>
<ul>
<li class="chapter" data-level="36.1" data-path="lecture-35-april-05-2024.html"><a href="lecture-35-april-05-2024.html#properties-of-mgf"><i class="fa fa-check"></i><b>36.1</b> Properties of MGF</a></li>
<li class="chapter" data-level="36.2" data-path="lecture-35-april-05-2024.html"><a href="lecture-35-april-05-2024.html#chapter-999-additional-topics-will-not-be-on-the-exam-unless-otherwise-specified"><i class="fa fa-check"></i><b>36.2</b> Chapter 999 Additional Topics (will not be on the exam unless otherwise specified)</a>
<ul>
<li class="chapter" data-level="36.2.1" data-path="lecture-35-april-05-2024.html"><a href="lecture-35-april-05-2024.html#distributions-that-do-not-have-moments"><i class="fa fa-check"></i><b>36.2.1</b> Distributions that do not have moments</a></li>
<li class="chapter" data-level="36.2.2" data-path="lecture-35-april-05-2024.html"><a href="lecture-35-april-05-2024.html#other-generating-functions"><i class="fa fa-check"></i><b>36.2.2</b> Other generating functions</a></li>
<li class="chapter" data-level="36.2.3" data-path="lecture-35-april-05-2024.html"><a href="lecture-35-april-05-2024.html#delta-method"><i class="fa fa-check"></i><b>36.2.3</b> Delta Method</a></li>
<li class="chapter" data-level="36.2.4" data-path="lecture-35-april-05-2024.html"><a href="lecture-35-april-05-2024.html#other-interesting-results"><i class="fa fa-check"></i><b>36.2.4</b> Other interesting results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="37" data-path="lecture-36-april-08-2024.html"><a href="lecture-36-april-08-2024.html"><i class="fa fa-check"></i><b>37</b> Lecture 36, April 08, 2024</a></li>
<li class="divider"></li>
<li><a href="https://chikuang.github.io/course/stat230" target="blank">Published by Chi-Kuang Yeh</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Stat 230 Introduction to Probability</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="lecture-35-april-05-2024" class="section level1 hasAnchor" number="36">
<h1><span class="header-section-number">36</span> Lecture 35, April 05, 2024<a href="lecture-35-april-05-2024.html#lecture-35-april-05-2024" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Last lecture, we started introducing the last concept of the course, the moment generating function (MGF). Today, we will keep looking the properties of the MGF, and extend MGF to the multivariate case (i.e. MGF for 2 or more random variables).</p>
<div id="properties-of-mgf" class="section level2 hasAnchor" number="36.1">
<h2><span class="header-section-number">36.1</span> Properties of MGF<a href="lecture-35-april-05-2024.html#properties-of-mgf" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The following shows why this is called a <strong>moment</strong> generating function</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(M_X(0)=1\)</span>:</p></li>
<li><p>So long as <span class="math inline">\(M_X(t)\)</span> is defined in a neighbourhood of <span class="math inline">\(t=0\)</span>,
<span class="math display">\[
   \frac{d}{dt^k}M_X(0) = \mathbb{E}[X^k]
   \]</span></p></li>
<li><p>So long as <span class="math inline">\(M_X(t)\)</span> is defined in a neighbourhood of <span class="math inline">\(t=0\)</span>, by Taylor
<span class="math display">\[M_X(t) = \sum_{j=0}^{\infty} \frac{t^j\mathbb{E}[X^j]}{j!} \]</span></p></li>
</ol>
<div class="example">
<p><span id="exm:unlabeled-div-124" class="example"><strong>Example 36.1  (MGF of Poisson) </strong></span>Let <span class="math inline">\(X \sim Poi(\lambda)\)</span>. Show that the MGF of <span class="math inline">\(X\)</span> is given by
<span class="math display">\[ M_X(t)= e^{\lambda(e^t-1)},\quad t\in\mathbb{R},\]</span>
and use the MGF to show that
<span class="math display">\[
\mathbb{E}[X] = \lambda = \mathbb{V}ar(X).
\]</span></p>
<p><strong>Solution</strong>:</p>
<p>The mgf is computed using the exponential sum as
<span class="math display">\[\begin{align*}
M_X(t) &amp;= E(e^{tX})=\sum_{x=0}^\infty e^{tx} e^{-\lambda} \frac{\lambda^x}{x!}\\
&amp;=e^{-\lambda}  \sum_{x=0}^\infty  \frac{ (e^t \lambda)^x}{x!}\\
&amp;= e^{-\lambda} e^{e^t \lambda}=e^{\lambda(e^t-1)}
\end{align*}\]</span>
and we get
<span class="math display">\[M_X&#39;(t)=e^{\lambda(e^t-1)} \lambda e^t \Rightarrow E(X)=M_X&#39;(0)=\lambda\]</span>
and similarly
<span class="math display">\[ E(X^2)=M_X&#39;&#39;(0)=\lambda^2+\lambda\]</span>
so that
<span class="math display">\[ Var(X) = E(X^2)-E(X)^2=\lambda^2+\lambda-\lambda^2=\lambda\]</span></p>
</div>
<p>The significance of MGF is that, under certain conditions, it can show equivalence of two distributions:</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-125" class="theorem"><strong>Theorem 36.1  (Continuity theorem) </strong></span>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have MGFs <span class="math inline">\(M_X(t)\)</span> and <span class="math inline">\(M_Y(t)\)</span> defined in neighbourhoods of the origin, and satisfying
<span class="math inline">\(M_X(t) = M_Y(t)\)</span> for all <span class="math inline">\(t\)</span> where they are defined, then
<span class="math display">\[
X \stackrel{D}{=} Y.
\]</span></p>
</div>
<div id="poisson-to-binomial-approximation" class="section level4 hasAnchor" number="36.1.0.1">
<h4><span class="header-section-number">36.1.0.1</span> Poisson to binomial approximation<a href="lecture-35-april-05-2024.html#poisson-to-binomial-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Recall: For large <span class="math inline">\(n\)</span> and small <span class="math inline">\(p\)</span> such that <span class="math inline">\(\mu=np\)</span>, we can approximate the <span class="math inline">\(Bin(n,p)\)</span> distribution with a <span class="math inline">\(Poi(\mu)\)</span> distribution.</p>
<p>We can prove this by showing that the mgf of <span class="math inline">\(Bin(n,p)\)</span> converges to the mgf of <span class="math inline">\(Poi(\mu)\)</span> as <span class="math inline">\(n\rightarrow\infty\)</span> where <span class="math inline">\(\mu=np\)</span>:</p>
<p><span class="math display">\[\begin{align*}
M_{Bin(n,p)}(t) &amp;= (pe^t + 1-p)^n\\
&amp;= (1+p(e^t-1))^n\\
&amp;= \left(1+\frac{\mu}{n}(e^t-1)\right)^n\\
&amp;\rightarrow e^{\mu(e^t-1)}=M_{Poi(\mu)}(t),\quad n\rightarrow\infty
\end{align*}\]</span></p>
</div>
<div id="multivariate-moment-generating-functions" class="section level4 hasAnchor" number="36.1.0.2">
<h4><span class="header-section-number">36.1.0.2</span> Multivariate Moment Generating Functions<a href="lecture-35-april-05-2024.html#multivariate-moment-generating-functions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="definition">
<p><span id="def:unlabeled-div-126" class="definition"><strong>Definition 36.1  </strong></span>The <strong>joint moment generating function</strong> of two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, is</p>
<p><span class="math display">\[M(s,t) = E(e^{sX+tY}).\]</span></p>
<p>And so if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent</p>
<p><span class="math display">\[M(s,t) = E(e^{sX})E(e^{tY}).\]</span></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-127" class="theorem"><strong>Theorem 36.2  </strong></span>Suppose that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent and each have moment generating functions <span class="math inline">\(M_X(t)\)</span> and <span class="math inline">\(M_Y(t)\)</span>. Then the moment generating function of <span class="math inline">\(X+Y\)</span> is</p>
<p><span class="math display">\[
M_{X+Y}(t) = E\left(e^{t(X+Y)}\right) = E\left(e^{tX}\right)E\left(e^{tY}\right)  = M_X(t) M_Y(t).
\]</span>
In general, for more than 2 random variables, the MGF of the sum is the product of the individual MGFs.</p>
</div>
</div>
<div id="comments-1" class="section level4 hasAnchor" number="36.1.0.3">
<h4><span class="header-section-number">36.1.0.3</span> Comments<a href="lecture-35-april-05-2024.html#comments-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The result from the previous slide can be used to prove the Central Limit Theorem (a simpler form of it) !!!</li>
<li>It can also be used to show that
<ul>
<li>the sum of independent Poissons is Poisson;</li>
<li>the sum of independent Binomials with the same <span class="math inline">\(p\)</span> is again Binomial;</li>
<li>the sum of independent Normals is again normal;</li>
<li>etc</li>
</ul></li>
</ul>
<div class="example">
<p><span id="exm:unlabeled-div-128" class="example"><strong>Example 36.2  </strong></span>Use mgfs to show that if <span class="math inline">\(X\sim Poi(\lambda)\)</span> is independent of <span class="math inline">\(Y\sim Poi(\mu)\)</span>, then <span class="math inline">\(X+Y\sim Poi(\lambda+\mu)\)</span>. \</p>
<p>Solution:</p>
<ul>
<li>We know that <span class="math inline">\(M_X(t)=e^{\lambda(e^t-1)}\)</span> and <span class="math inline">\(M_Y(t)=e^{\mu(e^t-1)}\)</span>.</li>
<li>Since <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, the mgf of <span class="math inline">\(X+Y\)</span> is
<span class="math display">\[\begin{align*}
M_{X+Y}(t) &amp;= M_X(t) M_Y(t)\\
&amp;= e^{\lambda(e^t-1} e^{\mu(e^t-1)}\\
&amp;= e^{(\lambda+\mu)(e^t-1)}
\end{align*}\]</span></li>
<li>This is the mgf of <span class="math inline">\(Poi(\lambda+\mu)\)</span>.</li>
<li>By uniqueness of the mgf, we conclude <span class="math inline">\(X+Y\sim Poi(\lambda+\mu)\)</span>.</li>
</ul>
</div>
</div>
</div>
<div id="chapter-999-additional-topics-will-not-be-on-the-exam-unless-otherwise-specified" class="section level2 hasAnchor" number="36.2">
<h2><span class="header-section-number">36.2</span> Chapter 999 Additional Topics (will not be on the exam unless otherwise specified)<a href="lecture-35-april-05-2024.html#chapter-999-additional-topics-will-not-be-on-the-exam-unless-otherwise-specified" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="distributions-that-do-not-have-moments" class="section level3 hasAnchor" number="36.2.1">
<h3><span class="header-section-number">36.2.1</span> Distributions that do not have moments<a href="lecture-35-april-05-2024.html#distributions-that-do-not-have-moments" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Cauchy distribution</strong>: All the distributions we have seen in the course have well-defined moments. However, it is not always the case.</p>
<div class="definition">
<p><span id="def:unlabeled-div-129" class="definition"><strong>Definition 36.2  (Cauchy distribution) </strong></span>The Cauchy distribution is a symmetric, bell-shaped distribution on <span class="math inline">\((-\infty,\infty)\)</span> with PDF
<span class="math display">\[
    f_X(x) = \frac{1}{\pi} \frac{1}{1+(x-\theta)^2},\quad -\infty &lt;x&lt;\infty ,\quad -\infty&lt;\theta &lt; \infty.
\]</span></p>
</div>
<p>But <span class="math display">\[\mathbb{E}[|X|]= \int_{-\infty}^\infty \frac{1}{\pi}\frac{|x|}{1+(x-\theta)^2}dx = \infty\]</span>, which is the neccessary condition for the expectation to exist. Also, if the first moment does not exist, the higher moments also do not exist!</p>
<p><strong>Conclusion</strong>: No moments exist!!</p>
<p>Comment: in this case, we do not have <span class="math inline">\(\mathbb{E}[X]\)</span>, and MGF is essentially a special form of the LOTUS <span class="math inline">\(\mathbb{E}[g(X)]\)</span>, where <span class="math inline">\(g(X)=\exp(X t)\)</span>. Does it exist in this case?? If not, what do we do? Can we still use the MGF? The answers are <strong>NO</strong>!!!</p>
<p>If this is the case, can we use other ways to characterize these kind of distribution?</p>
</div>
<div id="other-generating-functions" class="section level3 hasAnchor" number="36.2.2">
<h3><span class="header-section-number">36.2.2</span> Other generating functions<a href="lecture-35-april-05-2024.html#other-generating-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the course, we saw the moment generating function. But there exists other generating functions.</p>
<div id="factorial-moment-generating-function-fmgf" class="section level4 hasAnchor" number="36.2.2.1">
<h4><span class="header-section-number">36.2.2.1</span> Factorial Moment Generating function (FMGF)<a href="lecture-35-april-05-2024.html#factorial-moment-generating-function-fmgf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
    G_X(t):= \mathbb{E}[ t^X ],
\]</span>
provided the expectation exists for all <span class="math inline">\(t\)</span> in some interval of the form <span class="math inline">\(1-h&lt;t&lt;1+h\)</span>.\</p>
<p>The relationship between the FMGF and MGF is as follows:
<span class="math display">\[
    G_X(t) = \mathbb{E}[t^X] = E[e^{X\ln(t)}] = M_X\{\ln(t)\}
\]</span></p>
FMGF can be used to calculate:
<span class="math display">\[
\frac{d^r}{dt^r} G_X(t)|_{t=1} = \mathbb{E}[X(X-1)\cdots (X-r+1)].
\]</span>
In particular we have
<p>What if we want to have the central moments, say <span class="math inline">\(\mathbb{E}[X^2]\)</span>?</p>
<p><span class="math display">\[\begin{align*}
\mathbb{E}[X^2] &amp;= \mathbb{E}[X] + \mathbb{E}\left\{X(X-1)\right\} \\
&amp;= \mathbb{E}[X] + G_X^{\prime\prime}(1).
\end{align*}\]</span></p>
</div>
<div id="characteristic-function" class="section level4 hasAnchor" number="36.2.2.2">
<h4><span class="header-section-number">36.2.2.2</span> Characteristic function<a href="lecture-35-april-05-2024.html#characteristic-function" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Characteristic function:
<span class="math display">\[
    \phi_X(t) = \mathbb{E}[e^{itX}],
\]</span>
where <span class="math inline">\(i\)</span> is the complex number <span class="math inline">\(\sqrt{-1}\)</span>. Note that this definition requires a complex integration. It <strong>ALWAYS EXISTS</strong> and completely determines the distribution, that is, every (C)DF has a unique characteristic function.
<span class="math display">\[
\phi_X(-it) =  M_X(t).
\]</span>
E.g. if <span class="math inline">\(X\sim N(0,1),~Y\sim U(-1,1)\)</span> and <span class="math inline">\(Z\)</span> follows a Cauchy distribution, then we have
<span class="math inline">\(\phi_X(t) = \exp(-t^2/2)\)</span>, <span class="math inline">\(\phi_Y(t)=\sin(t)/t\)</span> and <span class="math inline">\(\phi_Z(t)=\exp(-|t|)\)</span></p>
</div>
<div id="cumulant-generating-function-cgf" class="section level4 hasAnchor" number="36.2.2.3">
<h4><span class="header-section-number">36.2.2.3</span> Cumulant generating function (CGF):<a href="lecture-35-april-05-2024.html#cumulant-generating-function-cgf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math display">\[
    \ln\{M_X(t)\}
\]</span>
This is used to generated the <em>cumulants</em> of <span class="math inline">\(X\)</span>, which is defined as the coefficients of the Taylor series of the CGF.</p>
</div>
</div>
<div id="delta-method" class="section level3 hasAnchor" number="36.2.3">
<h3><span class="header-section-number">36.2.3</span> Delta Method<a href="lecture-35-april-05-2024.html#delta-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What else can we do if we know a random variable follows a normal distribution? <strong>YES</strong>!!!</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-130" class="theorem"><strong>Theorem 36.3  (Delta method) </strong></span>Let <span class="math inline">\(Y\)</span> be a random variable that satisfies <span class="math inline">\(\sqrt{n}(Y-\theta)\to \mathcal{N}(0,\sigma^2)\)</span>. For a given function <span class="math inline">\(g(\cdot)\)</span> and a specific value of <span class="math inline">\(\theta\)</span>, suppose that <span class="math inline">\(g^\prime(\theta)\)</span> exist and is not 0. Then
<span class="math display">\[
  \sqrt{n}(g(Y)-g(\theta)\ \to \mathcal{N}(0,\sigma^2[g^\prime(\theta)]^2).
\]</span></p>
</div>
<p>The proof is using the 2nd order Taylor expansion.</p>
<div class="example">
<p><span id="exm:unlabeled-div-131" class="example"><strong>Example 36.3  (Example of the usage of delta method) </strong></span>Suppose that <span class="math inline">\(X_1,\cdots,X_N\sim F\)</span> are i.i.d. with a common mean <span class="math inline">\(\mu\)</span> and a finite variance.</p>
<p>Consider a linear combination that <span class="math inline">\(\bar{X}_N := N^{-1}\sum_{i=1}^NX_i\)</span>.
What is the distribution of
<span class="math inline">\(\sqrt{N}\left\{(\bar{X}_N)^{-1}-\mu^{-1}\right\}\)</span>?</p>
<p>Since we know from the CLT that
<span class="math display">\[
    \sqrt{N}(\bar{X}_N - \mu) \to \mathcal{N}(0,\mathbb{V}ar(X_1))
\]</span>
Then, applying the delta method, we have
<span class="math display">\[
    \sqrt{N}(\bar{X}_N^{-1} - \mu^{-1}) \to \mathcal{N}\left\{0, \mu^{-4} \mathbb{V}ar(X_1)\right\}.
\]</span></p>
</div>
</div>
<div id="other-interesting-results" class="section level3 hasAnchor" number="36.2.4">
<h3><span class="header-section-number">36.2.4</span> Other interesting results<a href="lecture-35-april-05-2024.html#other-interesting-results" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="taylor-expansion" class="section level4 hasAnchor" number="36.2.4.1">
<h4><span class="header-section-number">36.2.4.1</span> Taylor expansion<a href="lecture-35-april-05-2024.html#taylor-expansion" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Taylor expansion/approximate/series (power series) is a very important tools in many fields, including statistics. We can use Taylor series to prove many results including the CLT, delta method, approximating the moments, and many more.</p>
<div class="definition">
<p><span id="def:unlabeled-div-132" class="definition"><strong>Definition 36.3  ((One dimensional) Taylor expansion) </strong></span>If a function <span class="math inline">\(h(x)\)</span> has derivative up to order <span class="math inline">\(r\)</span>, that is <span class="math inline">\(\frac{d^r}{dx^r}h(x)|_{x=a} :=h^{(r)}(x)\)</span> exists, then for any constant <span class="math inline">\(a\in\mathbb{R}\)</span>, the Taylor polynomial of order <span class="math inline">\(r\)</span> about <span class="math inline">\(a\)</span> is given as
<span class="math display">\[
  T_r(x):= \sum_{j=1}^r \frac{h^{(j)}(a)}{j!}(x-a)^j.
\]</span></p>
</div>
<p>If we use the Taylor series to make approximate on <span class="math inline">\(h(x)\)</span>, one may wonder what is the error of this approximate.</p>
<div class="definition">
<p><span id="def:unlabeled-div-133" class="definition"><strong>Definition 36.4  (Remainder/Error of Taylor expansion) </strong></span>The remainder/error of the Taylor expansion on the function <span class="math inline">\(h(x)\)</span> with order <span class="math inline">\(r\)</span> is given by
<span class="math display">\[
  R(x) := h(x) - T_r(x) = \int_a^x \frac{h^{(r+1)}(t)}{r!}(x-t)^r dt.
\]</span></p>
</div>
<p>We have another theorem that is useful in the discussion</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-134" class="theorem"><strong>Theorem 36.4  </strong></span>If <span class="math inline">\(h^{(r)}(a) =\frac{d^r}{dx^r}h(x)|_{x=a}\)</span> exists, then
<span class="math display">\[
  \lim_{x\to a} \frac{h(x)-T_r(x)}{(x-a)^r} =0.
\]</span></p>
</div>
<p>Most of the statistical application only uses the first or the second order approximation. We may show how to use those approximations to estimate the mean and the variance.</p>
<div class="example">
<p><span id="exm:unlabeled-div-135" class="example"><strong>Example 36.4  (Approximate of mean and variance using first/second order Taylor series) </strong></span>Assume <span class="math inline">\(X\)</span> is a random variable with the first moment not equal to zero, i.e. <span class="math inline">\(\mathbb{E}[X] \ne 0\)</span>. Then if we do the Taylor expansion, we have
<span class="math display">\[
  h(X) = h(\mu) + h^\prime(\mu)(X-\mu).
\]</span>
If <span class="math inline">\(h(X)\)</span> is an estimator of <span class="math inline">\(h(\mu)\)</span> (i.e. when the sample size is large, <span class="math inline">\(h(X)\)</span> is approximately equals to <span class="math inline">\(h(\mu)\)</span>), then we have
<span class="math display">\[\begin{align*}
  E[h(X)] &amp;\approx E[h(\mu)] + h^\prime(\mu) \mathbb{E}[X-\mu]\\
  &amp;= h(\mu) + h^\prime (\mu) \{\mathbb{E}X - \mu\}\\
  &amp;=h(\mu) + h^\prime (\mu) \{\mu - \mu\}\\
  &amp;=h(\mu).
\end{align*}\]</span>
Thus, we can approximate the expectation of <span class="math inline">\(g(X)\)</span> using <span class="math inline">\(g(\mu)\)</span>.</p>
<p>Similar for the variance <span class="math inline">\(\mathbb{V}ar(g(X))\)</span>, we can use <span class="math inline">\(g^{\prime \prime}(\mu)\)</span> to approximate. This two approximate lead us to the <strong>delta method</strong> we have seen above.</p>
</div>
</div>
<div id="proof-for-clt" class="section level4 hasAnchor" number="36.2.4.2">
<h4><span class="header-section-number">36.2.4.2</span> Proof for CLT<a href="lecture-35-april-05-2024.html#proof-for-clt" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>First we recall/restate the CLT</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-136" class="theorem"><strong>Theorem 36.5  (CLT revisit) </strong></span>Let <span class="math inline">\(X_1,\cdots,X_n\)</span> be a sequence of i.i.d. random variables with the existence of MGF in a neighbour of <span class="math inline">\(0\)</span> (i.e. <span class="math inline">\(M_{X_i}(t)\)</span> exists for <span class="math inline">\(|t|&lt;h\)</span> for an arbitrary <span class="math inline">\(h&gt;0\)</span>. Let <span class="math inline">\(\mathbb{E}X_i= \mu&lt;\infty\)</span> and <span class="math inline">\(\mathbb{V}ar(X_i) = \sigma^2 &lt;\infty\)</span>. Denote the mean be <span class="math inline">\(\bar{X}_n=n^{-1}\sum_{i=1}^n X_i\)</span>. Let <span class="math inline">\(G_n(x)\)</span> be the CDF of <span class="math inline">\(Z_n:= \sqrt{n}(\bar{X}_n-\mu)/\sigma\)</span>. Then for any <span class="math inline">\(x\in(-\infty,\infty)\)</span>,
<span class="math display">\[
  \lim_{n\to \infty} G_n(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}}\exp(\frac{-y^2}{2})dy,
\]</span>
which means <span class="math inline">\(Z_n\to \mathcal{N}(0,1)\)</span> as <span class="math inline">\(n\to \infty\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-137" class="proof"><em>Proof</em> (Proof for CLT). </span>Disclaimer: This proof is based on Casella and Berger (2002).</p>
<p>First, define the standardized random variables <span class="math inline">\(Y_i:= (X-\mu)/\sigma\)</span> with common MGF <span class="math inline">\(M_Y(t)\)</span> that exists for <span class="math inline">\(|t|&lt;\sigma h\)</span>. We can further write
<span class="math display">\[
  \frac{\sqrt{n}(\bar{X}_n-\mu)}{\sigma} = \frac{1}{\sqrt{n}}\sum_{i=1}^nY_i.
\]</span>
Using the uniqueness of MGF, we know that
<span class="math display">\[\begin{align*}
M_{Z_n}(t) &amp;= M_{\sqrt{n}(\bar{X}_n - \mu)/\sigma}(t) = M_{\frac{1}{\sqrt{n}}\sum_{i=1}^nY_i}(t)\\
&amp;=M_{\sum_{i=1}^n Y_i} (\frac{t}{\sqrt{n}})\\
&amp;=\left\{ M_Y\left(\frac{t}{\sqrt{n}}\right)\right\}^n.
\end{align*}\]</span>
We will then do the Taylor expansion on <span class="math inline">\(M_Y\left(\frac{t}{\sqrt{n}}\right)\)</span> around <span class="math inline">\(0\)</span>. Using the Taylor series, we have
<span class="math display">\[
  M_Y\left(\frac{t}{\sqrt{n}}\right) = \sum_{j=0}^\infty \frac{d^j}{dt^j}M_Y(t)|_{t=0}\frac{(t/\sqrt{n})^j}{j!},
\]</span>
where it exists when <span class="math inline">\(t&lt; \sqrt{n}\sigma h\)</span>. Since we know the mean and variance for the standard normal is <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> respectively, and the 0-th moment is <span class="math inline">\(1\)</span>, we have
<span class="math inline">\(\frac{d^0}{dt^0}M_Y(t)|_{t=0} = 1\)</span>, <span class="math inline">\(\frac{d}{dt}M_Y(t)|_{t=0} = 0\)</span> and <span class="math inline">\(\frac{d^2}{dt^2}M_Y(t)|_{t=0} = 1\)</span>. Using those facts, we can write out the Taylor series as
<span class="math display">\[
  M_Y(\frac{t}{\sqrt{n}})= 1 + \frac{(t/\sqrt{n})^2}{2!} + R_Y(\frac{t}{\sqrt{n}}),
\]</span>
where <span class="math inline">\(R_Y(\cdot)\)</span> is the remainder/approximation error such that
<span class="math display">\[
  R_Y(\frac{t}{\sqrt{n}}) = \sum_{j=1}\frac{d^j}{dt^j}M_Y(t)|_{t=0} \cdot \frac{(t/\sqrt{n})^j}{j!}.
\]</span>
By Theorem 36.4, we have, for <span class="math inline">\(t\ne0\)</span>,
<span class="math display">\[
  \lim_{n\to \infty} \frac{R_Y(t/\sqrt{n})}{(t/\sqrt{n})^2}=0.
\]</span>
For other terms, we also may also elminiate them as
<span class="math display">\[
  \lim_{n\to \infty}\frac{R_Y(t/\sqrt{n})}{(1/\sqrt{n})^2} = \lim_{n\to \infty}R_y(\frac{t}{\sqrt{n}}) = 0,
\]</span>
and for <span class="math inline">\(t=0\)</span>, we have <span class="math inline">\(R_Y(0/\sqrt{n})=0\)</span>. Thus, for any fixed <span class="math inline">\(t\)</span>, we can write the Taylor expansion of the MGF as
<span class="math display">\[\begin{align*}
  \lim_{n\to \infty} \left\{ M_Y\left(\frac{t}{\sqrt{n}}\right)\right\}^n
  &amp;= \lim_{n\to\infty}\left\{1+\frac{(t/\sqrt{n})^2}{2!} + R_Y(\frac{t}{\sqrt{n}})\right\}^n\\
  &amp;=\lim_{n\to\infty}\left\{1+\frac{1}{n}\left(\frac{t^2}{2} + nR_Y(\frac{t}{\sqrt{n}})\right)\right\}^n\\
  &amp;= \exp\left(\frac{t^2}{2}\right),
\end{align*}\]</span>
where the last equality is from the identity of exponential function. By the uniqueness of the MGF, and the fact that <span class="math inline">\(\exp\left(\frac{t^2}{2}\right)\)</span> is the MGF of the standard normal distribution, we conclude that we <span class="math inline">\(Z_n= \sqrt{n}(\bar{X}_n-\mu)/\sigma\)</span> follows a standard normal distribution <span class="math inline">\(\mathcal{N}(0,1)\)</span>. This concludes the proof.</p>
</div>
<p>Reference:</p>
<ul>
<li>Casella, G., and Berger, R.L. (2002). <em>Statistical lnference</em>. Duxbury Press.</li>
</ul>
<p><strong>TODO</strong>:</p>
<p>I wlll add something during the weekend, if not on Monday so stay tune!</p>
<p>Things I plan to add</p>
<ul class="task-list">
<li><p><label><input type="checkbox" checked="" />Result of Taylor expansion, review and its usage in proving the Delta method</label></p></li>
<li><p><label><input type="checkbox" checked="" />The proof for CLT, this also involves the Taylor expansion</label></p></li>
</ul>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="lecture-34-april-03-2024.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="lecture-36-april-08-2024.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/chikuang/REPO/edit/BRANCH/35-Lec35.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
