[["index.html", "Stat 230 Introduction to Probability Winter 2024 1 Information of the course 1.1 Course description 1.2 Chapters and associated Lectures", " Stat 230 Introduction to Probability Winter 2024 Chi-Kuang Yeh University of Waterloo 2024-01-19 1 Information of the course The purpose of this page is to hold some of the additional materials provided by myself. Students should consult UW Learn system. 1.1 Course description This course provides an introduction to probability models including sample spaces, mutually exclusive and independent events, conditional probability and Bayes’ Theorem. The named distributions (Discrete Uniform, Hypergeometric, Binomial, Negative Binomial, Geometric, Poisson, Continuous Uniform, Exponential, Normal (Gaussian), and Multinomial) are used to model real phenomena. Discrete and continuous univariate random variables and their distributions are discussed. Joint probability functions, marginal probability functions, and conditional probability functions of two or more discrete random variables and functions of random variables are also discussed. Students learn how to calculate and interpret means, variances and covariances particularly for the named distributions. The Central Limit Theorem is used to approximate probabilities. 1.1.1 Instructor Chi-Kuang Yeh, I am a postdoc at the Department of Statistics and Actuarial Science. Office: M3–3102 Desk 10. I will hold office hour in another location. Email: chi-kuang.yeh@uwaterloo.ca 1.1.2 Course Coordinator Dr. Erik Hintz. Office: M3–2106 Email: erik.hintz@uuwaterloo.ca 1.1.3 Logistic Issue Contact Divya Lala Email: divya.lala@uwaterloo.ca or the undergrad advising email sasugradadv@uwaterloo.ca. 1.1.4 EXAM and Tutorial assessment Date Midterm Midterm 1: February 08, 16:30–17:50 Midterm 2: March 14, 16:30–17:50 Final To be announced by the university Tutorial assessment Tutorial quiz 1: January 26 Tutorial test 1: February 02 Tutorial quiz 2: March 01 Tutorial test 2: March 08 Tutorial quiz 3: March 22 Tutorial test 3: April 05 1.2 Chapters and associated Lectures Those chapters are based on the lecture notes. The lecture covered is based on Section 002. This part will be updated frequently. Chapter Title Lecture Covered 1 Introduction to Probability 1 2 Mathematical Probability Models 2–3 3 Probability and Counting Techniques 3–6 4 Probability rules and Conditional Probability 6– 5 TBA TBA 6 TBA TBA 7 TBA TBA 8 TBA TBA 9 TBA TBA 10 TBA TBA "],["lecture-1-january-08-2024.html", "2 Lecture 1, January 08, 2024", " 2 Lecture 1, January 08, 2024 In this lecture, we went over Course syllabus and rules Chapter 1 – Basic definition of probability. We also saw the potential ambiguities when defining probabilities. Definition 2.1 (Classical Definition of probability) The classical definition: The probability of some event is \\[ \\frac{\\mathrm{number~of~ways~the~event~can~occur~}} {\\mathrm{{the~total~number~of~possible~outcomes}}}, \\] provided all outcomes are equally likely. Definition 2.2 (Relative Frequency Definition of of probability) The relative frequency definition: The probability of an event is the (limiting) proportion (or fraction) of times the event occurs in a very long series of repetitions of an experiment. Definition 2.3 (Subjective Definition of Probability) The subjective definition: The probability of an event is a measure of how sure the person making the statement is that the event will happen. Problem: Each of the above definitions has pitfall: Classical: We may not be able to know the total number of possible outcomes, or it may be uncountable Relative frequency: We need “repetition”, which is often expensive and may not be possible. Subjective: We want the probability to be consistent across different people, and and be rigorously defined. "],["lecture-2-january-10-2024.html", "3 Lecture 2, January 10, 2024 3.1 Questions from the class", " 3 Lecture 2, January 10, 2024 In this lecture, we went over some basic definitions from the set theory, and using them as the building block for the rest of the course. We started Chapter 2 today, with many definitions. As for the set operations, \\(\\cup,\\cap,A^c,...\\), the Venn diagrams help to visual the meaning behind. Here is a good reference HERE. Definition 3.1 (sample space) A sample space \\(S\\) is a set of distinct outcomes of an experiment with the property that in a single trial of the experiment only one of these outcomes occurs. Definition 3.2 (Discrete and non-discrete sample space) A sample space \\(S\\) is said to be discrete if it is finite, or ``countably infinite” (i.e.,there is a one-to-one correspondence with the natural numbers). Otherwise a sample space is said to be non-discrete. Definition 3.3 (Event) An event is a subset of the sample space that can be assigned probability. Definition 3.4 (Simple and Compound event) Let \\(S\\) be discrete and \\(A\\subset S\\) an event. If \\(A\\) is indivisible so it contains only one point, we call it a simple event, otherwise compound event. Definition 3.5 (Probability distribution) Let \\(S=\\{a_1,a_2,\\dots\\}\\) be discrete. Assign numbers \\(P(\\{a_i\\})\\) (or short: \\(P(a_i)\\)), \\(i=1,2,\\dots\\), so that 1.\\(0\\leq P(a_i)\\leq 1,\\quad i=1,2,\\dots\\) 2. \\(\\sum_{\\text{all }i}P(a_i)=1\\). We then call the set of probabilities \\(\\{P(a_i):i=1,2,\\dots\\}\\) a probability distribution. Definition 3.6 Let \\(S=\\{a_1,a_2,\\dots\\}\\) discrete. From any prob. distribution \\(P\\) on \\(S\\) we can define a prob. measure on $ {S} = 2^S$ (set of all subsets of \\(S\\)) by \\[\\forall A \\subseteq S \\qquad P(A)=\\sum_{a_i\\in A}P(a_i).\\] Definition 3.7 (Equally likely) We say a sample space \\(S\\) with a finite number of outcomes is equally likely if the probability of every individual outcome in \\(S\\) is the same. Observe that If \\(|A|\\) denote the number of outcomes in an event \\(A\\). In case of an equally likely sample space, \\[ 1=P(S)=\\sum_{i=1}^{|S|}P(a_{i})= P(a_i)|S|. \\] \\[ P(a_i)=\\frac{1}{|S|}. \\] Hence, \\[P(A) = \\sum_{i:\\;a_i \\in A} P(a_i) = \\sum_{i:\\;a_i \\in A} \\frac{1}{|S|} =|A|\\cdot \\frac{1}{|S|}\\] Conclusion: In a finite, equally likely sample space, the probability of an event \\(A\\) can be computed as \\[ P(A) = \\sum_{i:\\;a_i \\in A} P(a_i) = \\frac{|A|}{|S|}. \\] 3.1 Questions from the class What is the difference between “countably infinite” v.s. “infinite”? Ans: A set is countably infinite if its elements can be put in one-to-one correspondence with the set of natural numbers \\(\\mathbb{N}\\). Alternatively, you can think that a set is countably infinite if you can count off all elements in the set in such a way that, even though the counting will take forever, you will get to any particular element in a finite amount of time. [A good reference page to read]. If a set is not countable or countably infinite, it is infinite. Why do we have something such as \\(2^\\mathcal{S}\\) in the lecture? Ans: It is related to something called the power set. The power set consists all the possible subset of a set \\(\\mathcal{S}\\). In a subset of \\(S\\), (i.e. \\(A \\subseteq \\mathcal{S}\\), every element in \\(\\mathcal{S}\\) may be either in \\(A\\) or not in \\(A\\). Which means, each element has two possibilities, in \\(A\\) or not in \\(A\\). Hence, the cardinality (i.e. the size) of the power set is \\(2^\\mathcal{S}\\). What did we mean by “order does not matter” and “order matters” during the lecture. Order does not matter: I said when you write out the element of a set, the order does not matter. For instance, in the rolling a six-sided dice, which side would be faced on the top example, we can write \\(\\mathcal{S} = \\{1,2,3,4,5,6\\}\\), or \\(\\mathcal{S}^\\prime=\\{6,5,4,3,2,1\\}\\), and those two sets are essentially equal to each other. To represent a set, the order does not matter, but we tend to write in a way that is intuitive and easy to understand. Order does matter: In rolling two dices example, the dots show on each of the dice is an ordered pair, denoted by \\((x,y)\\). Hence, for instance, \\((1,2)\\) and \\((2,1)\\) are different. It is problem-dependent so be careful. "],["lecture-3-january-12-2024.html", "4 Lecture 3, January 12, 2024 4.1 Questions from the class", " 4 Lecture 3, January 12, 2024 Definition 4.1 (Odds) Odds in favour of an event \\(A\\) occurring is \\[ O(A) := \\frac{P(A)}{1-P(A)}. \\] Odds again an event \\(A\\) is \\[ \\frac{1-P(A)}{P(A)}. \\] The range of the odds is \\([0,\\infty)\\). It provide a measure of the likelihood of a particular outcome to happen. Abbreviation: ““p:q”. Note: Probability may be defined through the odds as follow. \\[\\begin{align*} &amp;O(A) := \\frac{P(A)}{1-P(A)} \\\\ &amp;\\implies O(A) - P(A)O(A) = P(A) \\\\ &amp;\\implies O(A) = P(A) (1+O(A)) \\\\ &amp;\\implies P(A) = \\frac{O(A)} {1+O(A)} \\end{align*}\\] Note: In finite, equally likely sample spaces, computing probabilities amounts to counting the number of elements in a set. It will often be difficult to do this manually, so we are looking for clever counting techniques in the next chapter. Chapter 3 Counting Techniques Addition rule v.s. Multiplication rule For addition rule Keyword for addition rule is “OR”; \\(|A|\\) is defined to be the size of the set, aka the cardinality of the set. If \\(A\\) and \\(B\\) are disjoint (i.e. \\(A\\cap B = \\emptyset\\)), then \\(|A\\cup B| = |A| + |B|\\). \\(A\\cup A^c = S\\) where \\(A \\cap A^c = \\emptyset\\). Thus \\(|S|=|A|+|A^c|\\). For multiplication rule for multiplication rule is “AND” An ordered k-tuple is an ordered set of \\(k\\) values: \\((a_1,a_2,\\dots,a_k)\\). If the outcomes in A can be wrttien as an ordered k-tuple where there are \\(n_1\\) choices for \\(a_1\\), \\(n_2\\) choices for \\(a_2,\\dots\\) and in general \\(n_i\\) choices for \\(a_i\\), then \\[ |A| = n_1n_2\\cdots n_k = \\prod_{i=1}^k n_i. \\] Definition 4.2 (Factorial) Given \\(n\\) distinct objects, there are \\[ n! = n \\times (n-1) \\times \\ldots 2 \\times 1, \\] different ordered arrangements of length \\(n\\) that can be made. Note that, we define, \\(0! = 1\\). We pronounce \\(n!\\) as “n factorial”. The following recursive definition is useful: \\[ n! = n \\cdot (n-1)! \\] When working with factorials, we can often cancel terms, e.g., \\[ \\frac{9!}{7!} = \\frac{9\\cdot 8 \\cdot 7\\cdot 6 \\cdot \\dots \\cdot 2 \\cdot 1}{7\\cdot 6 \\cdot \\dots \\cdot 2 \\cdot 1}=9\\cdot 8 = 72\\] Definition 4.3 (Permutation) Given \\(n\\) distinct objects, a permutation of size \\(k\\) is an \\(ordered\\) subset of \\(k\\) of the individuals. The number of permutations of size \\(k\\) taken from \\(n\\) objects is denoted \\(n^{(k)}\\) and \\[ n^{(k)}=n(n-1)\\dots (n-k+1) =\\frac{n!}{(n-k)!}. \\] The tricky part of this definition is the word “ordered”. An ordering need not be numerical, for example assigning labels like “President” and “Vice-President” has the effect of ordering the individuals. 4.1 Questions from the class Can we express the odds as \\(a:b\\)? YES. For instance, the example we saw in class (or the clicker question 1), if we roll a fair six sided dice, and let our event \\(A:=\\{\\text{# is } 5 \\}\\). Then the odds \\(O(A)=\\frac{1/6}{1/5}=\\frac{1}{5}\\). We can see that, there is exactly one possibility we have event \\(A\\), whereas there are 5 possibilities that \\(A\\) does not happen (i.e. the number we roll out is \\(1,2,3,4,6\\)). We can abbreviate it as “1:5”. For a good example of Odds, WIKI provides a good one. "],["lecture-4-january-15-2024.html", "5 Lecture 4, January 15, 2024", " 5 Lecture 4, January 15, 2024 Definition 5.1 (Combination) Given \\(n\\) distinct objects, a combination of size \\(k\\) is an unordered subset of \\(k\\) of the individuals. The number of combinations of size \\(k\\) taken from \\(n\\) objects is denoted \\({n \\choose k}\\) or \\({}_n C_k\\) and can be computed as \\[ {n \\choose k}=\\frac{n^{(k)}}{k!}=\\frac{n!}{(n-k)!\\ k!}. \\] "],["lecture-5-january-17-2024.html", "6 Lecture 5, January 17, 2024 6.1 Example in class", " 6 Lecture 5, January 17, 2024 Properties of the Binomial coefficients There are some useful/important results about permutation and combination. \\(n^{(k)} = n (n - 1)^{(k-1)}\\) for \\(k \\geq 1\\) \\({n \\choose k} = \\frac{n^{(k)}}{k!}\\) \\({n \\choose k} = {n \\choose n-k}\\) for \\(k \\geq 0\\) \\({n \\choose k} = {n-1 \\choose k-1} + {n-1 \\choose k}\\) Binomial theorem: \\((1 + x)^n = \\sum_{k=0}^n {n \\choose k} x^k\\) \\({n \\choose k}\\) is equal to the \\(k\\)th entry in the \\(n\\)th row of Pascal’s triangle. Note: Many of these idenetity may be proven using something called combinatorial proof. See Wiki for an (easy) example. Proof (4). \\[\\begin{align*} {n-1 \\choose k-1} + {n-1 \\choose k} &amp;= \\frac{(n-1)!}{(k-1)! (n-k)!} + \\frac{(n-1)!}{k! (n-k-1)!}\\\\ &amp;= \\frac{(n-1)!k }{(k-1)! (n-k)!k} + \\frac{(n-1)!(n-k)}{k! (n-k-1)!(n-1)} \\\\ &amp;= \\frac{(n-1)!k + (n-1)! (n-k)}{k! (n-k)!} \\\\ &amp;- \\frac{(n-1)!( k + (n-k))}{k! (n-k)!} \\\\ &amp;= \\frac{(n-1)! n}{k! (n-k)!} \\\\ &amp;= {n \\choose k} \\end{align*}\\] Aside: Stirling’s formula \\(n!\\) grows really fast as \\(n\\) increases, so sometimes we need to approximate its value for computational reasons. Stirling’s formula provides one such method, and it is given by \\[ n! \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n, \\] where \\(\\sim\\) means their ratio approaches 1 as \\(n\\) goes to infinity. We won’t need this approximation, but it’s useful to know it exists. Example of use: Show that \\(2^{-2n}\\binom{2n}{n} \\approx \\sqrt{\\frac{2}{\\pi n}}\\) 6.1 Example in class Example 6.1 (Application of Stirling's Formula/Approximation for factorial) Show that \\(2^{-2n}{2n \\choose n} \\approx \\sqrt{\\frac{1}{\\pi n}}\\) \\[\\begin{align*} 2^{-2n}{2n \\choose n} &amp;= 2^{-2n}\\frac{2n!}{n!n!} \\\\ &amp;\\approx 2^{-2n} \\frac{\\sqrt{2\\pi (2n)} (2n/e)^{2n}}{\\sqrt{2\\pi n} (n/e)^{n}\\sqrt{2\\pi n} (n/e)^{n}}\\\\ &amp;= 2^{-2n} \\frac{\\sqrt{4}}{\\sqrt{2}\\sqrt{2}} \\frac{\\sqrt{\\pi n}}{\\sqrt{\\pi n}\\sqrt{\\pi n}} \\frac{(2n)^{2n}}{n^n n^n} \\frac{e^{-2n}}{e^{-2n}}\\\\ &amp;= 2^{-2n} \\frac{1}{\\sqrt{\\pi n}} 2^{2n}\\\\ &amp;= \\frac{1}{\\sqrt{\\pi n}} = \\sqrt{\\frac{1}{\\pi n}} \\end{align*}\\] "],["lecture-6-january-19-2024.html", "7 Lecture 6, January 19, 2024", " 7 Lecture 6, January 19, 2024 7.0.1 Multinomial Coefficient Definition 7.1 (Multinomial Coefficient) Consider \\(n\\) objects which consist of \\(k\\) types. Suppose that there are \\(n_1\\) objects which are of type 1, \\(n_2\\) which are of type 2, and in general \\(n_i\\) objects of type \\(i\\). Then there are \\[ \\frac{n!}{n_1 ! n_2! \\dots n_k !} \\] distinguishable arrangements of the \\(n\\) objects. This quantity is known as a multinomial coefficient and denoted by \\[ \\binom{n}{n_1,n_2,\\dots,n_k}= \\frac{n !}{n_1 ! n_2! \\dots n_k !}. \\] Note: Multinomial coefficient is an extension of the binomial coefficient. In binomial coefficient, there are only two groups/objects, and the first type has size \\(n_1\\) and the size of the second type is consequently \\(n-n_1\\), where \\(n\\) is the total number of objects. Hence we have \\({n \\choose n_1} = \\frac{n!}{n_1! (n-n_1)!}\\). Try to compare this with the multinomial coefficient. 7.0.2 The Birthday Problem Suppose a room contains \\(n\\) people. What is the probability at least two people in the room share a birthday? Assumption: Suppose that each of the \\(n\\) people is equally likely to have any of the 365 days of the year as their birthday, so that all possible combinations of birthdays are equally likely. Let \\(A\\) be the event that at least two people share a birthday. Then \\[ P(A) = 1 - P(A^c),\\] where \\(A^c\\) is the event that nobody shares birthday with each other. For \\(n\\) people to have unique birthdays, we need to arrange them among 365 days w/o replacement. Thus, \\[|A^c| = 365^{(n)}.\\] For the size of the sample space, we see that each person has 365 possibilities for their birthday. Thus, \\[|S| = 365^n.\\] Since we are assuming that all possible combinations of birthdays are equally likely, our desired probability becomes \\[ P(A) = 1 - P(A^c) = 1 - \\frac{365^{(n)}}{365^n} = 1 - \\frac{n! {365 \\choose n}}{365^n}. \\] For \\(n\\in\\{100, 30, 23\\}\\) we find \\[P(A_{100})= .9999997,\\;\\;\\; P(A_{30})=.7063 \\;\\;\\;\\; P(A_{23})=.5073.\\] 7.0.3 Chapter 4 Probbility Rules and Conditional Probability Review the Venn Diagram "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
