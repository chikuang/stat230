[["index.html", "Stat 230 Introduction to Probability Winter 2024 1 Information of the course 1.1 Course description 1.2 Chapters and associated Lectures", " Stat 230 Introduction to Probability Winter 2024 Chi-Kuang Yeh University of Waterloo 2024-03-24 1 Information of the course The purpose of this page is to hold some of the additional materials provided by myself. Students should consult UW Learn system. 1.1 Course description This course provides an introduction to probability models including sample spaces, mutually exclusive and independent events, conditional probability and Bayes’ Theorem. The named distributions (Discrete Uniform, Hypergeometric, Binomial, Negative Binomial, Geometric, Poisson, Continuous Uniform, Exponential, Normal (Gaussian), and Multinomial) are used to model real phenomena. Discrete and continuous univariate random variables and their distributions are discussed. Joint probability functions, marginal probability functions, and conditional probability functions of two or more discrete random variables and functions of random variables are also discussed. Students learn how to calculate and interpret means, variances and covariances particularly for the named distributions. The Central Limit Theorem is used to approximate probabilities. 1.1.1 Instructor Chi-Kuang Yeh, I am a postdoc at the Department of Statistics and Actuarial Science. Office: M3–3102 Desk 10, but I hold office hour at M3 - 2101 Desk 1, 9:30 – 10:30 on Tuesday. Email: chi-kuang.yeh@uwaterloo.ca 1.1.2 Course Coordinator Dr. Erik Hintz. Office: M3–2106 Email: erik.hintz@uuwaterloo.ca 1.1.3 Logistic Issue Contact Divya Lala Email: divya.lala@uwaterloo.ca or the undergrad advising email sasugradadv@uwaterloo.ca. 1.1.4 EXAM and Tutorial assessment Date Midterm Midterm 1: February 08, 2024 16:30–17:50 (Coverage: Ch. 1 – 5.1) Midterm 2: March 14, 2024 16:30–17:50 (Coverage: Ch. 1–5, 7-8, up to Sec. 8.3) Final Tuesday April 16, 2024 19:30 – 22:00. Location: DC 1350 and DC 1351 Tutorial assessment Tutorial quiz 1: January 26, 2024 (Coverage: Ch. 1–3) Tutorial test 1: February 02, 2024 (Coverage: Ch. 1–4) Tutorial quiz 2: March 01, 2024 (Coverage: Ch. 1-4, and Ch. 7, up to Sec. 7.3) Tutorial test 2: March 08, 2024 (Coverage: Ch. 1-5, 7-8 up to Sec. 8.1) Tutorial quiz 3: March 22, 2024 (Coverage: Up to Sec. 9.1, exclude the independence) Tutorial test 3: April 05, 2024 1.2 Chapters and associated Lectures Those chapters are based on the lecture notes. The lecture covered is based on Section 002. This part will be updated frequently. Chapter Lecture Covered 1. Introduction to Probability L1 2. Mathematical Probability Models L2–3 3. Probability and Counting Techniques L3–6 4. Probability rules and Conditional Probability 6–9 5. Discrete Random Variable L10 –16 6. Computational Methods and the Statistical Software R In tutorial (not testable) 7. Expected Value and Variance L16 –20 8. Continuous Random Variable L20 – 27 9. Multivariate Distributions L27 – 10. TBA TBA "],["lecture-1-january-08-2024.html", "2 Lecture 1, January 08, 2024", " 2 Lecture 1, January 08, 2024 In this lecture, we went over Course syllabus and rules Chapter 1 – Basic definition of probability. We also saw the potential ambiguities when defining probabilities. Definition 2.1 (Classical Definition of probability) The classical definition: The probability of some event is \\[ \\frac{\\mathrm{number~of~ways~the~event~can~occur~}} {\\mathrm{{the~total~number~of~possible~outcomes}}}, \\] provided all outcomes are equally likely. Definition 2.2 (Relative Frequency Definition of of probability) The relative frequency definition: The probability of an event is the (limiting) proportion (or fraction) of times the event occurs in a very long series of repetitions of an experiment. Definition 2.3 (Subjective Definition of Probability) The subjective definition: The probability of an event is a measure of how sure the person making the statement is that the event will happen. Problem: Each of the above definitions has pitfall: Classical: We may not be able to know the total number of possible outcomes, or it may be uncountable Relative frequency: We need “repetition”, which is often expensive and may not be possible. Subjective: We want the probability to be consistent across different people, and and be rigorously defined. "],["lecture-2-january-10-2024.html", "3 Lecture 2, January 10, 2024 3.1 Questions from the class", " 3 Lecture 2, January 10, 2024 In this lecture, we went over some basic definitions from the set theory, and using them as the building block for the rest of the course. We started Chapter 2 today, with many definitions. As for the set operations, \\(\\cup,\\cap,A^c,...\\), the Venn diagrams help to visual the meaning behind. Here is a good reference HERE. Definition 3.1 (sample space) A sample space \\(S\\) is a set of distinct outcomes of an experiment with the property that in a single trial of the experiment only one of these outcomes occurs. Definition 3.2 (Discrete and non-discrete sample space) A sample space \\(S\\) is said to be discrete if it is finite, or ``countably infinite” (i.e.,there is a one-to-one correspondence with the natural numbers). Otherwise a sample space is said to be non-discrete. Definition 3.3 (Event) An event is a subset of the sample space that can be assigned probability. Definition 3.4 (Simple and Compound event) Let \\(S\\) be discrete and \\(A\\subset S\\) an event. If \\(A\\) is indivisible so it contains only one point, we call it a simple event, otherwise compound event. Definition 3.5 (Probability distribution) Let \\(S=\\{a_1,a_2,\\dots\\}\\) be discrete. Assign numbers \\(P(\\{a_i\\})\\) (or short: \\(P(a_i)\\)), \\(i=1,2,\\dots\\), so that 1.\\(0\\leq P(a_i)\\leq 1,\\quad i=1,2,\\dots\\) 2. \\(\\sum_{\\text{all }i}P(a_i)=1\\). We then call the set of probabilities \\(\\{P(a_i):i=1,2,\\dots\\}\\) a probability distribution. Definition 3.6 Let \\(S=\\{a_1,a_2,\\dots\\}\\) discrete. From any prob. distribution \\(P\\) on \\(S\\) we can define a prob. measure on $ {S} = 2^S$ (set of all subsets of \\(S\\)) by \\[\\forall A \\subseteq S \\qquad P(A)=\\sum_{a_i\\in A}P(a_i).\\] Definition 3.7 (Equally likely) We say a sample space \\(S\\) with a finite number of outcomes is equally likely if the probability of every individual outcome in \\(S\\) is the same. Observe that If \\(|A|\\) denote the number of outcomes in an event \\(A\\). In case of an equally likely sample space, \\[ 1=P(S)=\\sum_{i=1}^{|S|}P(a_{i})= P(a_i)|S|. \\] \\[ P(a_i)=\\frac{1}{|S|}. \\] Hence, \\[P(A) = \\sum_{i:\\;a_i \\in A} P(a_i) = \\sum_{i:\\;a_i \\in A} \\frac{1}{|S|} =|A|\\cdot \\frac{1}{|S|}\\] Conclusion: In a finite, equally likely sample space, the probability of an event \\(A\\) can be computed as \\[ P(A) = \\sum_{i:\\;a_i \\in A} P(a_i) = \\frac{|A|}{|S|}. \\] 3.1 Questions from the class What is the difference between “countably infinite” v.s. “infinite”? Ans: A set is countably infinite if its elements can be put in one-to-one correspondence with the set of natural numbers \\(\\mathbb{N}\\). Alternatively, you can think that a set is countably infinite if you can count off all elements in the set in such a way that, even though the counting will take forever, you will get to any particular element in a finite amount of time. [A good reference page to read]. If a set is not countable or countably infinite, it is infinite. Why do we have something such as \\(2^\\mathcal{S}\\) in the lecture? Ans: It is related to something called the power set. The power set consists all the possible subset of a set \\(\\mathcal{S}\\). In a subset of \\(S\\), (i.e. \\(A \\subseteq \\mathcal{S}\\), every element in \\(\\mathcal{S}\\) may be either in \\(A\\) or not in \\(A\\). Which means, each element has two possibilities, in \\(A\\) or not in \\(A\\). Hence, the cardinality (i.e. the size) of the power set is \\(2^\\mathcal{S}\\). What did we mean by “order does not matter” and “order matters” during the lecture. Order does not matter: I said when you write out the element of a set, the order does not matter. For instance, in the rolling a six-sided dice, which side would be faced on the top example, we can write \\(\\mathcal{S} = \\{1,2,3,4,5,6\\}\\), or \\(\\mathcal{S}^\\prime=\\{6,5,4,3,2,1\\}\\), and those two sets are essentially equal to each other. To represent a set, the order does not matter, but we tend to write in a way that is intuitive and easy to understand. Order does matter: In rolling two dices example, the dots show on each of the dice is an ordered pair, denoted by \\((x,y)\\). Hence, for instance, \\((1,2)\\) and \\((2,1)\\) are different. It is problem-dependent so be careful. "],["lecture-3-january-12-2024.html", "4 Lecture 3, January 12, 2024 4.1 Questions from the class", " 4 Lecture 3, January 12, 2024 Definition 4.1 (Odds) Odds in favour of an event \\(A\\) occurring is \\[ O(A) := \\frac{P(A)}{1-P(A)}. \\] Odds again an event \\(A\\) is \\[ \\frac{1-P(A)}{P(A)}. \\] The range of the odds is \\([0,\\infty)\\). It provide a measure of the likelihood of a particular outcome to happen. Abbreviation: ““p:q”. Note: Probability may be defined through the odds as follow. \\[\\begin{align*} &amp;O(A) := \\frac{P(A)}{1-P(A)} \\\\ &amp;\\implies O(A) - P(A)O(A) = P(A) \\\\ &amp;\\implies O(A) = P(A) (1+O(A)) \\\\ &amp;\\implies P(A) = \\frac{O(A)} {1+O(A)} \\end{align*}\\] Note: In finite, equally likely sample spaces, computing probabilities amounts to counting the number of elements in a set. It will often be difficult to do this manually, so we are looking for clever counting techniques in the next chapter. Chapter 3 Counting Techniques Addition rule v.s. Multiplication rule For addition rule Keyword for addition rule is “OR”; \\(|A|\\) is defined to be the size of the set, aka the cardinality of the set. If \\(A\\) and \\(B\\) are disjoint (i.e. \\(A\\cap B = \\emptyset\\)), then \\(|A\\cup B| = |A| + |B|\\). \\(A\\cup A^c = S\\) where \\(A \\cap A^c = \\emptyset\\). Thus \\(|S|=|A|+|A^c|\\). For multiplication rule for multiplication rule is “AND” An ordered k-tuple is an ordered set of \\(k\\) values: \\((a_1,a_2,\\dots,a_k)\\). If the outcomes in A can be wrttien as an ordered k-tuple where there are \\(n_1\\) choices for \\(a_1\\), \\(n_2\\) choices for \\(a_2,\\dots\\) and in general \\(n_i\\) choices for \\(a_i\\), then \\[ |A| = n_1n_2\\cdots n_k = \\prod_{i=1}^k n_i. \\] Definition 4.2 (Factorial) Given \\(n\\) distinct objects, there are \\[ n! = n \\times (n-1) \\times \\ldots 2 \\times 1, \\] different ordered arrangements of length \\(n\\) that can be made. Note that, we define, \\(0! = 1\\). We pronounce \\(n!\\) as “n factorial”. The following recursive definition is useful: \\[ n! = n \\cdot (n-1)! \\] When working with factorials, we can often cancel terms, e.g., \\[ \\frac{9!}{7!} = \\frac{9\\cdot 8 \\cdot 7\\cdot 6 \\cdot \\dots \\cdot 2 \\cdot 1}{7\\cdot 6 \\cdot \\dots \\cdot 2 \\cdot 1}=9\\cdot 8 = 72\\] Definition 4.3 (Permutation) Given \\(n\\) distinct objects, a permutation of size \\(k\\) is an \\(ordered\\) subset of \\(k\\) of the individuals. The number of permutations of size \\(k\\) taken from \\(n\\) objects is denoted \\(n^{(k)}\\) and \\[ n^{(k)}=n(n-1)\\dots (n-k+1) =\\frac{n!}{(n-k)!}. \\] The tricky part of this definition is the word “ordered”. An ordering need not be numerical, for example assigning labels like “President” and “Vice-President” has the effect of ordering the individuals. 4.1 Questions from the class Can we express the odds as \\(a:b\\)? YES. For instance, the example we saw in class (or the clicker question 1), if we roll a fair six sided dice, and let our event \\(A:=\\{\\text{# is } 5 \\}\\). Then the odds \\(O(A)=\\frac{1/6}{1/5}=\\frac{1}{5}\\). We can see that, there is exactly one possibility we have event \\(A\\), whereas there are 5 possibilities that \\(A\\) does not happen (i.e. the number we roll out is \\(1,2,3,4,6\\)). We can abbreviate it as “1:5”. For a good example of Odds, WIKI provides a good one. "],["lecture-4-january-15-2024.html", "5 Lecture 4, January 15, 2024", " 5 Lecture 4, January 15, 2024 Definition 5.1 (Combination) Given \\(n\\) distinct objects, a combination of size \\(k\\) is an unordered subset of \\(k\\) of the individuals. The number of combinations of size \\(k\\) taken from \\(n\\) objects is denoted \\({n \\choose k}\\) or \\({}_n C_k\\) and can be computed as \\[ {n \\choose k}=\\frac{n^{(k)}}{k!}=\\frac{n!}{(n-k)!\\ k!}. \\] "],["lecture-5-january-17-2024.html", "6 Lecture 5, January 17, 2024 6.1 Example in class", " 6 Lecture 5, January 17, 2024 Properties of the Binomial coefficients There are some useful/important results about permutation and combination. \\(n^{(k)} = n (n - 1)^{(k-1)}\\) for \\(k \\geq 1\\) \\({n \\choose k} = \\frac{n^{(k)}}{k!}\\) \\({n \\choose k} = {n \\choose n-k}\\) for \\(k \\geq 0\\) \\({n \\choose k} = {n-1 \\choose k-1} + {n-1 \\choose k}\\) Binomial theorem: \\((1 + x)^n = \\sum_{k=0}^n {n \\choose k} x^k\\) \\({n \\choose k}\\) is equal to the \\(k\\)th entry in the \\(n\\)th row of Pascal’s triangle. Note: Many of these idenetity may be proven using something called combinatorial proof. See Wiki for an (easy) example. Proof (4). \\[\\begin{align*} {n-1 \\choose k-1} + {n-1 \\choose k} &amp;= \\frac{(n-1)!}{(k-1)! (n-k)!} + \\frac{(n-1)!}{k! (n-k-1)!}\\\\ &amp;= \\frac{(n-1)!k }{(k-1)! (n-k)!k} + \\frac{(n-1)!(n-k)}{k! (n-k-1)!(n-1)} \\\\ &amp;= \\frac{(n-1)!k + (n-1)! (n-k)}{k! (n-k)!} \\\\ &amp;- \\frac{(n-1)!( k + (n-k))}{k! (n-k)!} \\\\ &amp;= \\frac{(n-1)! n}{k! (n-k)!} \\\\ &amp;= {n \\choose k} \\end{align*}\\] Aside: Stirling’s formula \\(n!\\) grows really fast as \\(n\\) increases, so sometimes we need to approximate its value for computational reasons. Stirling’s formula provides one such method, and it is given by \\[ n! \\sim \\sqrt{2 \\pi n} \\left( \\frac{n}{e} \\right)^n, \\] where \\(\\sim\\) means their ratio approaches 1 as \\(n\\) goes to infinity. We won’t need this approximation, but it’s useful to know it exists. Example of use: Show that \\(2^{-2n}\\binom{2n}{n} \\approx \\sqrt{\\frac{2}{\\pi n}}\\) 6.1 Example in class Example 6.1 (Application of Stirling's Formula/Approximation for factorial) Show that \\(2^{-2n}{2n \\choose n} \\approx \\sqrt{\\frac{1}{\\pi n}}\\) \\[\\begin{align*} 2^{-2n}{2n \\choose n} &amp;= 2^{-2n}\\frac{2n!}{n!n!} \\\\ &amp;\\approx 2^{-2n} \\frac{\\sqrt{2\\pi (2n)} (2n/e)^{2n}}{\\sqrt{2\\pi n} (n/e)^{n}\\sqrt{2\\pi n} (n/e)^{n}}\\\\ &amp;= 2^{-2n} \\frac{\\sqrt{4}}{\\sqrt{2}\\sqrt{2}} \\frac{\\sqrt{\\pi n}}{\\sqrt{\\pi n}\\sqrt{\\pi n}} \\frac{(2n)^{2n}}{n^n n^n} \\frac{e^{-2n}}{e^{-2n}}\\\\ &amp;= 2^{-2n} \\frac{1}{\\sqrt{\\pi n}} 2^{2n}\\\\ &amp;= \\frac{1}{\\sqrt{\\pi n}} = \\sqrt{\\frac{1}{\\pi n}} \\end{align*}\\] "],["lecture-6-january-19-2024.html", "7 Lecture 6, January 19, 2024", " 7 Lecture 6, January 19, 2024 7.0.1 Multinomial Coefficient Definition 7.1 (Multinomial Coefficient) Consider \\(n\\) objects which consist of \\(k\\) types. Suppose that there are \\(n_1\\) objects which are of type 1, \\(n_2\\) which are of type 2, and in general \\(n_i\\) objects of type \\(i\\). Then there are \\[ \\frac{n!}{n_1 ! n_2! \\dots n_k !} \\] distinguishable arrangements of the \\(n\\) objects. This quantity is known as a multinomial coefficient and denoted by \\[ \\binom{n}{n_1,n_2,\\dots,n_k}= \\frac{n !}{n_1 ! n_2! \\dots n_k !}. \\] Note: Multinomial coefficient is an extension of the binomial coefficient. In binomial coefficient, there are only two groups/objects, and the first type has size \\(n_1\\) and the size of the second type is consequently \\(n-n_1\\), where \\(n\\) is the total number of objects. Hence we have \\({n \\choose n_1} = \\frac{n!}{n_1! (n-n_1)!}\\). Try to compare this with the multinomial coefficient. 7.0.2 The Birthday Problem Suppose a room contains \\(n\\) people. What is the probability at least two people in the room share a birthday? Assumption: Suppose that each of the \\(n\\) people is equally likely to have any of the 365 days of the year as their birthday, so that all possible combinations of birthdays are equally likely. Let \\(A\\) be the event that at least two people share a birthday. Then \\[ P(A) = 1 - P(A^c),\\] where \\(A^c\\) is the event that nobody shares birthday with each other. For \\(n\\) people to have unique birthdays, we need to arrange them among 365 days w/o replacement. Thus, \\[|A^c| = 365^{(n)}.\\] For the size of the sample space, we see that each person has 365 possibilities for their birthday. Thus, \\[|S| = 365^n.\\] Since we are assuming that all possible combinations of birthdays are equally likely, our desired probability becomes \\[ P(A) = 1 - P(A^c) = 1 - \\frac{365^{(n)}}{365^n} = 1 - \\frac{n! {365 \\choose n}}{365^n}. \\] For \\(n\\in\\{100, 30, 23\\}\\) we find \\[P(A_{100})= .9999997,\\;\\;\\; P(A_{30})=.7063 \\;\\;\\;\\; P(A_{23})=.5073.\\] 7.0.3 Chapter 4 Probbility Rules and Conditional Probability Review the Venn Diagram "],["lecture-7-january-22-2024.html", "8 Lecture 7, January 22, 2024", " 8 Lecture 7, January 22, 2024 8.0.1 Some terminology about the set thoery. 8.0.1.1 Fundamental law of set algebra Let \\(A\\) and \\(B\\) be any arbitrary sets/events. Commutative \\[ A\\cup B = B\\cup A \\quad \\text{ and } A\\cap B = B\\cap A. \\] Associativity \\[ (A\\cup B)\\cup C = A \\cup (B\\cup C), \\quad \\text{and } (A\\cap B)\\cap C = A \\cap (B \\cap C). \\] Distributive Law \\[ A\\cup (B\\cap C) = (A \\cup B) \\cap (A \\cap C) , \\quad \\text{ and } A \\cap (B\\cup C) = (A\\cap B) \\cup (A\\cap C) \\] 8.0.1.2 DeMorgan’s Laws \\((A\\cup B)^c = A^c \\cap B^c\\) (Complement of an union is the intersection of the complements) \\(A\\cap B)^c = A^c \\cup B^c\\) (Complement of an intersection is the union of the complements) 8.0.1.3 Inclusion Exclusion Principle \\(P(A\\cup B ) = P(A) + P(B) - P(A\\cap B)\\) \\(P(A\\cup B \\cup C) = P(A) + P(B) + P(C) - P(A\\cap B) - P(A \\cap C) - P(B \\cap C) + P(A\\cap B \\ cap C)\\) Note that we can generalized the (2), and obtain the following by inducation. For arbitrary events \\(A_1,A_2,\\cdots,A_n,\\quad n\\ge 2\\), \\[\\begin{align*} P(\\bigcup_{i=1}^n A_i) &amp; =\\sum_{i}P(A_{i}% )-\\sum_{i&lt;j}P(A_{i}A_{j})+\\sum_{i&lt;j&lt;k}P(A_{i}A_{j}A_{k})\\\\ &amp; -\\sum_{i&lt;j&lt;k&lt;l}P(A_{i}A_{j}A_{k}A_{l})+\\cdots \\end{align*}\\] 8.0.2 Independence Definition 8.1 (independence) Any two events \\(A\\) and \\(B\\) are said to be independent if \\[ P(A \\cap B) = P(A)\\times P(B). \\] Note: Intuitively, it means that two events do not have any influence of each other. You will see that how this concept plays an important role in statistics, in particular through something called the covariance, which is beyond this course so do not worry about this for now. 8.0.3 Independence v.s. Multually Exclusive/Disjoint Recall the definition of mutually exclusive Definition 8.2 (Mutually Exclusive) Any two events \\(A\\) and \\(B\\) are said to be mutually exclusive or disjoint if \\[ P(A \\cap B) = 0. \\] Note: If \\(A\\) and \\(B\\) are mutually exclusive, \\(A\\) and \\(B\\) may NOT be independent! \\(A\\) and \\(B\\) CAN only be mutually exclusive and independent when either \\(A\\), \\(A\\), or both are the empty set \\(\\emptyset\\). Lemma 8.1 Let two events \\(A\\) and \\(B\\) such that NOT both events are trivial events (empty set). If \\(A\\) and \\(B\\) are independent and mutually exclusive/disjoint, then either \\(P(A) = 0\\) or \\(P(B) = 0\\). "],["lecture-8-january-24-2024.html", "9 Lecture 8, January 24, 2024", " 9 Lecture 8, January 24, 2024 Definition 9.1 (Conditional Probability) The conditional probability of an event \\(A\\) given an event \\(B\\), assuming \\(P(B)&gt;0\\), is \\[ P(A \\mid B) = \\frac{P(A\\cap B)}{P(B)}. \\] Definition 9.2 (Equivalent definition of independence) Two events \\(A\\) and \\(B\\) are independent, if \\[ P(A|B)=P(A), \\] provided \\(P(B)&gt;0\\). 9.0.1 Properties of Conditional Probability \\(0 \\le P(A \\mid B) \\le 1\\). This follows from the fact that if \\(A \\subset B\\) then \\(P(A) \\le P(B)\\) \\(P(A^c \\mid B) = 1-P(A \\mid B)\\). If \\(A_1\\) and \\(A_2\\) are disjoint (i.e. \\(P(A_1\\cap A_2)=\\emptyset\\): \\(P(A_1 \\cup A_2 \\mid B) = P(A_1 \\mid B) + P(A_2 \\mid B)\\). \\(P(S \\mid B)= 1 = P(B \\mid B)\\). Definition 9.3 (Product rule) For any events \\(A\\) and \\(B\\), we have \\[ P(A\\cap B) = P(A\\mid B) P(B) = P(B \\mid A) P(A). \\] "],["lecture-9-january-26-2024.html", "10 Lecture 9, January 26, 2024", " 10 Lecture 9, January 26, 2024 10.0.1 Law of Total Probability Definition 10.1 (Partition) A sequence of sets \\(B_1,B_2,...,B_k\\) are said to partition the sample space \\(S\\) if \\(B_i \\cap B_j = \\emptyset\\) for all \\(i \\ne j\\), and \\(\\cup_{j=1}^k B_j = S\\). Theorem 10.1 (Law of Total Probability) Suppose that \\(B_1,B_2,...,B_k\\) partition \\(S\\). Then for any event \\(A\\), \\[ P(A) = P(A | B_1) P(B_1) + P(A | B_2) P(B_2) + \\cdots +P(A | B_k) P(B_k). \\] Note: It is a simple usage of LTP such that \\[ P(A) = P(A\\cap B) + P(A \\cap B^c), \\] since \\(B\\) and \\(B^c\\) partition \\(S\\) (i.e. \\(B\\cup B^c = S\\) and \\(B\\cap B^c = \\emptyset\\)) 10.0.2 Bayes Rule If we can calculate the conditional profitability, then we may calculate the desire probability by using 1) LTP, 2) definition of conditional probability, and 3) property of the sets (through the Venn diagrams). However, sometimes we have to FLIP the event and the conditioning event. This brings us to the Bayes Theorem. Theorem 10.2 (Bayes Theorem) Suppose that \\(B_1,B_2,...,B_k\\) partition \\(S\\). Then for any event \\(A\\), \\[ P(B_i \\mid A ) = \\frac{P(A \\mid B_i)P(B_i)}{\\sum_{j=1}^k P(A \\mid B_j) P(B_j) }. \\] This concludes Chapter 4!. "],["lecture-10-january-29-2024.html", "11 Lecture 10, January 29, 2024", " 11 Lecture 10, January 29, 2024 We begin Chapter 5 in this lecture! 11.0.1 Chapter 5. Discrete Random Variable Definition 11.1 (Random Variable) A random variable is a function that maps assigns a real number \\(\\mathbb{R}\\) to each point in a sample space \\(S\\). That is, \\(X\\) is a random variable if \\[X:S\\to \\mathbb{R}\\]. Definition 11.2 (Range) The values that a random variables takes is called the range of the random variable. We often denote the range of a random variable \\(X\\) by \\(X(S)\\). Definition 11.3 (Discrete random variable) The discrete random variables take integer values, or more generally, values in a countable set (i.e. finite or countably infinite set). That is, its range is a discrete/countable subset of \\(\\mathbb{R}\\). Definition 11.4 (Continuous random variable) A random variable is continuous if its range is an interval that is a subset of \\({\\mathbb R}\\) (e.g. $[0,1], (0,), {R} $). Definition 11.5 (Probability (mass) function) The probability (mass) function of a discrete random variable \\(X\\) is the function \\[ f_X(x) = P(X=x),\\quad \\text{ for } x\\in \\mathbb{R}, \\] which is non-zero at at most countably many values. Notation: We write \\(P(X=x)\\) as the shorthanded notation for \\(P(\\{\\omega \\in S : X(\\omega)=x\\})\\). Notation: We can write \\(f_X(x)=P(X^{-1}(x))=(P\\circ X^{-1})(x)\\). We call this as push-forward probability measure. Note: The definition \\(f_X\\) is valid for all \\(x\\), but the value is zero when \\(x\\) is outside the range of the random variable \\(X\\). (This is called the null set). Properties of probability mass function \\(f\\): \\(f_X(x)\\in[0,1]\\) for all \\(x\\), and \\(\\sum_{x\\in X(\\omega)}f_X(x) =1\\). i.e. sum of the probability on ALL the events equal to \\(1\\). "],["lecture-11-january-31-2024.html", "12 Lecture 11, January 31, 2024 12.1 Distinction of the definition “discrete” of the sample space and the random variable 12.2 Cumulative distribution function 12.3 Special distributions with names", " 12 Lecture 11, January 31, 2024 12.1 Distinction of the definition “discrete” of the sample space and the random variable Recall the definition of the Range of a random variable from last lecture: ::: {.definition name=“Range”} The values that a random variables takes is called the range of the random variable. We often denote the range of a random variable \\(X\\) by \\(X(S)\\). ::: We say that, a random variable is discrete if its range \\(X(\\omega)\\) is discrete (finite or countable, in another word, we can say it is at most countable). We say, a sample space \\(S\\) is discrete if \\(S\\) is finite or countable. The sample space \\(S\\) and the range of the random variable are two different things, so do not get confused! We can have a discrete random variable while the sample space \\(S\\) is continuous! 12.2 Cumulative distribution function Definition 12.1 (Range) The cumulative distribution function (cdf) of a random variable \\(X\\) is \\[ F_X(x) = P(X \\le x),\\;\\; x \\in {\\mathbb{R}}. \\] Note: The cumulative distribution function \\(F_X\\) is always defined over the entire real line \\(\\mathbb{R}\\), while the probability function may not always be defined! Hence, the cumulative distribution function is an useful tool! (but do not worry about it now.) Notation: \\(F_X(x) = P(X\\le x) = P(\\{\\omega \\in S : X(\\omega)\\in x\\}).\\) If \\(X\\) is discrete with probability function \\(f_X\\) (i.e. if \\(f_X\\) exists), then we can calculate the cdf from summing up the pdf as \\[ F_X(x)=P( X \\le x ) = \\sum_{y:\\; y\\le x}f_X(y). \\] 12.2.1 Properties of the cumulative distribution function Let \\(F_X(\\cdot)\\) be a cdf. Then the following holds \\(F_X(x)\\in [0,1]\\) \\(F_X(x) \\le F_x(y)\\) whenever \\(x&lt;y\\) (i.e. \\(F_X(\\cdot)\\) is a non-increasing function.) \\(\\lim\\limits_{x \\to - \\infty } F_X(x)=0\\), and \\(\\lim\\limits_{x \\to \\infty } F_X(x) = 1\\). \\(F_X\\) is right continuous, i.e., \\(F(x_0)=\\lim_{x\\downarrow x_0} F(x)\\) for all \\(x_0\\in\\mathbb{R}\\). 12.3 Special distributions with names 12.3.1 Discrete uniform distribution The first named distribution we look at is the discrete uniform distribution Definition 12.2 (Discrete uniform distribution) uppose the range of the random variable \\(X\\) is \\(\\{a,a+1,\\dots, b\\}\\), where \\(a,b\\in\\mathbb{Z}\\), and suppose all values are equally likely. Then we say that \\(X\\) has a discrete uniform distribution on \\(\\{a,a+1,\\dots,b\\}\\), shorthand: \\(X \\sim U[a,b]\\). 12.3.1.1 Probability function and distribution function If \\(X \\sim U[a,b]\\), then its probability function is given by \\[ f_X(x)= P(X = x) = \\begin{cases} \\frac{1}{b - a + 1}, &amp;\\quad\\text{ if }x \\in\\{a,a+1,\\dots,b\\}, \\\\ 0, &amp;\\quad\\text{ otherwise} \\end{cases} \\] and corresponding (cumulative) distribution function is \\[ F_X(x)= P(X \\leq x) = \\begin{cases} 0, &amp;\\quad\\text{ if } x&lt;a\\\\ \\frac{\\lfloor x\\rfloor - a + 1}{b - a + 1}, &amp;\\quad\\text{ if }x \\in\\{a,a+1,\\dots,b\\}, \\\\ 1, &amp;\\quad\\text{ if } x\\geq b,\\end{cases} \\] where \\(\\lfloor x\\rfloor=\\max\\{z\\in\\mathbb{Z}: z\\leq x\\}\\) is the floor function. "],["lecture-12-feburary-02-2024.html", "13 Lecture 12, Feburary 02, 2024 13.1 Hypergeometric distribution 13.2 Bernoulli and Binimial distributions 13.3 Relationship and difference between binomial and hypergeometric", " 13 Lecture 12, Feburary 02, 2024 13.1 Hypergeometric distribution Definition 13.1 (Hypergeometric distribution) Consider a population that consists of \\(N\\) objects, of which \\(r\\) are considered successes and the remaining \\(N-r\\) are considered failures. Suppose that a subset of size \\(n\\) (with \\(n\\leq N\\)) is drawn from the population WITHOUT REPLACEMENT. Let \\(X\\)=Number of successes obtained, then we say \\(X\\) follows a hypergeometric distribution with parameters \\((N,r,n)\\). We sometimes write \\(X \\sim hyp(N,r,n)\\) or \\(X\\sim HG(N,r,n)\\). 13.1.1 Range of the Hypergeomnetric distribution We cannot have more successes than there are (\\(r\\)) \\(\\Rightarrow\\) \\(x\\leq r\\) We cannot have more successes than trials (\\(n\\)) \\(\\Rightarrow\\) \\(x\\leq n\\) When there are more trials than failures (\\(n&gt;(N-r)\\)) we will for sure have at least \\(n-(N-r)\\) successes \\(\\Rightarrow\\) \\(x\\geq \\max\\{0, n-(N-r)\\}\\). Overall, we have \\(\\max\\{0, n-(N-r)\\} \\leq x \\leq \\min\\{r,n\\}\\). 13.1.2 Probability function of hypergeometric distribution Total number of of subsets of size \\(n\\): \\(\\binom{N}{n}\\). Number of ways to select \\(x\\) successes out of \\(r\\) successes: \\(\\binom{r}{x}\\). Number of ways to choose remaining \\(n-x\\) failures from \\(N-r\\) failures: \\(\\binom{N-r}{n-x}\\). Thus, \\[f(x) = \\frac{\\binom{r}{x}\\binom{N-r}{n-x}}{\\binom{N}{n}},\\] where \\(\\max\\{0, n-(N-r)\\} \\leq x \\leq \\min\\{r,n\\}\\) and 0 otherwise. 13.2 Bernoulli and Binimial distributions Definition 13.2 (Bernoulli trail) A Bernoulli trial with probability of success \\(p\\) is an experiment that results in either a success or failure, and the probability of success is \\(p\\). Definition 13.3 (Bernoulli distribution) If a random variable \\(X\\) represents the number of successes in a Bernoulli trial with probability of success \\(p\\), it follows the Bernoulli distribution, and we denote it as \\[ X \\sim Bernoulli(p). \\] 13.2.1 Probability function and cumulative distribution function 13.2.2 Bernoulli If \\(X\\sim Bern(p)\\), the pf of \\(X\\) is \\[ f_X(0)=1-p,\\quad f_X(1)=p,\\quad f_X(x)=0\\,\\,\\text{ for }x\\not\\in\\{0,1\\}\\] and the cdf is \\[ F_X(x)=\\begin{cases} 0, &amp;\\text{ if }x&lt;0, \\\\ 1-p, &amp;\\text{ if } 0\\leq x &lt; 1,\\\\ 1, &amp;\\text{ if }x\\geq 1\\end{cases}\\] 13.2.3 Binomial Definition 13.4 (Bernoulli distribution) If we have \\(N\\) independent runs and record the numbers of successes obtained in these \\(n\\) runs, then \\(X\\) is said to have a binomial distribution, denoted by \\(X\\sim Bin(n,p)\\). Note: The probability function of \\(X\\sim Bin(n, p)\\) is \\[ f(x) = \\binom{n}{x} p^x(1-p)^{n-x},\\quad x=0,1,2,\\dots,n\\] and 0 otherwise. 13.3 Relationship and difference between binomial and hypergeometric Binomial and hypergeometric distributions are fundamentally different! In Binomial models, we pick WITH replacement, in the hypergeometric model WITHOUT replacement. If \\(N\\) is large and \\(n\\) is small, the chance we pick the same object twice is small. Thus, letting \\(r/N=p\\), \\(X\\sim Hyp(N,r,n)\\) and \\(Y\\sim Bin(n,p)\\), then we can APPROXIMATE \\[ P(X \\leq k) \\approx P(Y\\leq k).\\] (more precisely, in the limit as \\(N\\rightarrow\\infty\\) with \\(r/N\\rightarrow p\\) (the ratio of successes converges to the success probability). See pages 86/87 and later in the course for more. We’ll see an example next time. "],["lecture-13-feburary-05-2024.html", "14 Lecture 13, Feburary 05, 2024 14.1 Binomial v.s. hypergeometric 14.2 Negative binomial distribution 14.3 Binomial v.s. Negative Binomial 14.4 Geometric distribution", " 14 Lecture 13, Feburary 05, 2024 14.1 Binomial v.s. hypergeometric \\(Bin(n,\\frac{r}{N})\\) and \\(hyp(N,r,n)\\) are fundamentally different! If you have an urn with \\(r\\) successes and \\(N-r\\) failures… … and you draw \\(n\\) items, then the number of successes is… … Binomial: drawn replacement, … … Hypergeometric: drawn replacement. \\(N\\gg n \\Rightarrow\\) chance we pick same object twice w/ replacement small. For \\(p=\\frac{r}{N}\\), \\(X\\sim Hyp(N,r,n)\\) and \\(Y\\sim Bin(n,p)\\) \\[ P(X \\leq k) \\approx P(Y\\leq k).\\] (more precisely, in the limit as \\(N\\rightarrow\\infty\\) with \\(r/N\\rightarrow p\\)). 14.2 Negative binomial distribution Definition 14.1 (Negative Binomial) Consider an experiment with two possible outcomes success (Su) or failure (F). Without loss of generality, assume that the \\(P(Su)=p\\) (so \\(P(F)=1-P(Su)= 1-p\\)). Repeat the experiment independently until a specified number of \\(k\\) successes have been observed. Denote \\(X\\) the number of failures before the \\(k\\)-th suceess, then \\(X\\sim NegBin(k,p)\\). 14.2.1 Probability function The probability function of \\(X\\sim NegBin(k,p)\\) is \\[ f(x)=P(X=x)=\\binom{x+k-1}{x}p^k(1-p)^x,\\quad x=0,1,2,\\dots\\] since there are \\(\\binom{x+k-1}{x}\\) to choose \\(x\\) positions among the first \\(x+k-1\\) positions to be a failure (and the remaining ones are automatically success), and each of these sequence of outcomes has probability \\(p^k(1-p)^x\\). 14.3 Binomial v.s. Negative Binomial Binomial distribution: We know number of trials \\(n\\), but we do not know how many successes. Negative Binomial distribution: We know the number of successes \\(k\\), but we do not know how many trials will be needed. 14.4 Geometric distribution Definition 14.2 (Geometric) Consider an experiment with two possible outcomes success (Su) or failure (F). Without loss of generality, assume that the \\(P(Su)=p\\) (so \\(P(F)=1-P(Su)= 1-p\\)). Repeat the experiment independently before the 1st success has been observed. Denote \\(X\\) the number of failures before 1st success, then \\(X\\sim Geo(p)\\). Note: The Geometric distribution is a special case of the negative binomial distribution: \\(Geo(P) \\sim Neg(k=1,p)\\). 14.4.1 Probability function and distribution function If \\(X\\sim Geo(p)\\), then \\(X\\) has probability function \\[ f(x) = P(X=x)= (1-p)^x p,\\quad x\\in\\{0,1,2,\\dots\\}.\\] \\[ F(x)=P(X\\leq x) = P(X\\leq \\floor{x}) = \\sum_{k=0}^{\\floor{x}} (1-p)^k p = 1-(1-p)^{\\floor{x}+1}\\] if \\(x\\geq 0\\) and \\(0\\) otherwise. Note that \\(P(X&gt;x)=1-F(x)=(1-p)^{\\floor{x}+1}\\) which is nice for computations! 14.4.2 Memoryless property Let \\(X \\sim Geo(p)\\) and \\(s,t\\) be non-negative integers. Then, the following equation holds. \\[ P(X \\geq s+t | X \\geq s) = P(X \\geq t). \\] 14.4.3 Aside, Reason to call the Negative binomial distribution (This is contributed by Jeffery!) Can extend binomial coefficients to negative or fractional ``top’’ part. \\[\\binom{\\theta}{x} := \\frac{\\theta^{(X)}}{x!} = \\frac{\\theta(\\theta-1)(\\theta-1)...(\\theta-x+1)}{X!}\\] Note: ``bottom’’ part of coefficient still a non-negative integer. Then \\[\\begin{align*} \\binom{-k}{x} &amp; = \\frac{-k(-k-1)...(-k-x+1)}{x!} \\\\ &amp; = (-1)^x \\frac{(x+k-1)(x+k-2)...((x+k-1)-x+1)}{x!}\\\\ &amp; = (-1)^x \\binom{x+k-1}{x} \\end{align*}\\] So \\[ f_X(x) = \\binom{x+k-1}{x}p^k(1-p)^x = (-1)^x \\binom{-k}{x} p^k(1-p)^x \\] "],["lecture-14-feburary-07-2024.html", "15 Lecture 14, Feburary 07, 2024 15.1 Poisson distribution 15.2 Poisson as the limiting distribution of the binomial distribution 15.3 Poisson process 15.4 Side notes – Rigorous definition of convergence in distribution", " 15 Lecture 14, Feburary 07, 2024 15.1 Poisson distribution Definition 15.1 (Poisson distribution) We say the random variable \\(X\\) has a {} distribution with parameter \\(\\mu &gt; 0\\) if \\[ f(x) = e^{-\\mu} \\frac{ \\mu^x}{x!},\\;\\;x=0,1,2,\\dots\\] 15.1.1 Notation We write \\(X\\sim Poisson(\\mu)\\) or \\(Poi(\\mu)\\), where \\(\\mu\\) is called the rate parameter. 15.1.2 Interpreation of the Poisson distribution Limiting case of binomial distribution, where you fix \\(\\lambda = np\\) , and let \\(n \\rightarrow \\infty\\) and \\(p \\rightarrow 0\\) (This can be a consequence of b)) Poisson Process 15.2 Poisson as the limiting distribution of the binomial distribution One way to view the Poisson distribution is to consider the limiting case of binomial distribution, where you fix \\(\\mu = np\\) , and let \\(n \\rightarrow \\infty\\) and \\(p \\rightarrow 0\\). One can show that if \\(n\\to \\infty\\) and \\(p=p_n \\to 0\\) as \\(n\\to \\infty\\) in such a way that \\(n p_n \\to \\mu\\), then \\[ {n \\choose x} p^x (1-p)^{n-x} \\to e^{-\\mu} \\frac{ \\mu^x}{x!},\\;\\;\\;as\\;\\;\\; n\\to \\infty. \\] Actually here, it is something called the *convergence in distribution**. 15.3 Poisson process Consider counting the number of occurrences of an event that happens at random points in time (or space). Poisson process is the counting process that satisfies the following Independence: the number of occurrences in non-overlapping intervals are independent. Individuality: for sufficiently short time periods of length \\(\\Delta t,\\) the probability of 2 or more events occurring in the interval is close to zero \\[ \\frac{P\\left( \\text{2 or more events in }(t,t+\\Delta_t)\\right)}{\\Delta_t} \\rightarrow 0,\\;\\; \\Delta_t \\to 0 \\] Homogeneity or Uniformity: events occur at a uniform or homogeneous rate \\(\\lambda\\) and proportional to time interval \\(\\Delta_t\\), i.e. \\[ \\frac{P\\left( \\text{one event in }(t,t+\\Delta_t)\\right) - \\lambda\\Delta_t }{\\Delta_t} \\to 0. \\] If \\(X=\\) occurrences in a time period of length \\(t\\), then \\[X\\sim Poi(\\lambda t).\\] Definition 15.2 (Poisson process) A process that satisfies the prior conditions on the occurrence of events is often called a Poisson process. More precisely, if \\(X_t, \\; \\text{for } t\\ge0,\\) (a random variable for each \\(t\\)) denotes the number of events that have occurred up to time \\(t\\), then \\(X_t\\) is called a Poisson process. 15.4 Side notes – Rigorous definition of convergence in distribution This section is just served as a reference for those of you who are interested in the rigorous definition of convergence in distribution. Do not worry too much if you are not interested in knowing those. Definition 15.3 (Convergence in distribution) Let \\((F_n)_{n\\in\\mathbb{N}}\\) and \\(G\\) be CDFs. Let \\(c(G) = \\{x\\in\\mathbb{R} : G \\text{ is cts. at }x\\}\\) be the set of continuity points of \\(G\\). \\(F_n\\) converges in distribution to \\(G\\) if \\[\\begin{align} \\forall x\\in c(G) \\quad F_n(x)\\to G(x) \\tag{15.1} \\end{align}\\] If \\(X_n\\) has CDF \\(F_n\\) for each \\(n\\) and \\(Y\\) has CDF \\(G\\) and (15.1) holds then \\(X_n\\) converges in distribution to \\(Y\\). Denoted \\(X_n \\stackrel{d}{\\to} Y\\), or \\(F_n \\stackrel{d}{\\to} G\\) "],["lecture-15-feburary-09-2024.html", "16 Lecture 15, Feburary 09, 2024 16.1 Review of the distributions we covered before", " 16 Lecture 15, Feburary 09, 2024 16.1 Review of the distributions we covered before Distribution \\(f(x)=P(X=x)\\) Interpretation \\(U[a,b]\\) \\(\\frac{1}{b-a+1},\\, x=a,a+1,\\dots,b\\) Sample from \\(\\{a,a+1,\\dots,b\\}\\) once uniformly at random\\ \\(Bin (n,p)\\) \\(\\binom{n}{x}p^x(1-p)^{n-x},\\,x=0,1,\\dots,n\\) \\(\\#\\) of successes in \\(n\\) indep. trials with success prob. \\(p\\). \\(Hyp(N,r,n)\\) \\(\\frac{\\binom{r}{x}\\binom{N-r}{n-x}}{\\binom{N}{n}},\\) \\(\\max\\{0, n-(N-r)\\} \\leq x \\leq \\min\\{r,n\\}\\) \\(\\#\\) of successes in \\(n\\) draws without replacement from \\(N\\) objects with \\(r\\) successes. \\(NegBin(k,p)\\) \\(\\binom{x+k-1}{x}p^k(1-p)^x,\\, x=0,1,\\dots,\\) \\(\\#\\) of failures until \\(k\\) successes in indep. trials with success prob. \\(p\\) \\(Geo(p)\\) \\[p(1-p)^x,~ x=0,1,\\dots \\] \\(\\#\\) of failures until first success in indep. trials with success prob. \\(p\\) \\(Poi(\\mu)\\) \\[\\exp(-\\mu) \\mu^x/x!,~ x=0,1,\\dots \\] \\(\\#\\) of occurrences in Poi process. "],["lecture-16-feburary-12-2024.html", "17 Lecture 16, Feburary 12, 2024 17.1 Chapter 7 Expectation and Variance 17.2 Theoretical mean and the sample mean", " 17 Lecture 16, Feburary 12, 2024 17.1 Chapter 7 Expectation and Variance Probability and statistics are closely related to data; we often try to ``extract” additional information from data. Some simple way to analyse and visualize data are: frequency table frequency histogram However, sometimes it is unclear how to define the groups in the frequency or in the histogram. Hence, we often would like to have more concise defined ways to analyse the random variable and its associated distribution. We call those numerical values as the (summary) statistics. Definition 17.1 (Sample mean) Let \\(x_1, \\ x_2, \\ldots, \\ x_n\\) be \\(n\\) realizations of a random variable \\(X\\) (such a set is called a sample). The sample mean is defined as \\[ \\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i \\] Note: there are other summary Sample median: a value such that half of the results are below it and the other half above it, when the sample is arranged in numerical order. Median is more robust against some abnormally big/small observations, or recording errors. Sample mode: The most frequently-occuring value in a sample. We may have more than one mode. 17.2 Theoretical mean and the sample mean We can compute statistics for a random variable \\(X\\) directly if we know its distribution. Such a mean would be theoretical, as we are working from its probability distribution rather than an actual sample. That means, we can do experiment to get sample, then compute the sample mean, while if we know the distribution, we can compute the theoretical mean without any samples. 17.2.1 Definition Definition 17.2 ((Theoretical) Mean/Expectation/First moment) Suppose \\(X\\) is a discrete random variable with probability function \\(f_X(x)\\). The expected value of \\(X\\), denoted by \\(E[X]\\), is then the number \\[ E[X] = \\sum_{x\\in X(S) } x \\ f_X(x) = \\sum_{x\\in X(S) } x \\ P(X=x), \\] provided the sum converges absolutely (i.e., if \\(\\sum_{x\\in X(S) } |x| \\ f_X(x)&lt;\\infty\\)) 17.2.2 Interpretation Geometrical interpretation: \\(E[X]\\) is the balancing point of the probability function \\(f(x)\\). \\(E[X]\\) is what the average of many, many independent realizations of the random variable \\(X\\) would approach (Law of large numbers). "],["lecture-17-feburary-14-2024.html", "18 Lecture 17, Feburary 14, 2024 18.1 More about expectation 18.2 Law of Unconscious Statistician 18.3 Tricks 18.4 Property of expectation - linearity 18.5 Mean of binomial distribution", " 18 Lecture 17, Feburary 14, 2024 18.1 More about expectation Some observations we made from the example we went through in class. Suppose \\(X\\) is a random variable satisfying \\(a \\le X(\\omega) \\le b\\) for all \\(\\omega \\in S\\). Then \\(a&lt;E[X]&lt;b\\). We may think that the expectation is a weighting of the values \\(x\\in X(S)\\) by its probability function (which is always positive, and sum up to one). 18.2 Law of Unconscious Statistician If \\[ g: {\\mathbb R} \\to {\\mathbb R}, \\] and \\(X\\) is a random variable with probability function \\(f\\), then \\(g(X)\\) is a random variable taking values \\(g(X(S))\\) and \\[ E[g(X)] = \\sum_{x \\in X(S)} g(x) f(x) \\] NOTE: In general, \\(E[g(X)]\\ne g(E[X])\\)! e.g. For an arbitrary convex function \\(g(X)\\), \\(g\\{E(X)\\} \\le E\\{g(x)\\}\\). This is a famous theorem called Jansen’s inequality. 18.3 Tricks Sometimes, in order to calculate the expectation, some terminologies may help For \\(x\\in X(S)\\), \\(\\sum_{x\\in X(S)} f_X(x)=1\\). 18.4 Property of expectation - linearity Suppose the random variable \\(X\\) has \\(E[X]=\\mu\\). Then for any constants \\(a,b\\in\\mathbb{R}\\), \\[ E[aX+b]= a \\mu + b = a E[X]+b\\] If we have 2 random variables \\(X\\) and \\(Y\\), and three constants, \\(a,b,c\\in \\mathbb{R}\\), then \\[ E[aX+bY + c] = aE[X] + bE[Y] +c. \\] 18.4.1 Proof. Proof. By the Law of unconscious statistician with \\(g(x)=ax+b\\), we find \\[\\begin{align*} E[aX+b] &amp;=\\sum_{x\\in X(S)} (ax+b) f(x)\\\\ &amp; = a \\cdot \\sum_{x\\in X(S)} x f(x) + b \\cdot \\sum_{x\\in X(S)} f(x) \\\\ &amp;= a\\cdot E[X] + b \\cdot 1 \\end{align*}\\] 18.5 Mean of binomial distribution If \\(X \\sim Binomial(n,p)\\), then \\(E[X] = np\\) Proof. Suppose \\(X \\sim Bin(n,p)\\). Then we have \\[\\begin{align*} E[X] &amp;= \\sum_{x=0}^n x\\cdot \\binom{n}{x} p^x (1-p)^{n-x}\\\\ &amp;= \\sum_{x=1}^n x\\cdot \\frac{n!}{(n-x)!x!} \\cdot p^x (1-p)^{n-x}\\\\ &amp;= \\sum_{x=1}^n \\frac{x}{x(x-1)!} \\frac{n(n-1)!}{(n-x)!} p p^{x-1} (1-p)^{n-x}\\\\ &amp;= \\sum_{x=1}^n \\frac{1}{(x-1)!}\\frac{n(n-1)!}{(n-x-1+1)!} p p^{x-1} (1-p)^{n-x-1+1}\\\\ &amp;= np \\sum_{x=1}^n \\frac{1}{(x-1)!}\\frac{(n-1)!}{((n-1)-(x-1))! (x-1)!} p\\cdot p^{x-1} (1-p)^{(n-1)-(x-1)}\\\\ &amp;= np \\; \\underbrace{\\sum_{y=0}^{n-1} \\frac{(n-1)!}{((n-1)-y)! y!} \\cdot p^{y} (1-p)^{(n-1)-y}}_{=1\\text{b/c sum of PF of bin(n-1,p). is 1}}\\\\ &amp;= np \\end{align*}\\] "],["lecture-18-feburary-16-2024.html", "19 Lecture 18, Feburary 16, 2024 19.1 Mean of Poisson distribution 19.2 Mean of Hypergeometric and Negative binomial 19.3 Note that the expectation does not always exist", " 19 Lecture 18, Feburary 16, 2024 19.1 Mean of Poisson distribution Let \\(Z\\sim Poi(\\mu)\\). Then the expectation of \\(Z\\) is \\(E[Z] = \\mu\\). Proof. Suppose \\(X \\sim Poi(\\mu)\\). Then we have \\[\\begin{align*} E[X] &amp;= \\sum\\limits_\\text{x \\in X(S)} x\\cdot f(x) &amp;&amp; \\text{definition}\\\\ &amp;= \\sum_{x=0}^\\infty x \\cdot e^{-\\mu} \\frac{\\mu^x}{x!} &amp;&amp; \\text{P.F.}\\\\ &amp;= \\sum_{x=1}^\\infty x \\cdot e^{-\\mu} \\frac{\\mu^x}{x!}\\\\ &amp;= \\mu \\sum_{x=1}^\\infty e^{-\\mu} \\frac{\\mu^{x-1}}{(x-1)!} &amp;&amp; \\text{let $Y=x-1$}\\\\ &amp;= \\mu \\underbrace{\\sum_{y=0}^\\infty e^{-\\mu} \\frac{\\mu^{y}}{y!}}_{=1\\text{ as sum of p.f.}}\\\\ \\end{align*}\\] 19.2 Mean of Hypergeometric and Negative binomial 19.2.1 Hypergeometric If \\(X \\sim hyp(N,r,n)\\), then \\[E[X]= n \\frac{r}{N}.\\] Intuitively, \\(n \\cdot r/N\\) is the number of trials \\(n\\) times the success probability in the 19.2.2 Negative binomial If \\(Y \\sim NB(k,p)\\), then \\[E[Y] = \\frac{k(1-p)}{p}.\\] 19.3 Note that the expectation does not always exist Example 19.1 Consider a random variable \\(X\\) with probability function \\(f(x)=\\frac{1}{x}\\) for \\(x=2, 4, 8, 16,\\dots\\) and 0 otherwise. The rv \\(X\\) can only take values \\(x\\) of the form \\(x=2^n\\). Note that we can write \\(P(X=2^n) = 2^{-n}\\) for \\(n=1,2,\\dots\\). Thus \\[\\sum\\limits_{\\text{ all x}} f(x) = \\sum_{n=1}^\\infty P(X=2^n)= \\sum_{n=1}^\\infty \\left(\\frac{1}{2}\\right)^n=\\frac{1}{1-1/2} -1 = 1.\\] Then \\[ E[X] = \\sum_{n=1}^\\infty 2^n \\left(\\frac{1}{2^n}\\right)=\\sum_{n=1}^\\infty 1 = \\infty,\\] "],["lecture-19-feburary-26-2024.html", "20 Lecture 19, Feburary 26, 2024 20.1 Motivation to have higher moments 20.2 The defintion of variance 20.3 Variance of a binomial distribution", " 20 Lecture 19, Feburary 26, 2024 After discussing about the first moment/expectation/expected value, we want to see how other summary statistics we may want in order to describe a distribution/model. 20.1 Motivation to have higher moments Note: Does look at one summary statistics, the expectation, enough for describing and fully characterize a distribution? Example 20.1 Suppose we have two random variables \\(X\\) is a r.v. representing the outcome of one fair 6-sided die roll \\(Y\\) is a r.v. representing the number of phone calls over 1 minute at Lenovo call centre, with the rate of 3.5 calls per minute By looking at their expectations, we have \\[ E X = 3.5 = E Y. \\] So if we only look at the expectation, we CANNOT DISTINGUISH those two, but clearly those two RV are very different! Hence, we may need other quantities to describe the random variables. One key thing to study is how the RVs deviate from its mean/expectation. 20.1.1 Deviations from the mean Some common used ones are Deviation \\[\\mbox{E}((X- \\mu)) = \\mbox{E}(X)- \\mu = 0\\] 2. Absolute deviation \\[ \\mbox{E} \\left(| X - \\mu |\\right)\\] Squared deviation \\[ \\mbox{E} \\left((X-\\mu)^2\\right) \\]. The squared deviation turns out to be particular useful measure of variability, which we coin the term as the Variance. 20.2 The defintion of variance Definition 20.1 (variance) The variance of a random variable \\(X\\) is denoted by \\(Var(X)\\), and is defined by \\[ Var(X) := E[(X-EX)^2]. \\] 20.2.1 Properties of variance Shortcut definition \\[ Var(X) = E[X^2] - (E X)^2. \\] Variance is always positive. This can be easily seen from the definition of the variance, that it is the expectation of the square of \\(X-EX\\). Everything squared is a non-negative number. And expectation can be thought as the weighted average of a random variable. Thus, it is Always Nonnegative! Variance is not linear Let \\(a,b\\in \\mathbb{R}\\), and \\(X\\) be a RV. Then \\[ Var(aX +b ) = a^2 Var(X). \\] The case when variance is zero Suppose a random variable \\(X\\) has \\(E(X)=\\mu\\) and \\(Var(X)=0\\). This means, \\(X\\) does not ``vary’’ or deviate from its mean at all, and is (with probability 1) always the same value \\(\\mu\\), as we show in the following Theorem 20.1 \\(Var(X) = 0\\) if and only if \\(P(X = \\mbox{E}(X)) = 1\\). Alternative way to write the variance We can write the variance of X as \\[ Var(X) = E[X(X-1)] + E[X] - (EX)^2. \\] Proof. Look at the right-hand-side, \\[\\begin{align*} &amp;E[X(X-1)]+ E[X] - E^2[X]\\\\ &amp;=E[X^2-X] + E[X] - E^2[X]\\\\ &amp;= E[X^2 -X + X - E^2[X]] \\\\ &amp;= E[X^2 -\\mu^2] = E[X^2] - \\mu^2, \\end{align*}\\] as in our shortcut formula. 20.3 Variance of a binomial distribution Theorem 20.2 Suppose that \\(X\\sim Binomial(n,p)\\), then \\[ Var(X) = np(1-p). \\] Proof. We use the formula \\[Var(X) = E(X(X-1)) + E(X) - (E(X))^2\\] and note we already know \\(E(X)=np\\). Then we tweek the sum for \\(E(X(X-1))\\) similarly as before: \\[\\begin{align*} E&amp;(X(X-1)) =\\sum_{x=0}^n x(x-1) \\frac{n!}{(n-x)! x!} p^x (1-p)^{n-x} \\\\ &amp;= n(n-1) p^2 \\sum_{x=2}^n \\frac{(n-2)!}{(n-2-(x-2))!(x-2)!} p^{x-2} (1-p)^{n-2-(x-2)}\\\\ &amp;= n(n-1) p^2 \\underbrace{\\sum_{y=0}^{n-2} \\frac{(n-2)!}{(n-2-y)!y!} p^{y} (1-p)^{n-2-y}}_{=1\\text{ as sum of $Bin(n-2,p)$ p.f.}}\\\\ &amp;= n(n-1) p^2 \\end{align*}\\] "],["lecture-20-feburary-28-2024.html", "21 Lecture 20, Feburary 28, 2024 21.1 Variance of Poisson, Hypergeometric and Negative Binomial 21.2 Standard Deviation 21.3 Last note of the chapter 21.4 Chapter 8 Continuous Random Variables", " 21 Lecture 20, Feburary 28, 2024 21.1 Variance of Poisson, Hypergeometric and Negative Binomial Last time, we saw that if \\(X \\sim Bin(m,p)\\), then \\(\\mathbb{V}ar(X) = np(1-p)\\). We proved this using the definition of the expectation and with the summation trick. Similarly, one can show that If \\(X\\sim Poi(\\lambda)\\), then \\[ \\mathbb{V}ar(X) = \\lambda. \\] If \\(Y \\sim hyp(N,r,n)\\), then \\[ \\mathbb{V}ar(Y) = n \\frac{r}{N} \\left(1-\\frac{r}{N}\\right)\\left(\\frac{N-n}{N-1}\\right). \\] If \\(Z \\sim NB(k,p)\\), then \\[ \\mathbb{V}ar(Z) = \\frac{k(1-p)}{p^2}. \\] 21.2 Standard Deviation Note that \\(\\mathbb{V}ar(X)\\) is in the squared unit (e.g., \\(X\\) in \\(meters\\) \\(\\Rightarrow\\) \\(\\mathbb{V}ar(X)\\) is in \\(meters^2\\)). To recover the original unit, we take the square root of variance.\\ Definition 21.1 (Standard Deviation) The standard deviation of a random variable \\(X\\) is denoted \\(SD(X)\\), and defined by \\[ SD(X) = \\sqrt{\\mathbb{V}ar(X)}. \\] 21.3 Last note of the chapter The expectation and the variance give a simple giving the center and variability of the distribution We call \\(E[X]\\) and \\(E[X^2]\\) the first and second moment of \\(X\\) In general, \\(E[X^k]\\) is the \\(k\\)th moment of the distribution of \\(X\\), while \\(E[ (X-E(X))^k]\\) is the \\(k\\)th central moment of the distribution of \\(X\\) You’ll see other statistics later in STAT 231 and onwards, such as Skewness (measures asymmetry) \\[ E\\left[\\left( \\frac{(X - E(X))}{\\sqrt{\\mathbb{V}ar(X)}} \\right)^3\\right]. \\] Kurtosis (measures heavy tailedness) \\[ E\\left[\\left( \\frac{(X - E(X))}{\\sqrt{\\mathbb{V}ar(X)}} \\right)^4\\right]. \\] 21.4 Chapter 8 Continuous Random Variables 21.4.1 Continuous random variable Let \\(X\\) be a random variable and \\(F_X(x) = P(X\\leq x) = P(\\{\\omega\\in S : X(\\omega)\\leq x\\})\\) for \\(x\\in\\mathbb{R}\\) be its cumulative distribution function (cdf). We say that the random variable \\(X\\) is discrete if \\(F_X\\) is piecewise constant. The jumps of \\(F\\) are exactly the range of \\(X\\), \\(X(S)\\). For \\(x\\in X(S)\\) (at the jumps of \\(F\\)), the probability function is \\(f(x)=P(X=x)=\\lim_{h\\downarrow 0} F(x+h)-F(x)=\\text{size of jump at $x$}\\). continuous if \\(F_X\\) is a continuous function. absolutely continuous if \\[ F_X(x) = \\int_{-\\infty}^x f(t) dt\\] In this course, when talking about continuous random variables, we mean absolutely continuous. ### Probability density function Definition 21.2 (Probability Density Function) We say that an continuous random variable \\(X\\) with distribution function \\(F\\) admits probability function (PDF) \\(f(x)\\), if \\(f(x)\\geq 0\\) for all \\(x\\in\\mathbb{R}\\); \\(\\int_{-\\infty}^\\infty f(x)dx = 1\\); \\(F(x)=P(X\\leq x)= \\int_{-\\infty}^x f(t)\\;d t\\). In other words, \\(F\\) is an antiderivative of \\(f\\), of \\(f\\) is the derivative of \\(F\\), \\[ f(x) = F&#39;(x) = \\frac{d}{dx} F(x)\\] Definition 21.3 (Support) The support of a r.v. \\(X\\) with density \\(F\\) is the set \\[ supp(f) = \\{x \\in \\mathbb{R}: f(x) \\neq 0\\}. \\] If \\(X\\) was a discrete random variable instead, these 4 probabilities could all be different. If \\(X\\) is a continuous rv with probability density function (pdf) \\(f_X(x)\\), then \\(P(X=x)=0\\) \\(F(x) = \\int_{-\\infty}^x f_X(t)dt\\) \\(P(a &lt; X \\leq b) = \\int_a^b f_X(t)dt\\) We highlight: For a continuous random variable \\(X\\), \\(f(x)\\) is not \\(P(X=x)\\), which is always zero. 21.4.2 Equality does not matter in the continous case If \\(X\\) is a continuous random variable, then \\[ P(a&lt;X\\leq b) = F(b)-F(a)\\] \\[ P(a\\leq X \\leq b) = P(a&lt;X\\leq b) +P(X=a)=[F(b)-F(a)]+0\\] \\[ P(a&lt;X&lt;b)=P(a&lt;X\\leq b) -P(X=b)=[F(b)-F(a)]-0\\] \\[ P(a\\leq X&lt;b) = P(a&lt;X\\leq b) +P(X=a)-P(X=b)=[F(b)-F(a)]\\] so if \\(X\\) is continuous, all these probabilities coincide! "],["lecture-21-march-01-2024.html", "22 Lecture 21, March 01, 2024 22.1 Law of unconciousness of statistician, continuous version 22.2 function of random variable", " 22 Lecture 21, March 01, 2024 22.1 Law of unconciousness of statistician, continuous version Definition 22.1 (LOTUS) If \\(X\\) is a continuous random variable with pdf \\(f(x)\\), and \\(g:\\mathbb{R}\\to \\mathbb{R}\\) is a function, then \\[ \\mathbb{E}g(x) = \\int_{-\\infty}^\\infty g(x)f(x)dx \\] provided the expression exists. By above, we can calculate the expectation and the variance as follows \\(\\mathbb{E}X = \\int_{-\\infty}^\\infty x f(x) dx\\) \\(\\mathbb{V}ar(X) = \\mathbb{E}[(X-\\mathbb{E}X)^2] = \\int_{-\\infty}^\\infty (x-\\mathbb{E}X)^2 f(x)dx\\). Similar to the discrete random variable case, we have the shortcut formula to calculate the variance: \\[ \\mathbb{V}ar(X) = \\mathbb{E}[X^2] - (\\mathbb{E}X)^2. \\] 22.2 function of random variable If \\(X\\) is a random variable and \\(g\\) is a function, then \\(Y=g(X)\\) is also a random variable By the law of the unconscious statistician, \\[ \\mathbb{E}(Y)=\\mathbb{E}(g(X)) = \\begin{cases} \\sum_{\\text{all }x} g(x)\\cdot f(x),\\quad&amp;\\text{if $X$ discrete with pf }f,\\\\ \\int_{\\mathbb{R}} g(x)\\cdot f(x)dx,\\quad&amp;\\text{if $X$ continuous with pdf }f\\end{cases} \\] Next, we are studying how to find the distribution of \\(Y=g(X)\\). "],["lecture-22-march-04-2024.html", "23 Lecture 22, March 04, 2024 23.1 Receipt to find the distribution of the transformed random varaible \\(Y=g(X)\\). 23.2 Quantile", " 23 Lecture 22, March 04, 2024 23.1 Receipt to find the distribution of the transformed random varaible \\(Y=g(X)\\). In class, we have an easy three steps receipt: Let \\(Y=g(X)\\). In general, we can use the following steps to find the pdf of \\(Y=g(X)\\): Using the support of \\(X\\), find the support of \\(Y=g(X)\\) denoted by \\(\\text{supp}(Y)=\\{y\\in\\mathbb{R}:f_Y(y)&gt;0\\}\\) Express the cdf of \\(Y=g(X)\\) using the cdf of \\(X\\): $F_Y(y)=P(g(X)y)=$. Compute the pdf of \\(Y=g(X)\\) by differentiating \\(f_y(y)=F_y&#39;(y)\\) Notes: If the function \\(g\\) is invertible and differentiable with inverse \\(g^{-1}\\) on the support of \\(Y\\), then \\[ f_Y(y)= |(g^{-1})&#39;(y)| f_X(g^{-1}(y)),\\quad y\\in\\text{supp}(Y)\\] Question: When the function \\(g\\) is not strictly increasing (or decreasing) over the support of \\(X\\), then we must be careful when rewriting the inequality \\(P(g(X)\\leq y)\\). 23.2 Quantile Definition 23.1 (Quantile) Let \\(p\\in[0,1]\\). The \\(100\\times p\\)th percentile (or \\(100\\times p\\%\\) quantile) of the distribution of \\(X\\) with cdf \\(F_X\\) is the value \\(c_p\\) given by \\[ c_p = \\inf\\{x\\in\\mathbb{R}: F_X(x) \\geq p \\}\\] The infimum of a set \\(A\\) is the largest lower bound of \\(A\\) (e.g., \\(\\inf\\{x\\in\\mathbb{R}: 0&lt;x&lt;1\\}=0\\)) The quantile function \\(p\\mapsto c_p\\) is also called generalized invere function. The probability that \\(X\\) is at most \\(c_p\\) is at least \\(100\\times p\\)%. More precisely, \\(c_p\\) is the smallest value \\(c\\) so \\(P(X\\leq c)\\) is at least \\(p\\). *The of a distribution is its 50% quantile. If the distribution function \\(F_X\\) is continuous and strictly increasing, it has an inverse \\(F^{-1}\\), and we get \\[ c_p = F^{-1}(p)\\] In the previous clicker question, we computed the 95% quantile. "],["lecture-23-march-06-2024.html", "24 Lecture 23, March 06, 2024 24.1 Recap the quantile function 24.2 Quantiles for discrete distributions 24.3 Special named distributions", " 24 Lecture 23, March 06, 2024 24.1 Recap the quantile function Definition 24.1 (Quantile) Let \\(p\\in[0,1]\\). The \\(100\\times p\\)th percentile (or \\(100\\times p\\%\\) quantile) of the distribution of \\(X\\) with cdf \\(F_X\\) is the value \\(c_p\\) given by \\[ c_p = \\inf\\{x\\in\\mathbb{R}: F_X(x) \\geq p \\}\\] The infimum of a set \\(A\\) is the largest lower bound of \\(A\\) (e.g., \\(\\inf\\{x\\in\\mathbb{R}: 0&lt;x&lt;1\\}=0\\)) The quantile function \\(p\\mapsto c_p\\) is also called generalized invere function. The probability that \\(X\\) is at most \\(c_p\\) is at least \\(100\\times p\\)%. More precisely, \\(c_p\\) is the smallest value \\(c\\) so \\(P(X\\leq c)\\) is at least \\(p\\). The median of a distribution is its 50% quantile. If the distribution function \\(F_X\\) is continuous and strictly increasing, it has an inverse \\(F^{-1}\\), and we get \\[ c_p = F^{-1}(p)\\] In the previous clicker question, we computed the 95% quantile. 24.2 Quantiles for discrete distributions If \\(F\\) is strictly increasing and continuous, the \\(p\\)-quantile \\(c_p\\) satisfying \\(F(c_p)=p\\) is just \\(c_p=F^{-1}(p)\\), the (ordinary) inverse of \\(F\\) at \\(p\\) found by solving \\(F(c_p)=p\\) for \\(c_p\\). If \\(F\\) has jumps or flat parts, then \\(F(c_p)=p\\) may not have any solution or infinitely many! In this case, we use \\[ F^{-1}(p)=\\inf_{x\\in\\mathbb{R}}\\{F(x)\\geq p\\},\\] though \\(F^{-1}\\) is an abuse of notation here and does not mean the ordinary inverse. 24.3 Special named distributions 24.3.1 Continuous uniform distribution Definition 24.2 (Continuous uniform distribution) We say that \\(X\\) has a continuous uniform distribution on \\((a,b)\\) if \\(X\\) has pdf \\[ f(x) =\\begin{cases} \\frac{1}{b-a} &amp; \\mbox{ $x \\in (a,b)$,} \\\\ 0 &amp; \\mbox{ otherwise } \\end{cases} \\] This is abbreviated \\(X \\sim U(a,b)\\). For continuous random variables, \\(P(X=x)=0\\), so it does not matter mathematically if we think of the uniform distribution as sampling uniformly on \\((a,b)\\) or \\([a,b]\\) or \\((a,b]\\) or \\([a,b)\\) Examples: Cutting a stick of length 1 at a random position (motivating example!) Spinning a wheel in a game show 24.3.1.1 Expectation and variance Definition 24.3 (Expectation of continuous uniform distribution) Let \\(X\\sim U(a,b)\\). Then \\[E(X) = \\frac{a + b}{2}\\] Proof. Recall that \\(X\\sim U(a,b)\\) has density \\(f(x)=\\frac{1}{b-a}\\) if \\(x\\in(a,b)\\) and 0 otherwise. Thus, \\[\\begin{align*} E(X) &amp;= \\int x f(x)\\; d x = \\int_a^b x \\cdot \\frac{1}{b-a}\\;d x \\\\ &amp;= \\frac{1}{2} \\frac{b^2-a^2}{b-a}=\\frac{(b-a)(b+a)}{2(b-a)}=\\frac{a+b}{2}.\\end{align*}\\] Definition 24.4 (Variance of continuous uniform distribution) Let \\(X\\sim U(a,b)\\). Then \\[Var(X) = \\frac{(b-a)^2}{12}\\] Proof. Recall that \\(X\\sim U(a,b)\\) has density \\(f(x)=\\frac{1}{b-a}\\) if \\(x\\in(a,b)\\) and 0 otherwise. ALso from above, we have \\(E X =\\frac{a + b}{2}\\). Then \\[\\begin{align*} E(X^2) &amp;= \\int x^2 f(x)\\;d x = \\int_a^b x^2 \\frac{1}{b-a}\\;d x \\\\ &amp;= \\frac{b^3-a^3}{3(b-a)}=\\frac{(b-a)(b^2+ab+a^2)}{3(b-a)}=\\frac{b^2+ab+a^2}{3}.\\end{align*}\\] Combining and simplifying gives \\[ Var(X) = E(X^2)-E(X)^2 = \\frac{(b-a)^2}{12}.\\] 24.3.2 Exponential distribution Definition 24.5 (Rate-parametrization of exponential distribution) We say that \\(X\\) has an exponential distribution with parameter \\(\\lambda\\), denoted by \\(X\\sim Exp(\\lambda)\\), if the density of \\(X\\) is \\[ f(x) =\\begin{cases} \\lambda e ^{- \\lambda x} &amp; \\mbox{ $x &gt;0$,} \\\\ 0 &amp; \\mbox{ $x \\le 0$ }. \\end{cases} \\] Since \\(\\int_{\\mathbb{R}} f(x) dx = \\int_0^\\infty \\lambda e^{-\\lambda x}dx =1\\) and \\(f(x)\\geq 0\\) for all \\(x\\in\\mathbb{R}\\), this is a valid pdf. 24.3.2.1 Moments When computing \\(E(X)\\) and \\(Var(X)\\), we need to solve integrals \\[ E(X) = \\int_0^\\infty x\\cdot \\frac{1}{\\theta} e^{-\\frac{x}{\\theta}}\\;d x\\] and \\[ E(X^2) = \\int_0^\\infty x^2 \\cdot \\frac{1}{\\theta} e^{-\\frac{x}{\\theta}}\\;d x\\] which can be done using integration by parts. Alternatively, we can use the gamma function Definition 24.6 (Gamma function) The integral \\[ \\Gamma(\\alpha) = \\int_0^\\infty y^{\\alpha - 1} e^{-y} dy, \\ \\alpha &gt; 0 \\] is called the gamma function of \\(\\alpha\\). 24.3.2.1.1 Properties of Gamma function Some useful properties of \\(\\Gamma(\\alpha)\\) are \\(\\Gamma(\\alpha) = (\\alpha - 1)\\Gamma(\\alpha - 1)\\) for \\(\\alpha &gt; 1\\) \\(\\Gamma(\\alpha) = (\\alpha - 1)!\\) for \\(\\alpha \\in \\mathbb{N}\\) \\(\\Gamma(1/2) = \\sqrt{\\pi}\\) The Gamma function is a continuous function that interpolates the factorial function. Gamma function is used to derive the Gamma distribution (\\(\\Rightarrow\\) STAT 330), which is extremely important in non-life insurance pricing, and it can be used to model certain brain signals in neuroscience. 24.3.2.2 Expectation and variance With the Gamma function at hand, show that if \\(X\\sim Exp(\\theta)\\), then \\[ E(X)=\\theta\\] and \\[Var(X)=\\theta^2.\\] There are other paramaterizations for exponential distribution. It is sometimes more convenient to express the parameter as \\(\\frac{1}{\\theta}=\\lambda\\). Definition 24.7 (theta-parametrisation of exponential distribution) We say that \\(X\\) has an exponential distribution with parameter \\(\\theta\\) \\((X\\sim Exp(\\theta))\\) if the density of \\(X\\) is \\[ f(x) =\\begin{cases} \\frac{1}{\\theta} e ^{- \\frac{x}{\\theta}} &amp; \\mbox{ $x &gt;0$,} \\\\ 0 &amp; \\mbox{ $x \\le 0$ }. \\end{cases} \\] If \\(\\lambda\\) denotes the rate of event occurrence in a Poisson process, then \\(\\theta = 1/\\lambda\\) denotes the waiting time until the first occurrence. "],["lecture-24-march-08-2024.html", "25 Lecture 24, March 08, 2024 25.1 Proof of the moments of exponential distribution", " 25 Lecture 24, March 08, 2024 25.1 Proof of the moments of exponential distribution Recall that we can have many ways to show the expectation and the variance of \\(X\\sim Exp(\\theta)\\) using the Gamma function. Then we have \\[ E(X)=\\theta\\] and \\[Var(X)=\\theta^2.\\] Let \\(X\\sim Exp(\\theta)\\). We use the change of variable \\(y=x\\theta\\) with \\(dx =\\theta dy\\) \\[\\begin{align*} E[X]&amp;= \\int_0^\\infty x \\cdot \\frac{1}{\\theta} e^{-\\frac{x}{\\theta}}dx \\overset{y=x/\\theta}{=} \\int_0^\\infty y e^{-y} \\theta dy\\\\ &amp;= \\theta \\underbrace{\\int_0^\\infty y e^{-y} dy}_{=\\Gamma(2)}= \\theta \\Gamma(2) = \\theta \\cdot (1!) =\\theta \\end{align*}\\] and similarly \\[\\begin{align*} E[X^2]&amp;= \\int_0^\\infty x^2 \\cdot \\frac{1}{\\theta} e^{-\\frac{x}{\\theta}}dx\\overset{y=x/\\theta}{=} \\int_0^\\infty \\theta y^2 e^{-y} \\theta dy\\\\ &amp;= \\theta^2 \\underbrace{\\int_0^\\infty y^{3-1} e^{-y} dy}_{=\\Gamma(3)}= \\theta^2 \\Gamma(3) = \\theta \\cdot (2!) =2\\theta^2 \\end{align*}\\] so that \\[ Var(X) = E[X^2]-E[X]^2=2\\theta^2-\\theta^2 =\\theta^2\\] 25.1.1 Memoryless property of exponential distribution Theorem 25.1 (Memoryless property) If \\(X \\sim Exp(\\theta)\\), then \\[ P(X &gt; s + t | X &gt; s) = P(X &gt; t). \\] We’ve seen the memoryless property for the \\(Geo(p)\\) earlier (and the geometric distribution is the only discrete distribution with this property) If a continuous random variable has memoryless property, it must follow exponential distribution. Intuitively, both the geometric and exponential distributions measure waiting time until first success Proof. Recall the cdf of \\(X\\sim Exp(\\theta)\\) is \\[ F(x)=P(X\\leq x) =\\int_{-\\infty}^x f(t)dt = \\int_0^x \\theta^{-1} e^{-t/\\theta}dt = 1-\\exp(-x/\\theta)\\] for \\(x&gt;0\\) and 0 otherwise. Hence, \\[\\begin{align*} P(X &gt; s + t | X &gt; s) &amp;= \\frac{P( X&gt;s+t \\text{ and }X&gt;s)}{P(X&gt;s)}\\\\ &amp;= \\frac{P(X&gt;s+t)}{P(X&gt;s)} = \\frac{e^{-(s+t)/\\theta}}{e^{-s/\\theta}}\\\\ &amp;= e^{-t/\\theta} = 1-F(t) = P(X&gt;t) \\end{align*}\\] as desired. "],["lecture-24-march-11-2024.html", "26 Lecture 24, March 11, 2024 26.1 Sampling realizations of random variables 26.2 Inversion method 26.3 Normal/Gaussian distribution", " 26 Lecture 24, March 11, 2024 26.1 Sampling realizations of random variables In practice, we may often want to sample realizations of random variables, and use those realizations to conduct some estimation of inference (\\(\\Rightarrow\\) Monte Carlo Simulation) For instance, suppose we wish to estimate (approximate) the probability \\(P(X&gt;2)\\) for \\(X\\sim Exp(1)\\) by simulation of course, we can compute \\(P(X&gt;2)\\) by hand, but let’s pretend we can’t. In this case, could sample independent realizations \\(x_1,\\dots,x_n\\) from \\(Exp(1)\\) (numbers that look like realizations \\(X(\\omega_1),\\dots,X(\\omega_n)\\)), and then estimate \\[P(X&gt;2)\\approx \\frac{|\\{x_1,\\dots,x_n: x_i&gt;2\\}|}{n}\\] that is, we estimate \\(P(X&gt;2)\\) by the relative frequency of observations larger than 2. 26.2 Inversion method 26.2.1 With strictly increasing and continuous assumption Lemma 26.1 If \\(F\\) is a continuous and strictly increasing cdf of some random variable \\(X\\) and if \\(U\\sim Unif(0,1)\\), then the random variable \\(Y=F^{-1}(U)\\) has cdf \\(F\\). Proof. Denote by \\(F_Y\\) the cdf of the random variable \\(Y = F^{-1}(U)\\). Then, \\[ F_Y(y) = P(F^{-1}(U)\\leq x) = P(F(F^{-1}(U))\\leq F(y))=P(U\\leq F(y)).\\] But for any \\(u\\in[0,1]\\), we know that \\(P(U\\leq u)=u\\), since \\(U\\sim U(0,1)\\). Hence, \\[ F_Y(x) = P(U\\leq F(y))=F(y)\\] Hence, the random variable \\(Y = F^{-1}(U)\\) has the cdf \\(F\\), as desired. 26.2.2 More general case using the quantile By using the more general definition of the quantile function \\[F^{-1}(y) = \\inf\\{x\\in\\mathbb{R}: F(x)\\geq y\\}\\] one can show the following generalization: Theorem 26.1 Let \\(F\\) be any cumulative distribution function of some random variable \\(X\\) and \\(U\\sim U(0,1)\\). Then the random variable \\(F^{-1}(U)\\) has cdf \\(F\\). No matter what cdf \\(F\\) (discrete or continuous), we can sample observations as follows: Sample \\(U\\sim U(0,1)\\) (eg via ) Return \\(X=F^{-1}(U)\\). Repeating this \\(n\\) times independently gives \\(n\\) realizations from \\(F\\). 26.3 Normal/Gaussian distribution Gaussian distribution is named as Gauss, and is perhaps one of the most important distribution, if not the most important one. Definition 26.1 \\(X\\) is said to have a (or Gaussian distribution) with mean \\(\\mu\\) and variance \\(\\sigma^2\\) if the density of \\(X\\) is \\[ f(x) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}, \\;\\;\\; x\\in {\\mathbb R}. \\] We denote it by \\(X\\sim N(\\mu,\\sigma^2)\\). 26.3.1 Properties of Gaussian distribution 1/ Symmetric about its mean: If \\(X \\sim N(\\mu, \\sigma^2)\\) \\[ P( X \\le \\mu - t) = P(X \\ge \\mu + t). \\] 2. Density is unimodal: Peak is at \\(\\mu\\). Mean and Variance are the parameters: \\[E(X)= \\mu\\] and \\[Var(X)=\\sigma^2.\\] \\(N(\\mu, \\sigma^2)\\) is sometimes (STAT 231) also parametrised as Gaussian distribution using \\(\\sigma\\) instead of \\(\\sigma^2\\), where \\[ X \\sim G(\\mu, \\sigma). \\] That is, \\(X\\sim N(1, 4)\\) and \\(X\\sim G(1, 2)\\) mean the same thing. Median = mean = mode = first moment. 26.3.2 Problem It is difficult to calculate the CDF! If \\(X\\sim N(\\mu, \\sigma^2)\\), then \\[ P(a \\le X \\le b ) = \\int_a^b \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}} dx = ??? \\] functions of the form \\(e^{-x^2}\\) do not have elementary anti-derivatives… :( "],["lecture-26-march-13-2024.html", "27 Lecture 26, March 13, 2024 27.1 Normal distribution continue", " 27 Lecture 26, March 13, 2024 27.1 Normal distribution continue If we have a random variable follows an arbitrary Gaussian distribution, i.e. \\(X\\sim \\mathcal{N}(\\mu,\\sigma^2)\\). How do we obtain the CDF and quantile? It turns out that we can standardize/transform the RV \\(X\\) to \\(Z\\sim\\mathcal{N}(0,1)\\). 27.1.1 Standard normal distribution We say that \\(Z\\) follows the standard normal distribution if \\(Z \\sim \\mathcal{N}(0,1)\\). Frequently in probability and statistics literature, the density of the standard normal random variable is denoted \\[ \\varphi(x)= \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-x^2}{2}}, \\] and the cdf of a standard normal random variable is denoted \\[ \\Phi(x)= \\int_{-\\infty}^x \\frac{1}{\\sqrt{2\\pi}}e^{\\frac{-y^2}{2}}dy. \\] Aside: Why is there \\(1/\\sqrt{2 \\pi}\\) in the pdf? That’s because \\[ \\int_{-\\infty}^\\infty e^{\\frac{-x^2}{2}} dx = \\sqrt{2\\pi}, \\] so if we divide \\(e^{\\frac{-x^2}{2}}\\) by the integral by \\(\\sqrt{2 \\pi}\\) we obtain a valid pdf (non-negative, integrates to 1). Values of \\(\\Phi(z)=P(Z\\leq z)=\\int_{-\\infty}^z \\varphi(t)dt\\) can be approximated numerically with high accuracy. Here we can either use a function in R, pnorm(), or use the z-table. Now we know how to computer the CDF for \\(\\mathcal{N}(0,1)\\), how to link it to \\(\\mathcal{N}(\\mu,\\sigma^2)\\)? Theorem 27.1 (Standardising normal random variable) If $X \\sim N(\\mu, \\sigma^2)$, then $$ Z = \\frac{X - \\mu}{\\sigma} \\sim \\N(0,1), $$ and $P(X \\leq x) = P\\left(Z \\leq \\dfrac{x - \\mu}{\\sigma}\\right)$. 27.1.2 Procedure for computing \\(P(X\\leq x)\\) for \\(X\\sim N(\\mu,\\sigma^2)\\): Compute \\(z=\\frac{x-\\mu}{\\sigma}\\) (``z-score’’). Find \\(\\Phi(z)=P(Z\\leq z)\\) in the table where \\(Z\\sim N(0,1)\\). Return \\(P(X\\leq x)=\\Phi(z)\\). 27.1.3 Quantile The Z-table can also be used to obtain percentiles and quantiles. Let \\(Z\\sim N(0,1)\\) and \\(p\\in(0,1)\\). Then we can find the value \\(z_p\\) so that \\(P(Z\\leq z_q)=\\Phi(z_p)=p\\) either by \\(\\dots\\) looking at the top of the \\(z\\)-table and selecting the value \\(z_p\\) so that \\(\\Phi(z_p)\\) is closest to \\(p\\) \\(\\dots\\) looking at the bottom of the table, which directly gives the quantile for selected \\(p\\geq 0.5\\) (for \\(p&lt;0.5\\), use \\(\\Phi^{-1}(p)=-\\Phi^{-1}(1-p)\\)). This is . If \\(X \\sim \\mathcal{N}(\\mu,\\sigma^2)\\) and we want \\(x_p\\) so that \\(P(X\\leq x_p)=p\\) First find \\(z_p\\) for the standard normal distribution \\(N(0,1)\\). Second, set \\(x_p= \\mu + \\sigma z_p\\). Then \\[P(X\\leq x_p) = P(X\\leq \\mu+\\sigma z_p)=P\\left( \\underbrace{(X-\\mu)/\\sigma}_{\\sim N(0,1)} \\leq z_p\\right)=\\Phi(z_p)=p\\] 27.1.4 68-95-99.7 rule for 1-2-3 standard deviation(s) An interesting empirical rule about normal distribution is the 68-95-99.7 rule, which states: If \\(X \\sim N(\\mu, \\sigma^2)\\), then \\[ P( \\mu - \\sigma \\le X \\le \\mu + \\sigma) \\approx 0.68 \\] \\[ P( \\mu - 2\\sigma \\le X \\le \\mu + 2\\sigma) \\approx 0.95 \\] \\[ P( \\mu - 3\\sigma \\le X \\le \\mu + 3\\sigma) \\approx 0.997. \\] "],["lecture-27-march-15-2024.html", "28 Lecture 27, March 15, 2024 28.1 Chapter 9: Multivariate distributions", " 28 Lecture 27, March 15, 2024 Recap: In previous lecture, we discussed about the relationship between a standard normal \\(\\mathcal{N}(0,1)\\) and an arbitrary normal \\(\\mathcal{N}(\\mu,\\sigma^2)\\). The steps are as fellows. Transform/standardize \\(X\\sim\\mathcal{N}(\\mu,\\sigma^2)\\) to \\(Z\\sim\\mathcal{N}(0,1)\\) by let \\(Z=\\frac{x-\\mu}{\\sigma}\\). Use the z-table or the R pronrm() and qnorm() functions. If you use the z-table in this class, we only provide the standard normal CDF for \\(Z&gt;0\\), so you may want to use the symmetric property of the normal distribution when you are asked to calculate the CDF for \\(P(Z\\le z)\\) where \\(z&lt;0\\). The other things to remember is the 68-95-99.7 rule for \\(\\pm\\) 1,2, and 3 standard deviation away from the mean \\(\\mu\\). 28.1 Chapter 9: Multivariate distributions Often, we may face problems that may have have multiple factors/covariates/features/causes. In those situations, only using one random variable is not sufficient to model the outcome. Hence, we need two or more random variables in those situations. When we have more than one random variables, we say we have a multivariate distribution. Q: How to extend what we have learned from univariate (i.e. one variable) to the multivariate case? A: We can extend the definitions we have before from the univariate case to the multivariate case! Definition 28.1 (Random Vector) Let \\(X_1,\\dots,X_n\\) be random variables defined on a common probability space. The vector \\((X_1,\\dots,X_n)\\) is called a random vector. Definition 28.2 (Joint Distribution) Suppose that \\(X\\) and \\(Y\\) are discrete random variables defined on the same sample space (in general, when we consider two or more random variables it is assumed they are defined on the same sample space.) The joint probability function of \\(X\\) and \\(Y\\) is \\[ f(x,y) = P( \\{\\omega\\in S:X(\\omega)=x\\} \\cap \\{\\omega\\in S:Y(\\omega)=y\\} ) \\] for \\(x\\in X(S),y\\in Y(S)\\) and 0 otherwise. As in the univariate case, a shorthand notation for this is \\[ f(x,y) = P(X=x,Y=y). \\] Q: What if we have more than two random variables? For a collection of \\(n\\) discrete random variables, \\(X_1,...,X_n\\), the joint probability function is defined as \\[ f(x_1,x_2,...,x_n)=P(X_1=x_1,X_2=x_2,...,X_n=x_n). \\] and we call the vector \\((X_1,\\dots,X_n)\\) a random vector. 28.1.1 Properties of the joint probability function Let \\(f(x,y)\\) be a joint probability function. Then \\(0\\leq f(x,y) \\le 1\\) \\(\\sum_{x,y} f(x,y) = 1\\). 28.1.2 Marginal distribution of the joint distribution function The joint probability function \\(f(x,y)\\) gives us probabilities \\(P(X=x, Y=y)\\). What if we just care about the probability \\(P(X=x)\\) (without caring about \\(Y\\)?) Definition 28.3 (Marginal probability function) Suppose that \\(X\\) and \\(Y\\) are {} random variables with joint probability function \\(f(x,y)\\). The marginal probability function of \\(X\\) is \\[ f_X(x) = P( X=x )= \\sum_{y \\in Y(S)} f(x,y). \\] Similarly, the marginal probability function of \\(Y\\) is \\[ f_Y(y) = P( Y=y )= \\sum_{x \\in X(S)} f(x,y). \\] 28.1.3 Comments A common mistake is to think that there is a difference between the marginal distribution of \\(X\\) and the probability function of \\(X\\). They are the same! When we only have one random variable, say \\(X\\), you may write the CDF and PF as \\(f\\), \\(F\\) to represent \\(f_X\\) and \\(F_X\\). However, if you have two or more random variables, say if two, denote by \\(X\\) and \\(Y\\). Then we NEED to write the subscript, \\(f_X\\), \\(f_Y\\), \\(F_X\\) and \\(F_Y\\) to distinguish which random variable you are mentioning. "],["lecture-28-march-18-2024.html", "29 Lecture 28, March 18, 2024", " 29 Lecture 28, March 18, 2024 In the previous lecture, we introduced the multivariate case (i.e. to have more than one random variables). This lecture, we want to see how some concepts we saw before can be used here. Definition 29.1 (Independence between random variables) \\(X\\) and \\(Y\\) are independent random variables if \\[ f(x,y) = f_X(x) f_Y(y) \\] for all values of \\((x,y)\\). More generally, \\(X_1, X_2, \\ldots, X_n\\) are independent if \\[ f(x_1, x_2, \\ldots, x_n) = f_1(x_1) f_2(x_2) \\ldots f_n(x_n) \\] for all values of \\((x_1, \\ldots, x_n)\\). Note: This means that if you find a single realization of \\((x_1, \\ldots, x_n)\\) values that doesn’t satisfy the above equation, then \\(X_1, \\ldots, X_n\\) are not independent. 29.0.1 Conditional probability function for multivaraite random variable Definition 29.2 (conditional probability function for bivariate case) The conditional probability function of \\(X\\) given \\(Y=y\\) is denoted \\(f_X(x|y)\\), and is defined to be \\[ f_X(x|y)= P(X=x|Y=y) = \\frac{P(X=x,Y=y)}{P(Y=y)} = \\frac{f(x,y)}{f_Y(y)}, \\] Given that \\(f_Y(y) &gt; 0\\). \\(f_Y(y|x)\\) can be defined similarly. 29.0.2 Probably function of \\(U=g(X_1,\\cdots,X_n)\\) With multiple random variables, we are even more interested in functions of such variables. For example, Let \\(A, M, F\\) be the random variables for your assignment, midterm and final grades respectively. Then, it’s natural to consider the overall grade \\(G = g(A,M,F)\\) as a function of random variables \\(A, M, F\\). In general, we have the following formula for the probability function of \\(U = g(X_1, X_2, \\ldots, X_n).\\) \\[ P(U = u) = \\sum_{\\substack{(x_1, \\ldots x_n) \\text{ such that }\\\\ g(x_1, \\ldots, x_n) = u}} f(x_1, \\ldots, x_n) \\] Of course, we restrict ourselves to \\((x_1,\\dots,x_n)\\) in the range of \\((X_1,\\dots,X_n)\\) and omit terms with \\(f(x_1, \\ldots, x_n)=0\\) 29.0.3 Known results for named distributions Theorem 29.1 (Sum of independent Poisson is Poisson) If \\(X \\sim Poi(\\lambda_1)\\) and \\(Y \\sim Poi(\\lambda_2)\\) independently, then \\(T = X + Y \\sim Poi(\\lambda_1 + \\lambda_2)\\). Proof. \\[\\begin{align*} P(X+Y=t) &amp;= \\sum_{(x,y):x+y= t} P(X=x, Y=y) \\\\ &amp;= \\sum_{x=0}^t P(X=x, Y=t-x)\\\\ &amp;= \\sum_{x=0}^t \\underbrace{f(x,t-x)}_{=f_X(x)f_Y(t-x)}\\\\ &amp; =\\sum_{x=0}^t e^{-\\lambda_1} \\frac{\\lambda_1^x}{x!}e^{-\\lambda_2} \\frac{\\lambda_2^{t-x}}{(t-x)!} \\dots\\end{align*}\\] rest is exercise and in the course notes. Theorem 29.2 (Sum of 2 independent binomial is binomial) If \\(X \\sim Bin(n, p)\\) and \\(Y \\sim Bin(m, p)\\) independently, then \\(T = X + Y \\sim Bin(n + m, p)\\) Proof is left for an exercise. This can be easily extended to the \\(n\\)-case. Theorem 29.3 Let \\(X_1, X_2, \\ldots, X_n\\) each follow \\(Bernoulli(p)\\) independently. Then, \\[ X_1 + X_2 + \\ldots + X_n \\sim Bin(n,p). \\] This should not come as a surprise as it is the meaning of binomial distribution – run \\(n\\) independent Bernoulli trials with the same probability of success \\(p\\). Similar, we may have the same relationship between the Negative binomial distribution and the geometric distribution. Theorem 29.4 (Sum of n independent geometric is Negative binomial) Let \\(X_1, X_2, \\ldots, X_k\\) each follow \\(Geo(p)\\) independently. Then, \\[ X_1 + X_2 + \\ldots + X_k \\sim NB(k,p). \\] Theorem 29.5 (Conditional distribution of two independent poisson) Let \\(X \\sim Poi(\\lambda_1)\\) and \\(Y \\sim Poi(\\lambda_2)\\) independently. Then, given \\(X + Y = n\\), \\(X\\) follows binomial distribution. That is, \\[ X|X+Y = n \\sim Bin\\left(n, \\frac{\\lambda_1}{\\lambda_1 + \\lambda_2}\\right). \\] Similarly, for \\(Y\\), we have \\[ Y|X+Y = n \\sim Bin\\left(n, \\frac{\\lambda_2}{\\lambda_1 + \\lambda_2}\\right). \\] The proof is left as an exercise. Hint is to use \\(X+Y\\sim Poi(\\lambda_1+\\lambda_2)\\). "],["lecture-29-march-20-2024.html", "30 Lecture 29, March 20, 2024 30.1 Multinomial distribution", " 30 Lecture 29, March 20, 2024 In this lecture, we are going to introduce a named multivariate distribution – the multinomial distribution. 30.1 Multinomial distribution The multinomial distribution is a generalization of the binomial distribution. Definition 30.1 (Multinomial distribution) Consider an experiment in which: In this case we say \\(X_1,...,X_k\\) have a Multinomial distribution with parameters \\(n\\) and \\(p_1,...,p_k\\). We use the notation \\((X_1,...,X_k) \\sim Mult(n,p_1,...,p_k)\\). Note: The binomial distribution has only 2 class/categories, i.e. the success/failture, head/tail…with the associated probability \\(p_1\\) and \\(p_2=(1-p_1)\\) Whereas in the multinomial distribution, we have \\(n\\)-different classes/categories, \\(1,2,\\cdots,n\\), with the associated probabilities \\(p_1,p_2,\\cdots,p_n=1-\\sum_{i=1}^{n-1}p_i\\). 30.1.1 The probability distribution function If \\(X_1,...,X_k\\) have a joint multinomial distribution with parameters \\(n\\) and \\(p_1,...,p_k\\), then their joint probability function is \\[ f(x_1,...,x_k) = \\frac{n!}{x_1!x_2!\\cdots x_k!} p_1^{x_1}\\cdots p_k^{x_k}, \\] where \\(x_1,...,x_k\\) satisfy \\(x_1+\\cdots+x_k = n\\), \\(x_i \\ge 0\\). \\ The terms \\[ \\binom{n}{x_1, x_2, \\cdots,x_k} := \\frac{n!}{x_1!x_2!\\cdots x_k!}, \\quad\\text{where } x_1+\\cdots+x_k = n, \\] are called the multinomial coefficients Just like the binomial coefficient where we can use one variable to two outcomes/class/categories, we can write the multinomial with \\(k\\)-class as \\(k-1\\) random variables: Multinomial distribution over \\((X_1, \\ldots, X_k)\\) can also be written in terms of \\(k-1\\) variables. If \\(x_1 + \\ldots + x_n = n\\), and we know \\(x_1, \\ldots x_{k-1}\\), then we can let \\[ x_k = n - x_1 - x_2 - \\ldots - x_{k-1}. \\] Similarly, we can let \\[ p_k = 1 - p_1 - p_2 - \\ldots - p_{k-1}. \\] Thus, we can write the probability function of \\(Mult(n, p_1, \\ldots, p_k)\\) as \\[ f(x_1,...,x_{k-1}) = \\frac{n! p_1^{x_1}\\cdots p_{k-1}^{x_{k-1}} \\left(1 - \\sum_{i=1}^{k-1}p_i\\right)^{n - \\sum_{i=1}^{k-1} x_i}}{x_1!x_2!\\cdots x_{k-1}! (n - \\sum_{i=1}^{k-1} x_i)!} \\] ### Marginal ditribution and associated probability We often wonder what is the marginal distribution of a multivariate joint distribution. So what’s the marginal distribution of the multinomial random variable \\((X_1, X_2, \\ldots, X_k)\\)? ::: {.theorem name=Marginal distribution of multinomial”} Let \\((X_1, X_2, \\ldots, X_k) \\sim Mult(n, p_1, \\ldots, p_k)\\). Then, \\[ X_j \\sim Bin(n, p_j), \\] for \\(j = 1, 2, \\ldots, k\\). ::: This follows either by construction (since \\(X_j\\) counts the number of successes in \\(n\\) independent trials with constant success prob \\(p_j\\), or by computing the sum \\[ P(X_j=x_j)=f_j(x_j)= \\sum\\limits_{\\text{all }x_1,x_2,\\dots,x_{j-1},x_{j+1},\\dots,x_k} f(x_1,\\dots,x_{j-1},x_j,\\x_{j+1},\\dots, x_k)\\] which is illustrated in the case \\(k=3\\) in the course notes. We can also write out the marginal distribution for 2 random variables in a multinomial distribution: Theorem 30.1 (Sum of individual rvs in multinomial) Let \\((X_1, X_2, \\ldots, X_k) \\sim Mult(n, p_1, \\ldots, p_k)\\). Then, \\[ X_i + X_j \\sim Bin(n, p_i + p_j), \\] for \\(i \\neq j\\). Again, the rationale behind it is either by construction (since \\(X_i+X_j\\) counts the number of successes in \\(n\\) independent trials with constant success prob \\(p_i+p_j\\), or by computing the sum \\[ P(X_i + X_j=t)= \\sum\\limits_{\\text{all }x_1,x_2,\\dots,x_k:x_i+x_j=t} f(x_1,\\dots, x_k)\\] ### Conditional distribution of multinomial The other things that we are often interested in, when we have multivariate distribution, is the conditional distribution. For multinomial distribution, we have the following theorem: Theorem 30.2 (Conditional distribution of multinomial) Let \\((X_1, X_2, \\ldots, X_k) \\sim Mult(n, p_1, \\ldots, p_k)\\). Then, \\[ X_i|X_i + X_j = t \\sim Bin\\left(t, \\frac{p_i}{p_i + p_j}\\right), \\] for \\(i \\neq j\\). "],["lecture-30-march-22-2024.html", "31 Lecture 30, March 22, 2024", " 31 Lecture 30, March 22, 2024 In this lecture, we will finish up where we left over – the properties of multinomial distribution. Theorem 31.1 (Conditional distribution of multinomial) Let \\((X_1, X_2, \\ldots, X_k) \\sim Mult(n, p_1, \\ldots, p_k)\\). Then, \\[ X_i|X_i + X_j = t \\sim Bin\\left(t, \\frac{p_i}{p_i + p_j}\\right), \\] for \\(i \\neq j\\). 31.0.1 Summary statistics of multivariate distributions Akin the univaraite distributions, we want to use the moments to summaries the multivariate distributions. First, we define the Mulvariate Law of Unconciouss Statistician (mLOTUS). Definition 31.1 (Bivariate LOTUS) Suppose \\(X\\) and \\(Y\\) are discrete random variables with joint probability function \\(f(x,y)\\). Then for a function \\(g: {\\mathbb R}^2 \\to {\\mathbb R}\\), \\[ \\mbox{E} \\left[g(X,Y) \\right] = \\sum_{(x,y)} g(x,y)f(x,y). \\] Definition 31.2 (Multivariate LOTUS) More generally, if \\(g: {\\mathbb R}^n \\to {\\mathbb R}\\), and \\(X_1,...,X_n\\) are discrete random variables with joint probability function \\(f(x_1,...,x_n)\\), then \\[ \\mbox{E} \\left[g(X_1,...,X_n)\\right] = \\sum_{(x_1,...,x_n)} g(x_1,...,x_n)f(x_1,...,x_n). \\] With mLOTUS, we may define the expectation for multivariate distributions. In addition, the expectation has the linearity property as in the univariate case. \\(\\mbox{E}[ a g_1(X,Y) + b g_2(X,Y) ] = a \\cdot \\mbox{E}[g_1(X,Y)] + b \\cdot \\mbox{E}[g_2(X,Y)].\\) \\(\\mbox{E}[X+Y] = \\mbox{E}[X] + \\mbox{E}[Y]\\) Observation: Regardless of the relationship between the random variables \\(X\\) and \\(Y\\), we HAVE the linearity property! Proofs are left as exercises. Lemma 31.1 (Expectation for trivariate multinomial) Let \\((X_1, X_2, X_3) \\sim Mult(n, p_1, p_2, p_3)\\). Then \\[ E[X_1 X_2] = n(n-1) p_1 p_2. \\] 31.0.2 Relationship betwen the random variables So far, we only discuss about the independence between the ranodm variables. E.g. if \\(X\\) and \\(Y\\) are independent, we have \\(f(x,y) = f_X(x)f_Y(y)\\). However, what if they are not independent? In this case, we say they are dependent. We want to first determine if two random variables are dependent, and then measure how strong the correlation is. To do so, we use the terminology covariance. Definition 31.3 (Covariance) For two random variables \\(X\\) and \\(Y\\), we define \\[ Cov(X,Y) = \\mathbb{E}\\left[ (X - E(X))(Y-E(Y)) \\right]. \\] as the covariance between \\(X\\) and \\(Y\\), provided the expression exists. Similarly to \\(Var(X)=E(X^2)-E(X)^2\\), we have a shortcut formula for the covariance: \\[ Cov(X,Y) = \\mathbb{E}[XY] - \\mathbb{E}[X] \\mathbb{E}[Y]. \\] Actually, we can write the variance as a covariance as follows: \\(\\mathbb{V}ar(X) = cov(X,X) = \\mathbb{E}[(X-\\mathbb{E}X)(Y-\\mathbb{E}Y)] = \\mathbb{E}[XX] - \\mathbb{E}X \\mathbb{E}X\\), which can be simplified as what we have above. Intuition: Why do we look at \\(E \\left[ (X - E(X))(Y-E(Y)) \\right]\\)? Positive correlation: In this case, \\(Cov(X,Y)&gt;0\\) Suppose \\(X,Y\\) are positively related (when \\(X\\) large, \\(Y\\) likely large; when \\(X\\) small, \\(Y\\) likely small) If \\((X-E(X))&gt;0\\) (so \\(X\\) large), then likely \\((Y-E(Y))&gt;0\\) (so \\(Y\\) also large), hence the product \\((X-E(X))(Y-E(Y))&gt;0\\) If conversely \\((X-E(X))&lt;0\\) (so \\(X\\) small), then likely \\((Y-E(Y))&lt;0\\) (so \\(Y\\) also small), hence the product is likely \\((X-E(X))(Y-E(Y))&gt;0\\) Negative correlation: Conversely, suppose \\(X,Y\\) are negatively related (when \\(X\\) large, \\(Y\\) likely small; when \\(X\\) small, \\(Y\\) likely large). Then \\(Cov(X,Y)&lt;0\\). 31.0.3 Relationship between independence and covariance One may wonder that does indpendence and covariance relate, if so, in what way? Theorem 31.2 If \\(X\\) and \\(Y\\) are independent, then \\(Cov(X,Y)=0\\). NOTE: The converse statement is FALSE!!! That is, if \\(Cov(X,Y)=0\\) then \\(X\\) and \\(Y\\) are not necessarily independent. For instance, let \\(X\\sim U(-1,1)\\), and let \\(Y =X^2\\). Then \\(Cov(X,Y)=0\\) but \\(X\\) and \\(Y\\) are not independent. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
