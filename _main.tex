% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{mathtools}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Stat 230 Introduction to Probability},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Stat 230 Introduction to Probability}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Winter 2024}
\author{Chi-Kuang Yeh\\
University of Waterloo}
\date{2024-03-28}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Information of the course}\label{information-of-the-course}

The purpose of this page is to hold some of the additional materials provided by myself. Students should consult UW \href{https://api-4ccc589b.duosecurity.com/frame/v4/preauth/healthcheck?sid=frameless-c0657e9d-cb86-4ac9-a6a7-fd054ae21fd5}{Learn} system.

\section{Course description}\label{course-description}

This course provides an introduction to probability models including sample spaces, mutually exclusive and independent events, conditional probability and Bayes' Theorem. The named distributions (Discrete Uniform, Hypergeometric, Binomial, Negative Binomial, Geometric, Poisson, Continuous Uniform, Exponential, Normal (Gaussian), and Multinomial) are used to model real phenomena. Discrete and continuous univariate random variables and their distributions are discussed. Joint probability functions, marginal probability functions, and conditional probability functions of two or more discrete random variables and functions of random variables are also discussed. Students learn how to calculate and interpret means, variances and covariances particularly for the named distributions. The Central Limit Theorem is used to approximate probabilities.

\subsection{Instructor}\label{instructor}

Chi-Kuang Yeh, I am a postdoc at the \emph{Department of Statistics and Actuarial Science}.

\begin{itemize}
\tightlist
\item
  Office: M3--3102 Desk 10, but I hold office hour at M3 - 2101 Desk 1, 9:30 -- 10:30 on Tuesday.
\item
  Email: \href{mailto:chi-kuang.yeh@uwaterloo.ca}{\nolinkurl{chi-kuang.yeh@uwaterloo.ca}}
\end{itemize}

\subsection{Course Coordinator}\label{course-coordinator}

Dr.~\href{https://uwaterloo.ca/scholar/ehintz}{Erik Hintz}.

\begin{itemize}
\tightlist
\item
  Office: M3--2106
\item
  Email: \href{mailto:erik.hintz@uwaterloo.ca}{erik.hintz@uuwaterloo.ca}
\end{itemize}

\subsection{Logistic Issue}\label{logistic-issue}

Contact Divya Lala

\begin{itemize}
\tightlist
\item
  Email: \href{mailto:divya.lala@uwaterloo.ca}{\nolinkurl{divya.lala@uwaterloo.ca}} or the undergrad advising email \href{mailto:sasugradadv@uwaterloo.ca}{\nolinkurl{sasugradadv@uwaterloo.ca}}.
\end{itemize}

\subsection{EXAM and Tutorial assessment Date}\label{exam-and-tutorial-assessment-date}

Midterm

\begin{itemize}
\tightlist
\item[$\boxtimes$]
  Midterm 1: February 08, 2024 16:30--17:50 (Coverage: Ch. 1 -- 5.1)
\item[$\boxtimes$]
  Midterm 2: March 14, 2024 16:30--17:50 (Coverage: Ch. 1--5, 7-8, up to Sec. 8.3)
\end{itemize}

Final

\begin{itemize}
\tightlist
\item[$\square$]
  Tuesday April 16, 2024 19:30 -- 22:00. Location: DC 1350 and DC 1351
\end{itemize}

Tutorial assessment

\begin{itemize}
\tightlist
\item[$\boxtimes$]
  Tutorial quiz 1: January 26, 2024 (Coverage: Ch. 1--3)
\item[$\boxtimes$]
  Tutorial test 1: February 02, 2024 (Coverage: Ch. 1--4)
\item[$\boxtimes$]
  Tutorial quiz 2: March 01, 2024 (Coverage: Ch. 1-4, and Ch. 7, up to Sec. 7.3)
\item[$\boxtimes$]
  Tutorial test 2: March 08, 2024 (Coverage: Ch. 1-5, 7-8 up to Sec. 8.1)
\item[$\boxtimes$]
  Tutorial quiz 3: March 22, 2024 (Coverage: Up to Sec. 9.1, exclude the independence)
\item[$\square$]
  Tutorial test 3: April 05, 2024
\end{itemize}

\section{Chapters and associated Lectures}\label{chapters-and-associated-lectures}

Those chapters are based on the lecture notes. The lecture covered is based on \emph{Section 002}. This part will be updated frequently.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Chapter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Lecture Covered
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1. Introduction to Probability & L1 \\
2. Mathematical Probability Models & L2--3 \\
3. Probability and Counting Techniques & L3--6 \\
4. Probability rules and Conditional Probability & 6--9 \\
5. Discrete Random Variable & L10 --16 \\
6. Computational Methods and the Statistical Software R & In tutorial (not testable) \\
7. Expected Value and Variance & L16 --20 \\
8. Continuous Random Variable & L20 -- 27 \\
9. Multivariate Distributions & L27 -- \\
10. TBA & TBA \\
\end{longtable}

\chapter{Lecture 1, January 08, 2024}\label{lecture-1-january-08-2024}

In this lecture, we went over

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Course syllabus and rules
\item
  Chapter 1 -- Basic definition of probability. We also saw the potential ambiguities when defining probabilities.
\end{enumerate}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{definition}[Classical Definition of probability]
The \textbf{classical} definition: The probability of some event is
\[
\frac{\mathrm{number~of~ways~the~event~can~occur~}}
{\mathrm{{the~total~number~of~possible~outcomes}}},
\]
provided all outcomes are \emph{equally likely}.
\end{definition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{definition}[Relative Frequency Definition of of probability]
The \textbf{relative frequency} definition: The probability of an event
is the (limiting) proportion (or fraction) of times the event occurs in a very
long series of repetitions of an experiment.
\end{definition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{definition}[Subjective Definition of Probability]
The \textbf{subjective} definition: The probability of an event is a measure of how sure the person making the statement is that the event will happen.
\end{definition}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Problem}: Each of the above definitions has pitfall:

\begin{itemize}
\tightlist
\item
  Classical: We may not be able to know the total number of possible outcomes, or it may be uncountable
\item
  Relative frequency: We need ``repetition'', which is often expensive and may not be possible.
\item
  Subjective: We want the probability to be consistent across different people, and and be rigorously defined.
\end{itemize}

\chapter{Lecture 2, January 10, 2024}\label{lecture-2-january-10-2024}

In this lecture, we went over some basic definitions from the set theory, and using them as the building block for the rest of the course. We started Chapter 2 today, with many definitions.

As for the set operations, \(\cup,\cap,A^c,...\), the Venn diagrams help to visual the meaning behind. Here is a good reference \href{https://www.edrawmax.com/article/venn-diagram-symbols-and-set-notations.html}{HERE}.

\begin{definition}[sample space]
A \textbf{sample space} \(S\) is a \emph{set} of distinct outcomes of an experiment with the property that in a single trial of the experiment only one of these outcomes occurs.
\end{definition}

\begin{definition}[Discrete and non-discrete sample space]
A sample space \(S\) is said to be \textbf{discrete} if it is finite, or ``countably infinite'' (i.e.,there is a one-to-one correspondence with the natural numbers). Otherwise a sample space is said to be \textbf{non-discrete}.
\end{definition}

\begin{definition}[Event]
An \textbf{event} is a subset of the sample space that can be assigned probability.
\end{definition}

\begin{definition}[Simple and Compound event]
Let \(S\) be discrete and \(A\subset S\) an event. If \(A\) is indivisible so it contains only one point, we call it a \textbf{simple event}, otherwise \textbf{compound event}.
\end{definition}

\begin{definition}[Probability distribution]
Let \(S=\{a_1,a_2,\dots\}\) be discrete. Assign numbers \(P(\{a_i\})\) (or short: \(P(a_i)\)), \(i=1,2,\dots\), so that

1.\(0\leq P(a_i)\leq 1,\quad i=1,2,\dots\)
2. \(\sum_{\text{all }i}P(a_i)=1\).

We then call the set of probabilities \(\{P(a_i):i=1,2,\dots\}\) a \textbf{probability distribution}.
\end{definition}

\begin{definition}
Let \(S=\{a_1,a_2,\dots\}\) discrete. From any prob. distribution \(P\) on \(S\) we can define a prob. measure on \$ \{\mathcal S\} = 2\^{}S\$ (set of all subsets of \(S\)) by
\[\forall A \subseteq S \qquad P(A)=\sum_{a_i\in A}P(a_i).\]
\end{definition}

\begin{definition}[Equally likely]
We say a sample space \(S\) with a finite number of outcomes is \textbf{equally likely} if the probability of every individual outcome in \(S\) is the same.
\end{definition}

Observe that

\begin{itemize}
\item
  If \(|A|\) denote the number of outcomes in an event \(A\). In case of an equally likely sample space,
  \[
  1=P(S)=\sum_{i=1}^{|S|}P(a_{i})= P(a_i)|S|.
  \]
  \[
  P(a_i)=\frac{1}{|S|}.
  \]
\item
  Hence,
  \[P(A) = \sum_{i:\;a_i \in A} P(a_i) = \sum_{i:\;a_i \in A} \frac{1}{|S|} =|A|\cdot  \frac{1}{|S|}\]
\end{itemize}

\textbf{Conclusion}: In a \textbf{finite, equally likely sample space}, the probability of an event \(A\) can be computed as
\[
P(A) = \sum_{i:\;a_i \in A} P(a_i) = \frac{|A|}{|S|}.
\]

\section{Questions from the class}\label{questions-from-the-class}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  What is the difference between ``countably infinite'' v.s. ``infinite''?
\end{enumerate}

Ans: A set is \emph{countably infinite} if its elements can be put in one-to-one correspondence with the set of natural numbers \(\mathbb{N}\). Alternatively, you can think that a set is countably infinite if you can count off all elements in the set in such a way that, even though the counting will take forever, you will get to any particular element in a finite amount of time. {[}\href{https://mathinsight.org/definition/countably_infinite}{A good reference page to read}{]}. If a set is not countable or countably infinite, it is infinite.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Why do we have something such as \(2^\mathcal{S}\) in the lecture?
\end{enumerate}

Ans: It is related to something called the \emph{power set}. The power set consists all the possible subset of a set \(\mathcal{S}\). In a subset of \(S\), (i.e.~\(A \subseteq \mathcal{S}\), every element in \(\mathcal{S}\) may be either in \(A\) or not in \(A\). Which means, each element has two possibilities, in \(A\) or not in \(A\). Hence, the cardinality (i.e.~the size) of the power set is \(2^\mathcal{S}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  What did we mean by ``order does not matter'' and ``order matters'' during the lecture.
\end{enumerate}

\begin{itemize}
\item
  Order does not matter: I said when you write out the element of a set, the order does not matter. For instance, in the rolling a six-sided dice, which side would be faced on the top example, we can write \(\mathcal{S} = \{1,2,3,4,5,6\}\), or \(\mathcal{S}^\prime=\{6,5,4,3,2,1\}\), and those two sets are essentially equal to each other. To represent a set, the order does not matter, but we tend to write in a way that is intuitive and easy to understand.
\item
  Order does matter: In rolling two dices example, the dots show on each of the dice is an \emph{ordered pair}, denoted by \((x,y)\). Hence, for instance, \((1,2)\) and \((2,1)\) are different. It is problem-dependent so be careful.
\end{itemize}

\chapter{Lecture 3, January 12, 2024}\label{lecture-3-january-12-2024}

\begin{definition}[Odds]
Odds \textbf{in favour} of an event \(A\) occurring is
\[
  O(A) := \frac{P(A)}{1-P(A)}.
\]
Odds again an event \(A\) is
\[
  \frac{1-P(A)}{P(A)}.
\]
\end{definition}

The range of the odds is \([0,\infty)\).

It provide a measure of the likelihood of a particular outcome to happen.

Abbreviation: ``\,``p:q''.

Note: Probability may be defined through the odds as follow.
\begin{align*}
  &O(A) := \frac{P(A)}{1-P(A)} \\
  &\implies O(A) - P(A)O(A) = P(A) \\
  &\implies O(A) = P(A) (1+O(A)) \\
  &\implies P(A) = \frac{O(A)} {1+O(A)}
\end{align*}

Note: In finite, equally likely sample spaces, computing probabilities amounts to \emph{counting the number of elements in a set}. It will often be difficult to do this manually, so we are looking for clever \emph{counting techniques} in the next chapter.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{Chapter 3 Counting Techniques}

Addition rule v.s. Multiplication rule

For addition rule

\begin{itemize}
\tightlist
\item
  Keyword for addition rule is ``\textbf{OR}'';
\item
  \(|A|\) is defined to be the size of the set, aka the cardinality of the set.
\item
  If \(A\) and \(B\) are \emph{disjoint} (i.e.~\(A\cap B = \emptyset\)), then \(|A\cup B| = |A| + |B|\).
\item
  \(A\cup A^c = S\) where \(A \cap A^c = \emptyset\). Thus \(|S|=|A|+|A^c|\).
\end{itemize}

For multiplication rule

\begin{itemize}
\tightlist
\item
  for multiplication rule is ``\textbf{AND}''
\item
  An ordered k-tuple is an ordered set of \(k\) values: \((a_1,a_2,\dots,a_k)\). If the outcomes in A can be wrttien as an ordered k-tuple where there are \(n_1\) choices for \(a_1\), \(n_2\) choices for \(a_2,\dots\) and in general \(n_i\) choices for \(a_i\), then
  \[
  |A| = n_1n_2\cdots n_k = \prod_{i=1}^k n_i.
    \]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{definition}[Factorial]
Given \(n\) distinct objects, there are
\[
n! = n \times (n-1) \times \ldots 2 \times 1,
\]
different ordered arrangements of length \(n\) that can be made. Note that, we define, \(0! = 1\).
\end{definition}

\begin{itemize}
\tightlist
\item
  We pronounce \(n!\) as ``n factorial''.
\item
  The following recursive definition is useful:
  \[ 
  n! = n \cdot (n-1)!
  \]

  \item

  When working with factorials, we can often cancel terms, e.g.,
  \[ \frac{9!}{7!} = \frac{9\cdot 8 \cdot 7\cdot 6 \cdot \dots \cdot 2 \cdot 1}{7\cdot 6 \cdot \dots \cdot 2 \cdot 1}=9\cdot 8 = 72\]
\end{itemize}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{definition}[Permutation]
Given \(n\) distinct objects, a \textbf{permutation} of size \(k\) is an \(ordered\) subset of \(k\) of the individuals. The number of permutations of size \(k\) taken from \(n\) objects is denoted \(n^{(k)}\) and
\[
n^{(k)}=n(n-1)\dots (n-k+1) =\frac{n!}{(n-k)!}.
\]
\end{definition}

The tricky part of this definition is the word ``ordered''. An ordering need not be numerical, for example assigning labels like ``President'' and ``Vice-President'' has the effect of ordering the individuals.

\section{Questions from the class}\label{questions-from-the-class-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Can we express the odds as \(a:b\)?
\end{enumerate}

YES. For instance, the example we saw in class (or the clicker question 1), if we roll a fair six sided dice, and let our event \(A:=\{\text{# is } 5 \}\). Then the odds \(O(A)=\frac{1/6}{1/5}=\frac{1}{5}\). We can see that, there is exactly one possibility we have event \(A\), whereas there are 5 possibilities that \(A\) does not happen (i.e.~the number we roll out is \(1,2,3,4,6\)). We \textbf{can} abbreviate it as ``1:5''. For a good example of Odds, \href{https://en.wikipedia.org/wiki/Odds}{WIKI} provides a good one.

\chapter{Lecture 4, January 15, 2024}\label{lecture-4-january-15-2024}

\begin{definition}[Combination]
Given \(n\) distinct objects, a \emph{combination} of size \(k\) is an \emph{unordered} subset of \(k\) of the individuals. The number of combinations of size \(k\) taken from \(n\) objects is denoted \({n \choose k}\) or \({}_n C_k\) and can be computed as
\[
{n \choose k}=\frac{n^{(k)}}{k!}=\frac{n!}{(n-k)!\ k!}.
\]
\end{definition}

\chapter{Lecture 5, January 17, 2024}\label{lecture-5-january-17-2024}

Properties of the Binomial coefficients

There are some useful/important results about permutation and combination.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(n^{(k)} = n (n - 1)^{(k-1)}\) for \(k \geq 1\)
\item
  \({n \choose k} = \frac{n^{(k)}}{k!}\)
\item
  \({n \choose k} = {n \choose n-k}\) for \(k \geq 0\)
\item
  \({n \choose k} = {n-1 \choose k-1} + {n-1 \choose k}\)
\item
  Binomial theorem: \((1 + x)^n = \sum_{k=0}^n {n \choose k} x^k\)
\item
  \({n \choose k}\) is equal to the \(k\)th entry in the \(n\)th row of \textbf{Pascal's triangle}.
\end{enumerate}

Note: Many of these idenetity may be proven using something called \emph{combinatorial proof}. See \href{https://en.wikipedia.org/wiki/Combinatorial_proof}{Wiki} for an (easy) example.

\begin{proof}[4]
\begin{align*}
{n-1 \choose k-1} + {n-1 \choose k} &= \frac{(n-1)!}{(k-1)! (n-k)!} + \frac{(n-1)!}{k! (n-k-1)!}\\
&= \frac{(n-1)!k }{(k-1)! (n-k)!k} + \frac{(n-1)!(n-k)}{k! (n-k-1)!(n-1)} \\
&= \frac{(n-1)!k + (n-1)! (n-k)}{k! (n-k)!} \\
&- \frac{(n-1)!( k + (n-k))}{k! (n-k)!} \\
&= \frac{(n-1)! n}{k! (n-k)!} \\
&= {n \choose k}
\end{align*}
\end{proof}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Aside: Stirling's formula

\(n!\) grows really fast as \(n\) increases, so sometimes we need to approximate its value for computational reasons.

\textbf{Stirling's formula} provides one such method, and it is given by

\[
n! \sim \sqrt{2 \pi n} \left( \frac{n}{e} \right)^n,
\]
where \(\sim\) means their ratio approaches 1 as \(n\) goes to infinity.

We won't need this approximation, but it's useful to know it exists.

Example of use:
Show that \(2^{-2n}\binom{2n}{n} \approx \sqrt{\frac{2}{\pi n}}\)

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\section{Example in class}\label{example-in-class}

\begin{example}[Application of Stirling's Formula/Approximation for factorial]
Show that \(2^{-2n}{2n \choose n} \approx \sqrt{\frac{1}{\pi n}}\)

\begin{align*}
  2^{-2n}{2n \choose n} &= 2^{-2n}\frac{2n!}{n!n!} \\
  &\approx 2^{-2n} \frac{\sqrt{2\pi (2n)} (2n/e)^{2n}}{\sqrt{2\pi n} (n/e)^{n}\sqrt{2\pi n} (n/e)^{n}}\\
  &= 2^{-2n} \frac{\sqrt{4}}{\sqrt{2}\sqrt{2}} \frac{\sqrt{\pi n}}{\sqrt{\pi n}\sqrt{\pi n}} \frac{(2n)^{2n}}{n^n n^n} \frac{e^{-2n}}{e^{-2n}}\\
  &=  2^{-2n} \frac{1}{\sqrt{\pi n}} 2^{2n}\\
  &= \frac{1}{\sqrt{\pi n}} = \sqrt{\frac{1}{\pi n}}
\end{align*}
\end{example}

\chapter{Lecture 6, January 19, 2024}\label{lecture-6-january-19-2024}

\subsection{Multinomial Coefficient}\label{multinomial-coefficient}

\begin{definition}[Multinomial Coefficient]
Consider \(n\) objects which consist of \(k\) types. Suppose that there are \(n_1\) objects which are of type 1, \(n_2\) which are of type 2, and in general \(n_i\) objects of type \(i\). Then there are
\[
\frac{n!}{n_1 ! n_2! \dots n_k !}
\]
distinguishable arrangements of the \(n\) objects. This quantity is known as a \textbf{multinomial coefficient} and denoted by
\[
\binom{n}{n_1,n_2,\dots,n_k}= \frac{n !}{n_1 ! n_2! \dots n_k !}.
\]
\end{definition}

\textbf{Note}: Multinomial coefficient is an extension of the \emph{binomial coefficient}. In binomial coefficient, there are only \textbf{two groups/objects}, and the first type has size \(n_1\) and the size of the second type is consequently \(n-n_1\), where \(n\) is the total number of objects. Hence we have \({n \choose n_1} = \frac{n!}{n_1! (n-n_1)!}\). Try to compare this with the multinomial coefficient.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{The Birthday Problem}\label{the-birthday-problem}

Suppose a room contains \(n\) people. What is the probability at least two people in the room share a birthday?

\textbf{Assumption}: Suppose that each of the \(n\) people is equally likely to have any of the 365 days of the year as their birthday, so that all possible combinations of birthdays are equally likely.

Let \(A\) be the event that at least two people share a birthday. Then
\[ P(A) = 1 - P(A^c),\]
where \(A^c\) is the event that nobody shares birthday with each other.

For \(n\) people to have unique birthdays, we need to arrange them among 365 days w/o replacement. Thus,
\[|A^c| = 365^{(n)}.\]

For the size of the sample space, we see that each person has 365 possibilities for their birthday. Thus,
\[|S| = 365^n.\]

Since we are assuming that all possible combinations of birthdays are equally likely, our desired probability becomes
\[
P(A) = 1 - P(A^c) = 1 - \frac{365^{(n)}}{365^n} = 1 - \frac{n! {365 \choose n}}{365^n}.
\]

For \(n\in\{100, 30, 23\}\) we find
\[P(A_{100})= .9999997,\;\;\; P(A_{30})=.7063 \;\;\;\; P(A_{23})=.5073.\]

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Chapter 4 Probbility Rules and Conditional Probability}\label{chapter-4-probbility-rules-and-conditional-probability}

Review the Venn Diagram

\chapter{Lecture 7, January 22, 2024}\label{lecture-7-january-22-2024}

\subsection{Some terminology about the set thoery.}\label{some-terminology-about-the-set-thoery.}

\subsubsection{Fundamental law of set algebra}\label{fundamental-law-of-set-algebra}

Let \(A\) and \(B\) be any arbitrary sets/events.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Commutative
\end{enumerate}

\[
  A\cup B = B\cup A \quad \text{ and } A\cap B = B\cap A.
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Associativity
\end{enumerate}

\[
  (A\cup B)\cup C = A \cup (B\cup C), \quad \text{and } (A\cap B)\cap C =  A \cap (B \cap C).
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Distributive Law
\end{enumerate}

\[
  A\cup (B\cap C) = (A \cup B) \cap (A \cap  C) , \quad \text{ and } A \cap (B\cup C) =  (A\cap B) \cup (A\cap C)
\]

\subsubsection{DeMorgan's Laws}\label{demorgans-laws}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \((A\cup B)^c = A^c \cap B^c\) (Complement of an union is the intersection of the complements)
\item
  \(A\cap B)^c = A^c \cup B^c\) (Complement of an intersection is the union of the complements)
\end{enumerate}

\subsubsection{Inclusion Exclusion Principle}\label{inclusion-exclusion-principle}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(P(A\cup B ) = P(A) + P(B) - P(A\cap B)\)
\item
  \(P(A\cup B \cup C)  = P(A) + P(B) + P(C) - P(A\cap B) - P(A \cap C) - P(B \cap C) + P(A\cap B \ cap C)\)
\item
  Note that we can generalized the (2), and obtain the following by \emph{inducation.} For arbitrary events \(A_1,A_2,\cdots,A_n,\quad n\ge 2\),
  \begin{align*}
  P(\bigcup_{i=1}^n A_i)  &  =\sum_{i}P(A_{i}%
  )-\sum_{i<j}P(A_{i}A_{j})+\sum_{i<j<k}P(A_{i}A_{j}A_{k})\\
  &  -\sum_{i<j<k<l}P(A_{i}A_{j}A_{k}A_{l})+\cdots
  \end{align*}
\end{enumerate}

\subsection{Independence}\label{independence}

\begin{definition}[independence]
Any two events \(A\) and \(B\) are said to be \textbf{independent} if
\[
  P(A \cap B) = P(A)\times P(B).
\]
\end{definition}

Note: Intuitively, it means that two events do not have any influence of each other. You will see that how this concept plays an important role in statistics, in particular through something called the \emph{covariance}, which is beyond this course so do not worry about this for now.

\subsection{Independence v.s. Multually Exclusive/Disjoint}\label{independence-v.s.-multually-exclusivedisjoint}

Recall the definition of mutually exclusive

\begin{definition}[Mutually Exclusive]
Any two events \(A\) and \(B\) are said to be \textbf{mutually exclusive} or \textbf{disjoint} if
\[
  P(A \cap B) = 0.
\]
\end{definition}

Note:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  If \(A\) and \(B\) are mutually exclusive, \(A\) and \(B\) may NOT be independent!
\item
  \(A\) and \(B\) CAN only be mutually exclusive and independent when either \(A\), \(A\), or both are the empty set \(\emptyset\).
\end{enumerate}

\begin{lemma}
Let two events \(A\) and \(B\) such that NOT both events are trivial events (empty set). If \(A\) and \(B\) are independent and mutually exclusive/disjoint, then either \(P(A) = 0\) or \(P(B) = 0\).
\end{lemma}

\chapter{Lecture 8, January 24, 2024}\label{lecture-8-january-24-2024}

\begin{definition}[Conditional Probability]
The conditional probability of an event \(A\) given an event \(B\), assuming \(P(B)>0\), is

\[
  P(A \mid B) = \frac{P(A\cap B)}{P(B)}.
\]
\end{definition}

\begin{definition}[Equivalent definition of independence]
Two events \(A\) and \(B\) are independent, if
\[
P(A|B)=P(A),
\]
provided \(P(B)>0\).
\end{definition}

\subsection{Properties of Conditional Probability}\label{properties-of-conditional-probability}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(0 \le P(A \mid B) \le 1\).
\end{enumerate}

This follows from the fact that if \(A \subset B\) then \(P(A) \le P(B)\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \(P(A^c \mid B) = 1-P(A \mid B)\).
\item
  If \(A_1\) and \(A_2\) are disjoint (i.e.~\(P(A_1\cap A_2)=\emptyset\): \(P(A_1 \cup A_2  \mid B) = P(A_1 \mid B) + P(A_2 \mid B)\).
\item
  \(P(S \mid B)= 1 = P(B \mid B)\).
\end{enumerate}

\begin{definition}[Product rule]
For any events \(A\) and \(B\), we have
\[
P(A\cap B) = P(A\mid B) P(B)  = P(B \mid A) P(A).
\]
\end{definition}

\chapter{Lecture 9, January 26, 2024}\label{lecture-9-january-26-2024}

\subsection{Law of Total Probability}\label{law-of-total-probability}

\begin{definition}[Partition]
A sequence of sets \(B_1,B_2,...,B_k\) are said to \textbf{partition} the sample space \(S\) if \(B_i \cap B_j = \emptyset\) for all \(i \ne j\), and \(\cup_{j=1}^k B_j = S\).
\end{definition}

\begin{theorem}[Law of Total Probability]
Suppose that \(B_1,B_2,...,B_k\) partition \(S\). Then for any event \(A\),
\[
P(A) = P(A | B_1) P(B_1) + P(A | B_2) P(B_2) + \cdots +P(A | B_k) P(B_k).
\]
\end{theorem}

Note: It is a simple usage of LTP such that
\[
  P(A) = P(A\cap B) + P(A \cap B^c),
\]
since \(B\) and \(B^c\) partition \(S\) (i.e.~\(B\cup B^c = S\) and \(B\cap B^c = \emptyset\))

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Bayes Rule}\label{bayes-rule}

If we can calculate the conditional profitability, then we may calculate the desire probability by using 1) LTP, 2) definition of conditional probability, and 3) property of the sets (through the Venn diagrams). However, sometimes we have to \emph{FLIP} the event and the conditioning event. This brings us to the \textbf{Bayes Theorem}.

\begin{theorem}[Bayes Theorem]
Suppose that \(B_1,B_2,...,B_k\) partition \(S\). Then for any event \(A\),
\[
P(B_i \mid A ) =  \frac{P(A \mid B_i)P(B_i)}{\sum_{j=1}^k P(A \mid B_j) P(B_j) }.
\]
\end{theorem}

This concludes Chapter 4!.

\chapter{Lecture 10, January 29, 2024}\label{lecture-10-january-29-2024}

We begin \textbf{Chapter 5} in this lecture!

\subsection{Chapter 5. Discrete Random Variable}\label{chapter-5.-discrete-random-variable}

\begin{definition}[Random Variable]
A \emph{random variable} is a function that maps assigns a real number \(\mathbb{R}\) to each point in a sample space \(S\). That is, \(X\) is a random variable if
\[X:S\to \mathbb{R}\].
\end{definition}

\begin{definition}[Range]
The values that a random variables takes is called the \emph{range} of the random variable. We often denote the range of a random variable \(X\) by \(X(S)\).
\end{definition}

\begin{definition}[Discrete random variable]
The \emph{discrete} random variables take integer values, or more generally, values in a countable set (i.e.~finite or countably infinite set). That is, its range is a discrete/countable subset of \(\mathbb{R}\).
\end{definition}

\begin{definition}[Continuous random variable]
A random variable is \emph{continuous} if its range is an interval that is a subset of \({\mathbb R}\) (e.g.~\${[}0,1{]}, (0,\infty), \{\mathbb R\} \$).
\end{definition}

\begin{definition}[Probability (mass) function]
The \emph{probability (mass) function} of a \emph{discrete} random variable \(X\) is the function
\[
f_X(x) = P(X=x),\quad \text{ for } x\in \mathbb{R},
\]
which is non-zero at at most countably many values.
\end{definition}

Notation: We write \(P(X=x)\) as the shorthanded notation for \(P(\{\omega \in S : X(\omega)=x\})\).

Notation: We can write \(f_X(x)=P(X^{-1}(x))=(P\circ X^{-1})(x)\). We call this as \emph{push-forward probability measure}.

Note: The definition \(f_X\) is valid for all \(x\), but the value is zero when \(x\) is outside the range of the random variable \(X\). (This is called the null set).

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

Properties of probability mass function \(f\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(f_X(x)\in[0,1]\) for all \(x\), and
\item
  \(\sum_{x\in X(\omega)}f_X(x) =1\). i.e.~sum of the probability on ALL the events equal to \(1\).
\end{enumerate}

\chapter{Lecture 11, January 31, 2024}\label{lecture-11-january-31-2024}

\section{Distinction of the definition ``discrete'' of the sample space and the random variable}\label{distinction-of-the-definition-discrete-of-the-sample-space-and-the-random-variable}

Recall the definition of the \emph{Range of a random variable} from last lecture:
::: \{.definition name=``Range''\}
The values that a random variables takes is called the \emph{range} of the random variable. We often denote the range of a random variable \(X\) by \(X(S)\).
:::

\begin{itemize}
\item
  We say that, a \textbf{random variable} is discrete if its range \(X(\omega)\) is discrete (finite or countable, in another word, we can say it is \emph{at most countable}).
\item
  We say, a sample space \(S\) is discrete if \(S\) is finite or countable.
\end{itemize}

The sample space \(S\) and the range of the random variable are two different things, so do not get confused! We can have a discrete random variable while the sample space \(S\) is continuous!

\section{Cumulative distribution function}\label{cumulative-distribution-function}

\begin{definition}[Range]
The \textbf{cumulative distribution function} (cdf) of a random variable \(X\) is
\[
F_X(x) = P(X \le x),\;\; x \in {\mathbb{R}}.
\]
\end{definition}

Note: The cumulative distribution function \(F_X\) is \textbf{always defined} over the entire real line \(\mathbb{R}\), while the probability function may not always be defined! Hence, the cumulative distribution function is an useful tool! (but do not worry about it now.)

Notation: \(F_X(x) = P(X\le x) = P(\{\omega \in S : X(\omega)\in x\}).\)

If \(X\) is \emph{discrete with probability function \(f_X\) (i.e.~if \(f_X\) exists)}, then we can calculate the cdf from summing up the pdf as
\[
F_X(x)=P( X \le x ) = \sum_{y:\; y\le x}f_X(y).
\]

\subsection{Properties of the cumulative distribution function}\label{properties-of-the-cumulative-distribution-function}

Let \(F_X(\cdot)\) be a cdf. Then the following holds

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(F_X(x)\in [0,1]\)
\item
  \(F_X(x) \le F_x(y)\) whenever \(x<y\) (i.e.~\(F_X(\cdot)\) is a non-increasing function.)
\item
  \(\lim\limits_{x \to - \infty } F_X(x)=0\), and \(\lim\limits_{x \to \infty } F_X(x)  = 1\).
\item
  \(F_X\) is \emph{right continuous}, i.e., \(F(x_0)=\lim_{x\downarrow x_0} F(x)\) for all \(x_0\in\mathbb{R}\).
\end{enumerate}

\section{Special distributions with names}\label{special-distributions-with-names}

\subsection{Discrete uniform distribution}\label{discrete-uniform-distribution}

The first named distribution we look at is the \emph{discrete uniform distribution}

\begin{definition}[Discrete uniform distribution]
uppose the range of the random variable \(X\) is \(\{a,a+1,\dots, b\}\), where \(a,b\in\mathbb{Z}\), and suppose all values are equally likely. Then we say that \(X\) has a \textbf{discrete uniform distribution} on \(\{a,a+1,\dots,b\}\), shorthand: \(X \sim U[a,b]\).
\end{definition}

\subsubsection{Probability function and distribution function}\label{probability-function-and-distribution-function}

If \(X \sim U[a,b]\), then its probability function is given by
\[
   f_X(x)= P(X = x) = \begin{cases} \frac{1}{b - a + 1}, &\quad\text{ if }x \in\{a,a+1,\dots,b\}, \\ 0, &\quad\text{ otherwise} \end{cases}   \]
and corresponding (cumulative) distribution function is
\[
   F_X(x)= P(X \leq x) = \begin{cases} 0, &\quad\text{ if } x<a\\
   \frac{\lfloor x\rfloor - a + 1}{b - a + 1}, &\quad\text{ if }x \in\{a,a+1,\dots,b\}, \\ 
   1, &\quad\text{ if } x\geq b,\end{cases}   \]
where \(\lfloor x\rfloor=\max\{z\in\mathbb{Z}: z\leq x\}\) is the \emph{floor function}.

\chapter{Lecture 12, Feburary 02, 2024}\label{lecture-12-feburary-02-2024}

\section{Hypergeometric distribution}\label{hypergeometric-distribution}

\begin{definition}[Hypergeometric distribution]
Consider a population that consists of \(N\) objects, of which \(r\) are considered \emph{successes} and the remaining \(N-r\) are considered \emph{failures}. Suppose that a subset of size \(n\) (with \(n\leq N\)) is drawn from the population WITHOUT REPLACEMENT. Let \(X\)=Number of successes obtained, then we say \(X\) follows a \textbf{hypergeometric distribution} with parameters \((N,r,n)\). We sometimes write \(X \sim hyp(N,r,n)\) or \(X\sim HG(N,r,n)\).
\end{definition}

\subsection{Range of the Hypergeomnetric distribution}\label{range-of-the-hypergeomnetric-distribution}

\begin{itemize}
\tightlist
\item
  We cannot have more successes than there are (\(r\)) \(\Rightarrow\) \(x\leq r\)
\item
  We cannot have more successes than trials (\(n\)) \(\Rightarrow\) \(x\leq n\)
\item
  When there are more trials than failures (\(n>(N-r)\)) we will for sure have at least \(n-(N-r)\) successes \(\Rightarrow\) \(x\geq \max\{0, n-(N-r)\}\).
\item
  Overall, we have \textbf{\(\max\{0, n-(N-r)\} \leq x \leq \min\{r,n\}\)}.
\end{itemize}

\subsection{Probability function of hypergeometric distribution}\label{probability-function-of-hypergeometric-distribution}

\begin{itemize}
\tightlist
\item
  Total number of of subsets of size \(n\): \(\binom{N}{n}\).
\item
  Number of ways to select \(x\) successes out of \(r\) successes: \(\binom{r}{x}\).
\item
  Number of ways to choose remaining \(n-x\) failures from \(N-r\) failures: \(\binom{N-r}{n-x}\).
\item
  Thus,
  \[f(x) = \frac{\binom{r}{x}\binom{N-r}{n-x}}{\binom{N}{n}},\]
  where \(\max\{0, n-(N-r)\} \leq x \leq \min\{r,n\}\) and 0 otherwise.
\end{itemize}

\section{Bernoulli and Binimial distributions}\label{bernoulli-and-binimial-distributions}

\begin{definition}[Bernoulli trail]
A \textbf{Bernoulli trial} with probability of success \(p\) is an experiment that results in either a success or failure, and the probability of success is \(p\).
\end{definition}

\begin{definition}[Bernoulli distribution]
If a random variable \(X\) represents the number of successes in a Bernoulli trial with probability of success \(p\), it follows the \emph{Bernoulli distribution}, and we denote it as
\[
X \sim Bernoulli(p).
\]
\end{definition}

\subsection{Probability function and cumulative distribution function}\label{probability-function-and-cumulative-distribution-function}

\subsection{Bernoulli}\label{bernoulli}

If \(X\sim Bern(p)\), the pf of \(X\) is
\[ f_X(0)=1-p,\quad f_X(1)=p,\quad f_X(x)=0\,\,\text{ for }x\not\in\{0,1\}\]
and the cdf is
\[ F_X(x)=\begin{cases} 0, &\text{ if }x<0, \\ 1-p, &\text{ if } 0\leq x < 1,\\ 1, &\text{ if }x\geq 1\end{cases}\]

\subsection{Binomial}\label{binomial}

\begin{definition}[Bernoulli distribution]
If we have \(N\) independent runs and record the numbers of successes obtained in these \(n\) runs, then \(X\) is said to have a binomial distribution, denoted by \(X\sim Bin(n,p)\).
\end{definition}

Note: The probability function of \(X\sim Bin(n, p)\) is
\[ f(x) = \binom{n}{x} p^x(1-p)^{n-x},\quad x=0,1,2,\dots,n\]
and 0 otherwise.

\section{Relationship and difference between binomial and hypergeometric}\label{relationship-and-difference-between-binomial-and-hypergeometric}

\begin{itemize}
\tightlist
\item
  Binomial and hypergeometric distributions are fundamentally different!
\item
  In Binomial models, we pick \textbf{WITH} replacement, in the hypergeometric model \textbf{WITHOUT} replacement.
\item
  If \(N\) is large and \(n\) is small, the chance we pick the same object twice is small.
\item
  Thus, letting \(r/N=p\), \(X\sim Hyp(N,r,n)\) and \(Y\sim Bin(n,p)\), then we can \textbf{APPROXIMATE}
  \[ P(X \leq k) \approx P(Y\leq k).\]
  (more precisely, in the limit as \(N\rightarrow\infty\) with \(r/N\rightarrow p\) (the ratio of successes converges to the success probability).
\item
  See pages 86/87 and later in the course for more.
\item
  We'll see an example next time.
\end{itemize}

\chapter{Lecture 13, Feburary 05, 2024}\label{lecture-13-feburary-05-2024}

\section{Binomial v.s. hypergeometric}\label{binomial-v.s.-hypergeometric}

\begin{itemize}
\tightlist
\item
  \(Bin(n,\frac{r}{N})\) and \(hyp(N,r,n)\) are fundamentally different!
\item
  If you have an urn with \(r\) successes and \(N-r\) failures\ldots{}

  \begin{itemize}
  \tightlist
  \item
    \ldots{} and you draw \(n\) items, then the number of successes is\ldots{}

    \begin{itemize}
    \tightlist
    \item
      \ldots{} Binomial: drawn \blue{with} replacement, \ldots{}
    \item
      \ldots{} Hypergeometric: drawn \blue{without} replacement.
    \end{itemize}
  \end{itemize}
\item
  \(N\gg n \Rightarrow\) chance we pick same object twice w/ replacement small.
\item
  \alert{Approximation:} For \(p=\frac{r}{N}\), \(X\sim Hyp(N,r,n)\) and \(Y\sim Bin(n,p)\)
  \[ P(X \leq k) \approx P(Y\leq k).\]
  (more precisely, in the limit as \(N\rightarrow\infty\) with \(r/N\rightarrow p\)).
\end{itemize}

\section{Negative binomial distribution}\label{negative-binomial-distribution}

\begin{definition}[Negative Binomial]
Consider an experiment with two possible outcomes \emph{success} (Su) or \emph{failure} (F). Without loss of generality, assume that the \(P(Su)=p\) (so \(P(F)=1-P(Su)= 1-p\)). Repeat the experiment \textbf{independently} until a specified number of \(k\) successes have been observed. Denote \(X\) the number of failures before the \(k\)-th suceess, then \(X\sim NegBin(k,p)\).
\end{definition}

\subsection{Probability function}\label{probability-function}

The probability function of \(X\sim NegBin(k,p)\) is
\[ f(x)=P(X=x)=\binom{x+k-1}{x}p^k(1-p)^x,\quad x=0,1,2,\dots\]
since there are \(\binom{x+k-1}{x}\) to choose \(x\) positions among the first \(x+k-1\) positions to be a failure (and the remaining ones are automatically success), and each of these sequence of outcomes has probability \(p^k(1-p)^x\).

\section{Binomial v.s. Negative Binomial}\label{binomial-v.s.-negative-binomial}

\begin{itemize}
\item
  Binomial distribution: We know number of trials \(n\), but we do not know how many successes.
\item
  Negative Binomial distribution: We know the number of successes \(k\), but we do not know how many trials will be needed.
\end{itemize}

\section{Geometric distribution}\label{geometric-distribution}

\begin{definition}[Geometric]
Consider an experiment with two possible outcomes \emph{success} (Su) or \emph{failure} (F). Without loss of generality, assume that the \(P(Su)=p\) (so \(P(F)=1-P(Su)= 1-p\)). Repeat the experiment \textbf{independently} \textbf{before the 1st success} has been observed. Denote \(X\) the number of failures before 1st success, then \(X\sim Geo(p)\).
\end{definition}

Note: The Geometric distribution is a special case of the negative binomial distribution: \(Geo(P) \sim Neg(k=1,p)\).

\subsection{Probability function and distribution function}\label{probability-function-and-distribution-function-1}

\begin{itemize}
\tightlist
\item
  If \(X\sim Geo(p)\), then \(X\) has probability function
  \[ f(x) = P(X=x)= (1-p)^x p,\quad x\in\{0,1,2,\dots\}.\]
\end{itemize}

\[ F(x)=P(X\leq x) 
= P(X\leq \floor{x}) = 
\sum_{k=0}^{\floor{x}} (1-p)^k p =
1-(1-p)^{\floor{x}+1}\]
if \(x\geq 0\) and \(0\) otherwise.

\begin{itemize}
\tightlist
\item
  Note that \(P(X>x)=1-F(x)=(1-p)^{\floor{x}+1}\) which is nice for computations!
\end{itemize}

\subsection{Memoryless property}\label{memoryless-property}

Let \(X \sim Geo(p)\) and \(s,t\) be non-negative integers. Then, the following equation holds.
\[
P(X \geq s+t | X \geq s) = P(X \geq t).
\]

\subsection{Aside, Reason to call the Negative binomial distribution}\label{aside-reason-to-call-the-negative-binomial-distribution}

(This is contributed by Jeffery!)

Can extend binomial coefficients to \textbf{negative or fractional} ``top'\,' part.

\[\binom{\theta}{x} := \frac{\theta^{(X)}}{x!} = \frac{\theta(\theta-1)(\theta-1)...(\theta-x+1)}{X!}\]

Note: ``bottom'\,' part of coefficient still a non-negative integer.

Then
\begin{align*}
    \binom{-k}{x} 
        & = \frac{-k(-k-1)...(-k-x+1)}{x!} \\
        & = (-1)^x \frac{(x+k-1)(x+k-2)...((x+k-1)-x+1)}{x!}\\
        & = (-1)^x \binom{x+k-1}{x}
\end{align*}
So
\[
    f_X(x) = \binom{x+k-1}{x}p^k(1-p)^x = (-1)^x \binom{-k}{x} p^k(1-p)^x 
\]

\chapter{Lecture 14, Feburary 07, 2024}\label{lecture-14-feburary-07-2024}

\section{Poisson distribution}\label{poisson-distribution}

\begin{definition}[Poisson distribution]
We say the random variable \(X\) has a \{\bf Poisson\} distribution with parameter \(\mu > 0\) if
\[
f(x) = e^{-\mu} \frac{ \mu^x}{x!},\;\;x=0,1,2,\dots\]
\end{definition}

\subsection{Notation}\label{notation}

We write \(X\sim Poisson(\mu)\) or \(Poi(\mu)\), where \(\mu\) is called the \emph{rate} parameter.

\subsection{Interpreation of the Poisson distribution}\label{interpreation-of-the-poisson-distribution}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Limiting case of binomial distribution}, where you fix \(\lambda = np\) , and let \(n \rightarrow \infty\) and \(p \rightarrow 0\) (This can be a consequence of b))
\item
  \emph{Poisson Process}
\end{enumerate}

\section{Poisson as the limiting distribution of the binomial distribution}\label{poisson-as-the-limiting-distribution-of-the-binomial-distribution}

One way to view the Poisson distribution is to consider the limiting case of binomial distribution, where you fix \(\mu = np\) , and let \(n \rightarrow \infty\) and \(p \rightarrow 0\).

One can show that if \(n\to \infty\) and \(p=p_n \to 0\) as \(n\to \infty\) in such a way that \(n p_n \to \mu\), then
\[
{n \choose x} p^x (1-p)^{n-x} \to e^{-\mu} \frac{ \mu^x}{x!},\;\;\;as\;\;\; n\to \infty.
\]
Actually here, it is something called the *convergence in distribution**.

\section{Poisson process}\label{poisson-process}

Consider counting the number of occurrences of an event that happens at random points in time (or space). Poisson process is the \emph{counting process} that satisfies the following

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Independence}: the number of occurrences in non-overlapping
  intervals are independent.
\item
  \textbf{Individuality}: for sufficiently short time periods of length
  \(\Delta t,\) the probability of 2 or more events occurring in the interval is
  close to zero
  \[
  \frac{P\left(  \text{2 or more events in }(t,t+\Delta_t)\right)}{\Delta_t}  \rightarrow 0,\;\; \Delta_t \to 0
  \]
\item
  \textbf{Homogeneity or Uniformity}: events occur at a uniform or
  homogeneous rate \(\lambda\) and proportional to time interval \(\Delta_t\), i.e.
  \[
  \frac{P\left(  \text{one event in }(t,t+\Delta_t)\right) - \lambda\Delta_t }{\Delta_t}  \to 0.
  \]
\end{enumerate}

If \(X=\) occurrences in a time period of length \(t\), then
\[X\sim Poi(\lambda t).\]

\begin{definition}[Poisson process]
A process that satisfies the prior conditions on the occurrence of events is often called a \textbf{Poisson process}. More precisely, if \(X_t, \; \text{for } t\ge0,\) (a random variable for each \(t\)) denotes the number of events that have occurred up to time \(t\), then \(X_t\) is called a Poisson process.
\end{definition}

\section{Side notes -- Rigorous definition of convergence in distribution}\label{side-notes-rigorous-definition-of-convergence-in-distribution}

This section is just served as a reference for those of you who are interested in the rigorous definition of \emph{convergence in distribution}. Do not worry too much if you are not interested in knowing those.

\begin{definition}[Convergence in distribution]
Let \((F_n)_{n\in\mathbb{N}}\) and \(G\) be CDFs. Let \(c(G) = \{x\in\mathbb{R} : G \text{ is cts. at }x\}\) be the set of continuity points of \(G\). \(F_n\) \emph{converges in distribution} to \(G\) if
\begin{align}
    \forall x\in c(G) \quad F_n(x)\to G(x)  \label{eq:convdist}
\end{align}
If \(X_n\) has CDF \(F_n\) for each \(n\) and \(Y\) has CDF \(G\) and \eqref{eq:convdist} holds then \(X_n\) converges in distribution to \(Y\). Denoted \(X_n \stackrel{d}{\to} Y\), or \(F_n \stackrel{d}{\to} G\)
\end{definition}

\chapter{Lecture 15, Feburary 09, 2024}\label{lecture-15-feburary-09-2024}

\section{Review of the distributions we covered before}\label{review-of-the-distributions-we-covered-before}

\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.0739}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4877}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4384}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\centering
Distribution
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
\(f(x)=P(X=x)\)
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Interpretation
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\(U[a,b]\) & \(\frac{1}{b-a+1},\, x=a,a+1,\dots,b\) & Sample from \(\{a,a+1,\dots,b\}\) once uniformly at random\textbackslash{} \\
\(Bin (n,p)\) & \(\binom{n}{x}p^x(1-p)^{n-x},\,x=0,1,\dots,n\) & \(\#\) of successes in \(n\) indep. trials with success prob. \(p\). \\
\(Hyp(N,r,n)\) & \(\frac{\binom{r}{x}\binom{N-r}{n-x}}{\binom{N}{n}},\) \(\max\{0, n-(N-r)\} \leq x \leq \min\{r,n\}\) & \(\#\) of successes in \(n\) draws without replacement from \(N\) objects with \(r\) successes. \\
\(NegBin(k,p)\) & \(\binom{x+k-1}{x}p^k(1-p)^x,\, x=0,1,\dots,\) & \(\#\) of failures until \(k\) successes in indep. trials with success prob. \(p\) \\
\(Geo(p)\) & \(p(1-p)^x,~ x=0,1,\dots \) & \(\#\) of failures until first success in indep. trials with success prob. \(p\) \\
\(Poi(\mu)\) & \(\exp(-\mu) \mu^x/x!,~ x=0,1,\dots \) & \(\#\) of occurrences in Poi process. \\
\end{longtable}

\chapter{Lecture 16, Feburary 12, 2024}\label{lecture-16-feburary-12-2024}

\section{Chapter 7 Expectation and Variance}\label{chapter-7-expectation-and-variance}

Probability and statistics are closely related to data; we often try to ``extract'' additional information from data.

Some simple way to analyse and visualize data are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  frequency table
\item
  frequency histogram
\end{enumerate}

However, sometimes it is unclear how to define the groups in the frequency or in the histogram. Hence, we often would like to have more concise defined ways to analyse the random variable and its associated distribution. We call those numerical values as the (summary) \emph{statistics}.

\begin{definition}[Sample mean]
Let \(x_1, \ x_2, \ldots, \ x_n\) be \(n\) realizations of a random variable \(X\) (such a set is called a \textbf{sample}). The \emph{sample mean} is defined as
\[
\bar{x} = \frac{1}{n}  \sum_{i=1}^n x_i
\]
\end{definition}

Note: there are other summary

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Sample median: a value such that half of the results are below it and the other half above it, when the sample is arranged in numerical order.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  Median is more \emph{robust} against some abnormally big/small observations, or recording errors.
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Sample mode: The most frequently-occuring value in a sample.
\end{enumerate}

\begin{itemize}
\tightlist
\item
  We may have more than one mode.
\end{itemize}

\section{Theoretical mean and the sample mean}\label{theoretical-mean-and-the-sample-mean}

We can compute \emph{statistics} for a random variable \(X\) directly if we know its distribution. Such a mean would be \emph{theoretical}, as we are working from its probability distribution rather than an actual sample.

That means, we can do experiment to get sample, then compute the sample mean, while if we know the distribution, we can compute the theoretical mean without any samples.

\subsection{Definition}\label{definition}

\begin{definition}[(Theoretical) Mean/Expectation/First moment]
Suppose \(X\) is a discrete random variable with probability function \(f_X(x)\). The expected value of \(X\), denoted by \(E[X]\), is then the number
\[
E[X] = \sum_{x\in X(S) } x \ f_X(x) = \sum_{x\in X(S) } x \ P(X=x),
\]
provided the sum converges absolutely (i.e., if \(\sum_{x\in X(S) } |x| \ f_X(x)<\infty\))
\end{definition}

\subsection{Interpretation}\label{interpretation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Geometrical interpretation: \(E[X]\) is the \emph{balancing point} of the probability function \(f(x)\).
\item
  \(E[X]\) is what the average of many, many independent realizations of the random variable \(X\) would approach (\textbf{Law of large numbers}).
\end{enumerate}

\chapter{Lecture 17, Feburary 14, 2024}\label{lecture-17-feburary-14-2024}

\section{More about expectation}\label{more-about-expectation}

Some observations we made from the example we went through in class.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Suppose \(X\) is a random variable satisfying \(a \le X(\omega) \le b\) for all \(\omega \in S\). Then \(a<E[X]<b\).
\item
  We may think that the expectation is a \textbf{weighting} of the values \(x\in X(S)\) by its probability function (which is always positive, and sum up to one).
\end{enumerate}

\section{Law of Unconscious Statistician}\label{law-of-unconscious-statistician}

If
\[
g: {\mathbb R} \to {\mathbb R},
\]
and \(X\) is a random variable with probability function \(f\), then \(g(X)\) is a random variable taking values \(g(X(S))\) and

\[
E[g(X)] = \sum_{x \in X(S)} g(x) f(x)
\]

NOTE: In general, \(E[g(X)]\ne g(E[X])\)!

e.g.~For an arbitrary \emph{convex} function \(g(X)\), \(g\{E(X)\} \le E\{g(x)\}\). This is a famous theorem called \textbf{Jansen's inequality}.

\section{Tricks}\label{tricks}

Sometimes, in order to calculate the expectation, some terminologies may help

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  For \(x\in X(S)\), \(\sum_{x\in X(S)} f_X(x)=1\).
\end{enumerate}

\section{Property of expectation - linearity}\label{property-of-expectation---linearity}

Suppose the random variable \(X\) has \(E[X]=\mu\). Then for any constants \(a,b\in\mathbb{R}\),
\[ E[aX+b]= a \mu + b = a E[X]+b\]

If we have 2 random variables \(X\) and \(Y\), and three constants, \(a,b,c\in \mathbb{R}\), then
\[
  E[aX+bY + c] = aE[X] + bE[Y] +c.
\]

\subsection{Proof.}\label{proof.}

\begin{proof}
By the \emph{Law of unconscious statistician} with \(g(x)=ax+b\), we find
\begin{align*}
E[aX+b] &=\sum_{x\in X(S)} (ax+b) f(x)\\
& = a \cdot  \sum_{x\in X(S)} x f(x) + b  \cdot \sum_{x\in X(S)} f(x) \\
&= a\cdot  E[X]  + b \cdot 1
\end{align*}
\end{proof}

\section{Mean of binomial distribution}\label{mean-of-binomial-distribution}

If \(X \sim Binomial(n,p)\), then \(E[X] = np\)

\begin{proof}
Suppose \(X \sim Bin(n,p)\). Then we have
\begin{align*}
E[X] &= \sum_{x=0}^n x\cdot \binom{n}{x}  p^x (1-p)^{n-x}\\
&= \sum_{x=1}^n x\cdot \frac{n!}{(n-x)!x!} \cdot p^x (1-p)^{n-x}\\
&= \sum_{x=1}^n \frac{x}{x(x-1)!} \frac{n(n-1)!}{(n-x)!} p p^{x-1} (1-p)^{n-x}\\
&= \sum_{x=1}^n \frac{1}{(x-1)!}\frac{n(n-1)!}{(n-x-1+1)!} p p^{x-1} (1-p)^{n-x-1+1}\\
&= np \sum_{x=1}^n \frac{1}{(x-1)!}\frac{(n-1)!}{((n-1)-(x-1))! (x-1)!} p\cdot p^{x-1} (1-p)^{(n-1)-(x-1)}\\
&= np \; \underbrace{\sum_{y=0}^{n-1} \frac{(n-1)!}{((n-1)-y)! y!} \cdot p^{y} (1-p)^{(n-1)-y}}_{=1\text{b/c sum of PF of bin(n-1,p). is 1}}\\
&= np
\end{align*}
\end{proof}

\chapter{Lecture 18, Feburary 16, 2024}\label{lecture-18-feburary-16-2024}

\section{Mean of Poisson distribution}\label{mean-of-poisson-distribution}

Let \(Z\sim Poi(\mu)\). Then the expectation of \(Z\) is \(E[Z] = \mu\).

\begin{proof}
Suppose \(X \sim Poi(\mu)\). Then we have
\begin{align*}
E[X] &= \sum\limits_\text{x \in X(S)} x\cdot f(x) && \text{definition}\\
&= \sum_{x=0}^\infty x \cdot e^{-\mu} \frac{\mu^x}{x!} && \text{P.F.}\\
&= \sum_{x=1}^\infty x \cdot e^{-\mu} \frac{\mu^x}{x!}\\
&= \mu \sum_{x=1}^\infty e^{-\mu} \frac{\mu^{x-1}}{(x-1)!} && \text{let $Y=x-1$}\\
&= \mu \underbrace{\sum_{y=0}^\infty e^{-\mu} \frac{\mu^{y}}{y!}}_{=1\text{ as sum of p.f.}}\\
\end{align*}
\end{proof}

\section{Mean of Hypergeometric and Negative binomial}\label{mean-of-hypergeometric-and-negative-binomial}

\subsection{Hypergeometric}\label{hypergeometric}

If \(X \sim hyp(N,r,n)\), then
\[E[X]= n \frac{r}{N}.\]

Intuitively, \(n \cdot r/N\) is the number of trials \(n\) times the success probability in the

\subsection{Negative binomial}\label{negative-binomial}

If \(Y \sim NB(k,p)\), then
\[E[Y] = \frac{k(1-p)}{p}.\]

\section{Note that the expectation does not always exist}\label{note-that-the-expectation-does-not-always-exist}

\begin{example}
Consider a random variable \(X\) with probability function \(f(x)=\frac{1}{x}\) for \(x=2, 4, 8, 16,\dots\) and 0 otherwise.

The rv \(X\) can only take values \(x\) of the form \(x=2^n\). Note that we can write \(P(X=2^n) = 2^{-n}\) for \(n=1,2,\dots\). Thus
\[\sum\limits_{\text{ all x}} f(x) = \sum_{n=1}^\infty P(X=2^n)= \sum_{n=1}^\infty \left(\frac{1}{2}\right)^n=\frac{1}{1-1/2} -1 = 1.\]
Then
\[ E[X] = \sum_{n=1}^\infty 2^n \left(\frac{1}{2^n}\right)=\sum_{n=1}^\infty 1 = \infty,\]
\end{example}

\chapter{Lecture 19, Feburary 26, 2024}\label{lecture-19-feburary-26-2024}

After discussing about the first moment/expectation/expected value, we want to see how other summary statistics we may want in order to describe a distribution/model.

\section{Motivation to have higher moments}\label{motivation-to-have-higher-moments}

Note: Does look at one summary statistics, the expectation, enough for describing and fully characterize a distribution?

\begin{example}
Suppose we have two random variables

\begin{itemize}
\tightlist
\item
  \(X\) is a r.v. representing the outcome of one fair 6-sided die roll
\item
  \(Y\) is a r.v. representing the number of phone calls over 1 minute at Lenovo call centre, with the rate of 3.5 calls per minute
\end{itemize}

By looking at their expectations, we have
\[
  E X = 3.5 = E Y.
\]
So if we only look at the expectation, we CANNOT DISTINGUISH those two, but clearly those two RV are very different!
\end{example}

Hence, we may need \emph{other quantities} to describe the random variables. One key thing to study is how the RVs \emph{deviate from its mean/expectation.}

\subsection{Deviations from the mean}\label{deviations-from-the-mean}

Some common used ones are

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Deviation
\end{enumerate}

\[\mbox{E}((X- \mu)) = \mbox{E}(X)- \mu = 0\]
2. Absolute deviation
\[ \mbox{E} \left(| X - \mu |\right)\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Squared deviation
  \[ \mbox{E} \left((X-\mu)^2\right) \].
\end{enumerate}

The squared deviation turns out to be particular useful measure of variability, which we coin the term as the \textbf{Variance.}

\section{The defintion of variance}\label{the-defintion-of-variance}

\begin{definition}[variance]
The variance of a random variable \(X\) is denoted by \(Var(X)\), and is defined by
\[
  Var(X) := E[(X-EX)^2].
\]
\end{definition}

\subsection{Properties of variance}\label{properties-of-variance}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Shortcut definition
  \[
    Var(X) = E[X^2] - (E X)^2.
  \]
\item
  Variance is always positive.
\end{enumerate}

This can be easily seen from the definition of the variance, that it is the expectation of the square of \(X-EX\). Everything squared is a non-negative number. And expectation can be thought as the weighted average of a random variable. Thus, it is \textbf{Always Nonnegative}!

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Variance is not linear
\end{enumerate}

Let \(a,b\in \mathbb{R}\), and \(X\) be a RV. Then
\[
  Var(aX +b ) = a^2 Var(X).
\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The case when variance is zero
\end{enumerate}

Suppose a random variable \(X\) has \(E(X)=\mu\) and \(Var(X)=0\). This means, \(X\) does not ``vary'\,' or deviate from its mean at all, and is (with probability 1) always the same value \(\mu\), as we show in the following

\begin{theorem}
\(Var(X) = 0\) if and only if \(P(X = \mbox{E}(X)) = 1\).
\end{theorem}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Alternative way to write the variance
\end{enumerate}

We can write the variance of X as
\[
  Var(X) = E[X(X-1)] + E[X] - (EX)^2.
\]

\begin{proof}
Look at the right-hand-side,
\begin{align*}
  &E[X(X-1)]+ E[X] - E^2[X]\\
  &=E[X^2-X] + E[X] - E^2[X]\\
  &= E[X^2 -X + X - E^2[X]] \\
  &= E[X^2 -\mu^2] = E[X^2] - \mu^2,
\end{align*}
as in our shortcut formula.
\end{proof}

\section{Variance of a binomial distribution}\label{variance-of-a-binomial-distribution}

\begin{theorem}
Suppose that \(X\sim Binomial(n,p)\), then

\[
Var(X) = np(1-p).
\]
\end{theorem}

\begin{proof}
We use the formula \[Var(X) = E(X(X-1)) + E(X) - (E(X))^2\] and note we already know \(E(X)=np\).\pause
Then we tweek the sum for \(E(X(X-1))\) similarly as before:

\begin{align*}
E&(X(X-1)) =\sum_{x=0}^n x(x-1) \frac{n!}{(n-x)! x!} p^x (1-p)^{n-x} \\
&= n(n-1) p^2 \sum_{x=2}^n \frac{(n-2)!}{(n-2-(x-2))!(x-2)!} p^{x-2} (1-p)^{n-2-(x-2)}\\
&= n(n-1) p^2 \underbrace{\sum_{y=0}^{n-2} \frac{(n-2)!}{(n-2-y)!y!} p^{y} (1-p)^{n-2-y}}_{=1\text{ as sum of $Bin(n-2,p)$ p.f.}}\\
&= n(n-1) p^2
\end{align*}
\end{proof}

\chapter{Lecture 20, Feburary 28, 2024}\label{lecture-20-feburary-28-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}

\section{Variance of Poisson, Hypergeometric and Negative Binomial}\label{variance-of-poisson-hypergeometric-and-negative-binomial}

Last time, we saw that if \(X \sim Bin(m,p)\), then \(\mathbb{V}ar(X) = np(1-p)\). We proved this using the definition of the expectation and with the \textbf{summation trick}.

Similarly, one can show that

\begin{itemize}
\tightlist
\item
  If \(X\sim Poi(\lambda)\), then
\end{itemize}

\[
\mathbb{V}ar(X) = \lambda.
\]

\begin{itemize}
\item
  If \(Y \sim hyp(N,r,n)\), then
  \[
  \mathbb{V}ar(Y) = n \frac{r}{N} \left(1-\frac{r}{N}\right)\left(\frac{N-n}{N-1}\right).
  \]
\item
  If \(Z \sim NB(k,p)\), then
  \[
  \mathbb{V}ar(Z) = \frac{k(1-p)}{p^2}.
  \]
\end{itemize}

\section{Standard Deviation}\label{standard-deviation}

Note that \(\mathbb{V}ar(X)\) is in the squared unit (e.g., \(X\) in \(meters\) \(\Rightarrow\) \(\mathbb{V}ar(X)\) is in \(meters^2\)). To recover the original unit, we take the square root of variance.\textbackslash{}

\begin{definition}[Standard Deviation]
The \textbf{standard deviation} of a random variable \(X\) is denoted \(SD(X)\), and defined by
\[
SD(X) = \sqrt{\mathbb{V}ar(X)}.
\]
\end{definition}

\section{Last note of the chapter}\label{last-note-of-the-chapter}

\begin{itemize}
\item
  The expectation and the variance give a simple \blue{summary of the distribution} giving the center and variability of the distribution
\item
  We call \(E[X]\) and \(E[X^2]\) the first and second moment of \(X\)
\item
  In general, \(E[X^k]\) is \textbf{the \(k\)th moment of the distribution of \(X\)}, while \(E[ (X-E(X))^k]\) is \textbf{the \(k\)th central moment of the distribution of \(X\)}
\item
  You'll see other statistics later in STAT 231 and onwards, such as

  \begin{itemize}
  \item
    \textbf{Skewness} (measures asymmetry)
    \[
    E\left[\left( \frac{(X - E(X))}{\sqrt{\mathbb{V}ar(X)}} \right)^3\right].
    \]
  \item
    \textbf{Kurtosis} (measures heavy tailedness)
    \[
    E\left[\left( \frac{(X - E(X))}{\sqrt{\mathbb{V}ar(X)}} \right)^4\right].
    \]
  \end{itemize}
\end{itemize}

\section{Chapter 8 Continuous Random Variables}\label{chapter-8-continuous-random-variables}

\subsection{Continuous random variable}\label{continuous-random-variable}

Let \(X\) be a random variable and \(F_X(x) = P(X\leq x) = P(\{\omega\in S : X(\omega)\leq x\})\) for \(x\in\mathbb{R}\) be its cumulative distribution function (cdf).

We say that the random variable \(X\) is

\begin{itemize}
\item
  \textbf{discrete} if \(F_X\) is \emph{piecewise constant}.

  \begin{itemize}
  \tightlist
  \item
    The jumps of \(F\) are exactly the range of \(X\), \(X(S)\). For \(x\in X(S)\) (at the jumps of \(F\)),
  \item
    the probability function is \(f(x)=P(X=x)=\lim_{h\downarrow 0} F(x+h)-F(x)=\text{size of jump at $x$}\).
  \end{itemize}
\item
  \textbf{continuous} if \(F_X\) is a \emph{continuous function}.
\item
  \emph{absolutely continuous} if
  \[ F_X(x) = \int_{-\infty}^x f(t) dt\]
\end{itemize}

In this course, when talking about continuous random variables, we mean absolutely continuous.
\#\#\# Probability density function

\begin{definition}[Probability Density Function]
We say that an continuous random variable \(X\) with distribution function \(F\) admits probability \textbf{density} function (PDF) \(f(x)\), if

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(f(x)\geq 0\) for all \(x\in\mathbb{R}\);
\item
  \(\int_{-\infty}^\infty f(x)dx = 1\);
\item
  \(F(x)=P(X\leq x)= \int_{-\infty}^x f(t)\;d t\).
\end{enumerate}

In other words, \(F\) is an antiderivative of \(f\), of \(f\) is the derivative of \(F\),
\[ f(x) = F'(x) = \frac{d}{dx} F(x)\]
\end{definition}

\begin{definition}[Support]
The \textbf{support} of a r.v. \(X\) with density \(F\) is the set
\[
supp(f) = \{x \in \mathbb{R}: f(x) \neq 0\}.
\]
\end{definition}

If \(X\) was a discrete random variable instead, these 4 probabilities could all be different.
If \(X\) is a continuous rv with probability density function (pdf) \(f_X(x)\), then

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(P(X=x)=0\)
\item
  \(F(x) = \int_{-\infty}^x f_X(t)dt\)
\item
  \(P(a < X \leq b) = \int_a^b f_X(t)dt\)
\end{enumerate}

We highlight: For a continuous random variable \(X\), \(f(x)\) is \emph{not} \(P(X=x)\), which is always zero.

\subsection{Equality does not matter in the continous case}\label{equality-does-not-matter-in-the-continous-case}

If \(X\) is a continuous random variable, then
\[ P(a<X\leq b) = F(b)-F(a)\]
\[ P(a\leq X \leq b) =  P(a<X\leq b) +P(X=a)=[F(b)-F(a)]+0\]
\[ P(a<X<b)=P(a<X\leq b) -P(X=b)=[F(b)-F(a)]-0\]
\[ P(a\leq X<b) = P(a<X\leq b) +P(X=a)-P(X=b)=[F(b)-F(a)]\]
so if \(X\) is \textbf{continuous}, all these probabilities coincide!

\chapter{Lecture 21, March 01, 2024}\label{lecture-21-march-01-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\section{Law of unconciousness of statistician, continuous version}\label{law-of-unconciousness-of-statistician-continuous-version}

\begin{definition}[LOTUS]
If \(X\) is a continuous random variable with pdf \(f(x)\), and \(g:\mathbb{R}\to \mathbb{R}\) is a function, then
\[
  \mathbb{E}g(x) = \int_{-\infty}^\infty g(x)f(x)dx
\]
provided the expression exists.
\end{definition}

By above, we can calculate the expectation and the variance as follows

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\mathbb{E}X = \int_{-\infty}^\infty x f(x) dx\)
\item
  \(\mathbb{V}ar(X) = \mathbb{E}[(X-\mathbb{E}X)^2] = \int_{-\infty}^\infty (x-\mathbb{E}X)^2 f(x)dx\).
\end{enumerate}

Similar to the discrete random variable case, we have the shortcut formula to calculate the variance:
\[
  \mathbb{V}ar(X) = \mathbb{E}[X^2] - (\mathbb{E}X)^2.
\]

\section{function of random variable}\label{function-of-random-variable}

\begin{itemize}
\tightlist
\item
  If \(X\) is a random variable and \(g\) is a function, then \(Y=g(X)\) is also a random variable
\item
  By the law of the unconscious statistician,
\end{itemize}

\[ \mathbb{E}(Y)=\mathbb{E}(g(X)) = \begin{cases} \sum_{\text{all }x} g(x)\cdot f(x),\quad&\text{if $X$ discrete with pf }f,\\
\int_{\mathbb{R}} g(x)\cdot f(x)dx,\quad&\text{if $X$ continuous with pdf }f\end{cases}
\]

\begin{itemize}
\tightlist
\item
  Next, we are studying how to find the distribution of \(Y=g(X)\).
\end{itemize}

\chapter{Lecture 22, March 04, 2024}\label{lecture-22-march-04-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\section{\texorpdfstring{Receipt to find the distribution of the transformed random varaible \(Y=g(X)\).}{Receipt to find the distribution of the transformed random varaible Y=g(X).}}\label{receipt-to-find-the-distribution-of-the-transformed-random-varaible-ygx.}

In class, we have an easy three steps receipt:

Let \(Y=g(X)\). In general, we can use the following steps to find the pdf of \(Y=g(X)\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Using the support of \(X\), find the support of \(Y=g(X)\) denoted by \(\text{supp}(Y)=\{y\in\mathbb{R}:f_Y(y)>0\}\)
\item
  Express the cdf of \(Y=g(X)\) using the cdf of \(X\): \$F\_Y(y)=P(g(X)\leq y)=\dots \$.
\item
  Compute the pdf of \(Y=g(X)\) by differentiating \(f_y(y)=F_y'(y)\)
\end{enumerate}

Notes: If the function \(g\) is \emph{invertible} and \emph{differentiable} with inverse \(g^{-1}\) on the support of \(Y\), then
\[ f_Y(y)= |(g^{-1})'(y)| f_X(g^{-1}(y)),\quad y\in\text{supp}(Y)\]

Question:
When the function \(g\) is \emph{not strictly increasing (or decreasing)} over the support of \(X\), then we \emph{must be careful} when rewriting the inequality \(P(g(X)\leq y)\).

\section{Quantile}\label{quantile}

\begin{definition}[Quantile]
Let \(p\in[0,1]\). The \(100\times p\)th percentile (or \(100\times p\%\) quantile) of the distribution of \(X\) with cdf \(F_X\) is the value \(c_p\) given by
\[ c_p = \inf\{x\in\mathbb{R}: F_X(x) \geq p \}\]
\end{definition}

\begin{itemize}
\tightlist
\item
  The infimum of a set \(A\) is the largest lower bound of \(A\) (e.g., \(\inf\{x\in\mathbb{R}: 0<x<1\}=0\))
\item
  The quantile function \(p\mapsto c_p\) is also called generalized invere function.
\item
  The probability that \(X\) is at most \(c_p\) is at least \(100\times p\)\%. More precisely, \(c_p\) is the smallest value \(c\) so \(P(X\leq c)\) is at least \(p\).
  *The \textbf{median} of a distribution is its 50\% quantile. \pause
\item
  If the distribution function \(F_X\) is continuous and strictly increasing, it has an inverse \(F^{-1}\), and we get
  \[ c_p = F^{-1}(p)\]
\item
  In the previous clicker question, we computed the 95\% quantile.
\end{itemize}

\chapter{Lecture 23, March 06, 2024}\label{lecture-23-march-06-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\section{Recap the quantile function}\label{recap-the-quantile-function}

\begin{definition}[Quantile]
Let \(p\in[0,1]\). The \(100\times p\)th percentile (or \(100\times p\%\) quantile) of the distribution of \(X\) with cdf \(F_X\) is the value \(c_p\) given by
\[ c_p = \inf\{x\in\mathbb{R}: F_X(x) \geq p \}\]
\end{definition}

\begin{itemize}
\tightlist
\item
  The infimum of a set \(A\) is the largest lower bound of \(A\) (e.g., \(\inf\{x\in\mathbb{R}: 0<x<1\}=0\))
\item
  The quantile function \(p\mapsto c_p\) is also called generalized invere function.
\item
  The probability that \(X\) is at most \(c_p\) is at least \(100\times p\)\%. More precisely, \(c_p\) is the smallest value \(c\) so \(P(X\leq c)\) is at least \(p\).
\item
  The \textbf{median} of a distribution is its 50\% quantile.
\item
  If the distribution function \(F_X\) is continuous and strictly increasing, it has an inverse \(F^{-1}\), and we get
  \[ c_p = F^{-1}(p)\]
\item
  In the previous clicker question, we computed the 95\% quantile.
\end{itemize}

\section{Quantiles for discrete distributions}\label{quantiles-for-discrete-distributions}

If \(F\) is strictly increasing and continuous, the \(p\)-quantile \(c_p\) satisfying \(F(c_p)=p\) is just \(c_p=F^{-1}(p)\), the (ordinary) inverse of \(F\) at \(p\) found by solving \(F(c_p)=p\) for \(c_p\).

If \(F\) has jumps or flat parts, then \(F(c_p)=p\) may not have any solution or infinitely many! In this case, we use
\[ F^{-1}(p)=\inf_{x\in\mathbb{R}}\{F(x)\geq p\},\]
though \(F^{-1}\) is an abuse of notation here and does not mean the ordinary inverse.

\section{Special named distributions}\label{special-named-distributions}

\subsection{Continuous uniform distribution}\label{continuous-uniform-distribution}

\begin{definition}[Continuous uniform distribution]
We say that \(X\) has a continuous uniform distribution on \((a,b)\) if \(X\) has pdf
\[ f(x) =\begin{cases}
      \frac{1}{b-a} & \mbox{ $x \in (a,b)$,} \\
      0 & \mbox{ otherwise }
   \end{cases}
\]
This is abbreviated \(X \sim U(a,b)\).
\end{definition}

\begin{itemize}
\tightlist
\item
  For continuous random variables, \(P(X=x)=0\), so it does not matter mathematically if we think of the uniform distribution as sampling uniformly on \((a,b)\) or \([a,b]\) or \((a,b]\) or \([a,b)\)
\item
  Examples:

  \begin{itemize}
  \tightlist
  \item
    Cutting a stick of length 1 at a random position (\emph{motivating example}!)
  \item
    Spinning a wheel in a game show
  \end{itemize}
\end{itemize}

\subsubsection{Expectation and variance}\label{expectation-and-variance}

\begin{definition}[Expectation of continuous uniform distribution]
Let \(X\sim U(a,b)\). Then
\[E(X) = \frac{a + b}{2}\]
\end{definition}

\begin{proof}
Recall that \(X\sim U(a,b)\) has density \(f(x)=\frac{1}{b-a}\) if \(x\in(a,b)\) and 0 otherwise. Thus,
\begin{align*}
 E(X) &= \int x f(x)\; d x = \int_a^b x \cdot \frac{1}{b-a}\;d x \\
 &= \frac{1}{2} \frac{b^2-a^2}{b-a}=\frac{(b-a)(b+a)}{2(b-a)}=\frac{a+b}{2}.\end{align*}
\end{proof}

\begin{definition}[Variance of continuous uniform distribution]
Let \(X\sim U(a,b)\). Then
\[Var(X) = \frac{(b-a)^2}{12}\]
\end{definition}

\begin{proof}
Recall that \(X\sim U(a,b)\) has density \(f(x)=\frac{1}{b-a}\) if \(x\in(a,b)\) and 0 otherwise. ALso from above, we have \(E X =\frac{a + b}{2}\). Then
\begin{align*}
 E(X^2) &= \int x^2 f(x)\;d x = \int_a^b x^2 \frac{1}{b-a}\;d x \\
 &= \frac{b^3-a^3}{3(b-a)}=\frac{(b-a)(b^2+ab+a^2)}{3(b-a)}=\frac{b^2+ab+a^2}{3}.\end{align*}

Combining and simplifying gives
\[ Var(X) = E(X^2)-E(X)^2 = \frac{(b-a)^2}{12}.\]
\end{proof}

\subsection{Exponential distribution}\label{exponential-distribution}

\begin{definition}[Rate-parametrization of exponential distribution]
We say that \(X\) has an exponential distribution with parameter \(\lambda\), denoted by \(X\sim Exp(\lambda)\), if the density of \(X\) is

\[ f(x) =\begin{cases}
      \lambda e ^{- \lambda x} & \mbox{ $x >0$,} \\
      0 & \mbox{ $x \le 0$ }.
   \end{cases}
\]
\end{definition}

\begin{itemize}
\tightlist
\item
  Since \(\int_{\mathbb{R}} f(x) dx = \int_0^\infty \lambda e^{-\lambda x}dx =1\) and \(f(x)\geq 0\) for all \(x\in\mathbb{R}\), this is a valid pdf.
\end{itemize}

\subsubsection{Moments}\label{moments}

When computing \(E(X)\) and \(Var(X)\), we need to solve integrals
\[ E(X) = \int_0^\infty x\cdot \frac{1}{\theta} e^{-\frac{x}{\theta}}\;d x\]
and
\[ E(X^2) = \int_0^\infty x^2 \cdot \frac{1}{\theta} e^{-\frac{x}{\theta}}\;d x\]
which can be done using integration by parts.

\begin{itemize}
\tightlist
\item
  Alternatively, we can use the \textbf{gamma function}
\end{itemize}

\begin{definition}[Gamma function]
The integral
\[
        \Gamma(\alpha) = \int_0^\infty y^{\alpha - 1} e^{-y} dy, \ \alpha > 0
        \]
is called the gamma function of \(\alpha\).
\end{definition}

\paragraph{Properties of Gamma function}\label{properties-of-gamma-function}

Some useful properties of \(\Gamma(\alpha)\) are

\begin{itemize}
\tightlist
\item
  \(\Gamma(\alpha) = (\alpha - 1)\Gamma(\alpha - 1)\) for \(\alpha > 1\)
\item
  \(\Gamma(\alpha) = (\alpha - 1)!\) for \(\alpha \in \mathbb{N}\)
\item
  \(\Gamma(1/2) = \sqrt{\pi}\)
\item
  The Gamma function is a continuous function that interpolates the factorial function.
\item
  Gamma function is used to derive the Gamma distribution (\(\Rightarrow\) STAT 330), which is extremely important in non-life insurance pricing, and it can be used to model certain brain signals in neuroscience.
\end{itemize}

\subsubsection{Expectation and variance}\label{expectation-and-variance-1}

With the Gamma function at hand, show that if \(X\sim Exp(\theta)\), then
\[ E(X)=\theta\]
and
\[Var(X)=\theta^2.\]

There are other paramaterizations for exponential distribution.
It is sometimes more convenient to express the parameter as \(\frac{1}{\theta}=\lambda\).

\begin{definition}[theta-parametrisation of exponential distribution]
We say that \(X\) has an exponential distribution with parameter \(\theta\) \((X\sim Exp(\theta))\) if the density of \(X\) is

\[ f(x) =\begin{cases}
      \frac{1}{\theta} e ^{- \frac{x}{\theta}} & \mbox{ $x >0$,} \\
      0 & \mbox{ $x \le 0$ }.
   \end{cases}
\]
\end{definition}

If \(\lambda\) denotes the \emph{rate} of event occurrence in a Poisson process, then \(\theta = 1/\lambda\) denotes the \emph{waiting time} until the first occurrence.

\chapter{Lecture 24, March 08, 2024}\label{lecture-24-march-08-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\section{Proof of the moments of exponential distribution}\label{proof-of-the-moments-of-exponential-distribution}

Recall that we can have many ways to show the expectation and the variance of \(X\sim Exp(\theta)\) using the Gamma function. Then we have
\[ E(X)=\theta\]
and
\[Var(X)=\theta^2.\]

Let \(X\sim Exp(\theta)\). We use the change of variable \(y=x\theta\) with \(dx =\theta dy\)
\begin{align*}
        E[X]&= \int_0^\infty x \cdot \frac{1}{\theta} e^{-\frac{x}{\theta}}dx \overset{y=x/\theta}{=}  \int_0^\infty y e^{-y} \theta dy\\
        &= \theta \underbrace{\int_0^\infty y e^{-y}   dy}_{=\Gamma(2)}= \theta \Gamma(2) = \theta \cdot (1!) =\theta 
    \end{align*}
and similarly
\begin{align*}
        E[X^2]&= \int_0^\infty x^2 \cdot \frac{1}{\theta} e^{-\frac{x}{\theta}}dx\overset{y=x/\theta}{=}  \int_0^\infty \theta y^2 e^{-y} \theta dy\\
        &= \theta^2 \underbrace{\int_0^\infty y^{3-1} e^{-y}   dy}_{=\Gamma(3)}= \theta^2 \Gamma(3) = \theta \cdot (2!) =2\theta^2 
    \end{align*}
so that
\[ Var(X) = E[X^2]-E[X]^2=2\theta^2-\theta^2 =\theta^2\]

\subsection{Memoryless property of exponential distribution}\label{memoryless-property-of-exponential-distribution}

\begin{theorem}[Memoryless property]
If \(X \sim Exp(\theta)\), then
\[
        P(X > s + t | X > s) = P(X > t).
        \]
\end{theorem}

\begin{itemize}
\tightlist
\item
  We've seen the memoryless property for the \(Geo(p)\) earlier (and the geometric distribution is the only discrete distribution with this property)
\item
  If a continuous random variable has memoryless property, it must follow exponential distribution.
\item
  Intuitively, both the geometric and exponential distributions measure waiting time until first success
\end{itemize}

\begin{proof}
Recall the cdf of \(X\sim Exp(\theta)\) is
\[ F(x)=P(X\leq x) =\int_{-\infty}^x f(t)dt = \int_0^x \theta^{-1} e^{-t/\theta}dt = 1-\exp(-x/\theta)\]
for \(x>0\) and 0 otherwise. Hence,
\begin{align*}
       P(X > s + t | X > s) &= \frac{P( X>s+t \text{ and }X>s)}{P(X>s)}\\
       &= \frac{P(X>s+t)}{P(X>s)} = \frac{e^{-(s+t)/\theta}}{e^{-s/\theta}}\\
       &= e^{-t/\theta} = 1-F(t) = P(X>t)
   \end{align*}
as desired.
\end{proof}

\chapter{Lecture 24, March 11, 2024}\label{lecture-24-march-11-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}

\section{Sampling realizations of random variables}\label{sampling-realizations-of-random-variables}

\begin{itemize}
\tightlist
\item
  In practice, we may often want to sample realizations of random variables, and use those realizations to conduct some estimation of inference (\(\Rightarrow\) Monte Carlo Simulation)
\item
  For instance, suppose we wish to estimate (approximate) the probability \(P(X>2)\) for \(X\sim Exp(1)\) by simulation

  \begin{itemize}
  \tightlist
  \item
    of course, we can compute \(P(X>2)\) by hand, but let's pretend we can't.
  \item
    In this case, could sample independent realizations \(x_1,\dots,x_n\) from \(Exp(1)\) (numbers that look like realizations \(X(\omega_1),\dots,X(\omega_n)\)), and then estimate
    \[P(X>2)\approx \frac{|\{x_1,\dots,x_n: x_i>2\}|}{n}\]
    that is, we estimate \(P(X>2)\) by the relative frequency of observations larger than 2.
  \end{itemize}
\end{itemize}

\section{Inversion method}\label{inversion-method}

\subsection{With strictly increasing and continuous assumption}\label{with-strictly-increasing-and-continuous-assumption}

\begin{lemma}
If \(F\) is a continuous and strictly increasing cdf of some random variable \(X\) and if \(U\sim Unif(0,1)\), then the random variable \(Y=F^{-1}(U)\) has cdf \(F\).
\end{lemma}

\begin{proof}
Denote by \(F_Y\) the cdf of the random variable \(Y = F^{-1}(U)\). Then,
\[ F_Y(y) = P(F^{-1}(U)\leq x) = P(F(F^{-1}(U))\leq F(y))=P(U\leq F(y)).\]
But for any \(u\in[0,1]\), we know that \(P(U\leq u)=u\), since \(U\sim U(0,1)\). Hence,
\[ F_Y(x) = P(U\leq F(y))=F(y)\]
Hence, the random variable \(Y = F^{-1}(U)\) has the cdf \(F\), as desired.
\end{proof}

\subsection{More general case using the quantile}\label{more-general-case-using-the-quantile}

By using the more general definition of the quantile function
\[F^{-1}(y) = \inf\{x\in\mathbb{R}: F(x)\geq y\}\]
one can show the following generalization:\pause

\begin{theorem}
Let \(F\) be any cumulative distribution function of some random variable \(X\) and \(U\sim U(0,1)\). Then the random variable \(F^{-1}(U)\) has cdf \(F\).
\end{theorem}

\begin{itemize}
\tightlist
\item
  No matter what cdf \(F\) (discrete or continuous), we can sample observations as follows:

  \begin{enumerate}
  \def\labelenumi{\alph{enumi}.}
  \tightlist
  \item
    Sample \(U\sim U(0,1)\) (eg via \texttt{runif()})
  \item
    Return \(X=F^{-1}(U)\).
  \end{enumerate}
\item
  Repeating this \(n\) times independently gives \(n\) realizations from \(F\).
\end{itemize}

\section{Normal/Gaussian distribution}\label{normalgaussian-distribution}

Gaussian distribution is named as Gauss, and is perhaps one of the most important distribution, if not the most important one.

\begin{definition}
\(X\) is said to have a \textbf{normal distribution} (or Gaussian distribution) with mean \(\mu\) and variance \(\sigma^2\) if the density of \(X\) is
\[
f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{\frac{-(x-\mu)^2}{2\sigma^2}}, \;\;\; x\in {\mathbb R}.
\]
We denote it by \(X\sim N(\mu,\sigma^2)\).
\end{definition}

\subsection{Properties of Gaussian distribution}\label{properties-of-gaussian-distribution}

1/ Symmetric about its mean: If \(X \sim N(\mu, \sigma^2)\)

\[
P( X \le \mu - t) = P(X \ge \mu + t).
\]
2. Density is unimodal: Peak is at \(\mu\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Mean and Variance are the parameters:
  \[E(X)= \mu\]
  and
  \[Var(X)=\sigma^2.\]
\item
  \(N(\mu, \sigma^2)\) is sometimes (STAT 231) also parametrised as Gaussian distribution using \(\sigma\) instead of \(\sigma^2\), where
  \[
  X \sim G(\mu, \sigma).
  \]
  That is, \(X\sim N(1, 4)\) and \(X\sim G(1, 2)\) mean the same thing.
\item
  Median = mean = mode = first moment.
\end{enumerate}

\subsection{Problem}\label{problem}

It is difficult to calculate the CDF! If \(X\sim N(\mu, \sigma^2)\), then
\[
P(a \le X \le b ) = \int_a^b \frac{1}{\sqrt{2\pi \sigma^2}} e^{\frac{-(x-\mu)^2}{2\sigma^2}} dx = ???
\]
functions of the form \(e^{-x^2}\) do not have elementary anti-derivatives\ldots{} :(

\chapter{Lecture 26, March 13, 2024}\label{lecture-26-march-13-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}

\section{Normal distribution continue}\label{normal-distribution-continue}

If we have a random variable follows an arbitrary Gaussian distribution, i.e.~\(X\sim \mathcal{N}(\mu,\sigma^2)\). How do we obtain the CDF and quantile? It turns out that we can standardize/transform the RV \(X\) to \(Z\sim\mathcal{N}(0,1)\).

\subsection{Standard normal distribution}\label{standard-normal-distribution}

We say that \(Z\) follows the standard normal distribution if \(Z \sim \mathcal{N}(0,1)\).

Frequently in probability and statistics literature, the density of the standard normal random variable is denoted

\[
\varphi(x)= \frac{1}{\sqrt{2\pi}}e^{\frac{-x^2}{2}},
\]

and the cdf of a standard normal random variable is denoted

\[
\Phi(x)= \int_{-\infty}^x  \frac{1}{\sqrt{2\pi}}e^{\frac{-y^2}{2}}dy.
\]

Aside: Why is there \(1/\sqrt{2 \pi}\) in the pdf? That's because
\[
    \int_{-\infty}^\infty e^{\frac{-x^2}{2}} dx = \sqrt{2\pi},
    \]
so if we divide \(e^{\frac{-x^2}{2}}\) by the integral by \(\sqrt{2 \pi}\) we obtain a valid pdf (non-negative, integrates to 1).

Values of \(\Phi(z)=P(Z\leq z)=\int_{-\infty}^z \varphi(t)dt\) can be approximated numerically with high accuracy. Here we can either use a function in R, \texttt{pnorm()}, or use the \textbf{z-table}.

Now we know how to computer the CDF for \(\mathcal{N}(0,1)\), how to link it to \(\mathcal{N}(\mu,\sigma^2)\)?

\begin{theorem}[Standardising normal random variable]
\leavevmode

\begin{verbatim}
 If $X \sim N(\mu, \sigma^2)$, then 
    $$
    Z = \frac{X - \mu}{\sigma} \sim \N(0,1),
    $$
    and $P(X \leq x) = P\left(Z \leq \dfrac{x - \mu}{\sigma}\right)$.
\end{verbatim}

\end{theorem}

\subsection{Procedure}\label{procedure}

for computing \(P(X\leq x)\) for \(X\sim N(\mu,\sigma^2)\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute \(z=\frac{x-\mu}{\sigma}\) (``z-score'\,').
\item
  Find \(\Phi(z)=P(Z\leq z)\) in the table where \(Z\sim N(0,1)\).
\item
  Return \(P(X\leq x)=\Phi(z)\).
\end{enumerate}

\subsection{Quantile}\label{quantile-1}

The Z-table can also be used to obtain percentiles and quantiles.

\begin{itemize}
\tightlist
\item
  Let \(Z\sim N(0,1)\) and \(p\in(0,1)\). Then we can find the value \(z_p\) so that \(P(Z\leq z_q)=\Phi(z_p)=p\) either by

  \begin{itemize}
  \tightlist
  \item
    \(\dots\) looking at the top of the \(z\)-table and selecting the value \(z_p\) so that \(\Phi(z_p)\) is closest to \(p\)
  \item
    \(\dots\) looking at the bottom of the table, which directly gives the quantile for selected \(p\geq 0.5\) (for \(p<0.5\), use \(\Phi^{-1}(p)=-\Phi^{-1}(1-p)\)). This is \blue{the preferred method}.
  \end{itemize}
\item
  If \(X \sim \mathcal{N}(\mu,\sigma^2)\) and we want \(x_p\) so that \(P(X\leq x_p)=p\) \dots

  \begin{itemize}
  \tightlist
  \item
    First find \(z_p\) for the standard normal distribution \(N(0,1)\).
  \item
    Second, set \(x_p= \mu + \sigma z_p\). Then
    \[P(X\leq x_p) = P(X\leq \mu+\sigma z_p)=P\left( \underbrace{(X-\mu)/\sigma}_{\sim N(0,1)} \leq z_p\right)=\Phi(z_p)=p\]
  \end{itemize}
\end{itemize}

\subsection{68-95-99.7 rule for 1-2-3 standard deviation(s)}\label{rule-for-1-2-3-standard-deviations}

An interesting empirical rule about normal distribution is the \textbf{68-95-99.7} rule, which states:

If \(X \sim N(\mu, \sigma^2)\), then
\[
P( \mu - \sigma \le X \le \mu + \sigma) \approx 0.68
\]
\[
P( \mu - 2\sigma \le X \le \mu + 2\sigma) \approx 0.95
\]
\[
P( \mu - 3\sigma \le X \le \mu + 3\sigma) \approx  0.997.
\]

\chapter{Lecture 27, March 15, 2024}\label{lecture-27-march-15-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}

Recap:

In previous lecture, we discussed about the relationship between a standard normal \(\mathcal{N}(0,1)\) and an arbitrary normal \(\mathcal{N}(\mu,\sigma^2)\). The steps are as fellows.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Transform/standardize \(X\sim\mathcal{N}(\mu,\sigma^2)\) to \(Z\sim\mathcal{N}(0,1)\) by let \(Z=\frac{x-\mu}{\sigma}\).
\item
  Use the \texttt{z-table} or the \emph{R} \texttt{pronrm()} and \texttt{qnorm()} functions.
\item
  If you use the \texttt{z-table} in this class, we only provide the standard normal CDF for \(Z>0\), so you may want to use the symmetric property of the normal distribution when you are asked to calculate the CDF for \(P(Z\le z)\) where \(z<0\).
\end{enumerate}

The other things to remember is the 68-95-99.7 rule for \(\pm\) 1,2, and 3 standard deviation away from the mean \(\mu\).

\section{Chapter 9: Multivariate distributions}\label{chapter-9-multivariate-distributions}

Often, we may face problems that may have have multiple factors/covariates/features/causes. In those situations, only using \textbf{one} random variable is not sufficient to model the outcome. Hence, we need two or more random variables in those situations. When we have more than one random variables, we say we have a \textbf{multivariate distribution}.

Q: How to extend what we have learned from univariate (i.e.~one variable) to the multivariate case?

A: We can extend the definitions we have before from the univariate case to the multivariate case!

\begin{definition}[Random Vector]
Let \(X_1,\dots,X_n\) be random variables defined on a common probability space. The vector \((X_1,\dots,X_n)\) is called a \textbf{random vector}.
\end{definition}

\begin{definition}[Joint Distribution]
Suppose that \(X\) and \(Y\) are \emph{discrete} random variables defined on the same sample space (in general, when we consider two or more random variables it is assumed they are defined on the same sample space.)

The \textbf{joint probability function} of \(X\) and \(Y\) is
\[
f(x,y) = P( \{\omega\in S:X(\omega)=x\} \cap \{\omega\in S:Y(\omega)=y\} )
\]
for \(x\in X(S),y\in Y(S)\) and 0 otherwise.

As in the univariate case, a shorthand notation for this is
\[
f(x,y) = P(X=x,Y=y).
\]
\end{definition}

Q: What if we have more than two random variables?

For a collection of \(n\) discrete random variables, \(X_1,...,X_n\), the joint probability function is defined as

\[
f(x_1,x_2,...,x_n)=P(X_1=x_1,X_2=x_2,...,X_n=x_n).
\]

and we call the vector \((X_1,\dots,X_n)\) a random vector.

\subsection{Properties of the joint probability function}\label{properties-of-the-joint-probability-function}

Let \(f(x,y)\) be a joint probability function. Then

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(0\leq f(x,y) \le 1\)
\item
  \(\sum_{x,y} f(x,y) = 1\).
\end{enumerate}

\subsection{Marginal distribution of the joint distribution function}\label{marginal-distribution-of-the-joint-distribution-function}

The joint probability function \(f(x,y)\) gives us probabilities \(P(X=x, Y=y)\). What if we just care about the probability \(P(X=x)\) (without caring about \(Y\)?)

\begin{definition}[Marginal probability function]
Suppose that \(X\) and \(Y\) are \{\it discrete\} random variables with joint probability function \(f(x,y)\).

The \textbf{marginal probability function} of \(X\) is

\[
f_X(x) = P( X=x )= \sum_{y \in Y(S)} f(x,y).
\]
Similarly, the marginal probability function of \(Y\) is
\[
f_Y(y) = P( Y=y )= \sum_{x \in X(S)} f(x,y).
\]
\end{definition}

\subsection{Comments}\label{comments}

\begin{itemize}
\item
  A common mistake is to think that there is a difference between the marginal distribution of \(X\) and the probability function of \(X\). They are the same!
\item
  When we only have one random variable, say \(X\), you may write the CDF and PF as \(f\), \(F\) to represent \(f_X\) and \(F_X\). However, if you have two or more random variables, say if two, denote by \(X\) and \(Y\). Then we \textbf{NEED} to write the subscript, \(f_X\), \(f_Y\), \(F_X\) and \(F_Y\) to distinguish which random variable you are mentioning.
\end{itemize}

\chapter{Lecture 28, March 18, 2024}\label{lecture-28-march-18-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}

In the previous lecture, we introduced the \textbf{multivariate} case (i.e.~to have more than one random variables). This lecture, we want to see how some concepts we saw before can be used here.

\begin{definition}[Independence between random variables]
\(X\) and \(Y\) are \textbf{independent} random variables if
\[
f(x,y) = f_X(x) f_Y(y)
\]
for all values of \((x,y)\).

More generally, \(X_1, X_2, \ldots, X_n\) are \textbf{independent} if
\[
f(x_1, x_2, \ldots, x_n) = f_1(x_1) f_2(x_2) \ldots f_n(x_n)
\]
for all values of \((x_1, \ldots, x_n)\).
\end{definition}

\textbf{Not}e: This means that if you find a \textbf{single realization} of \((x_1, \ldots, x_n)\) values that doesn't satisfy the above equation, then \(X_1, \ldots, X_n\) are not independent.

\subsection{Conditional probability function for multivaraite random variable}\label{conditional-probability-function-for-multivaraite-random-variable}

\begin{definition}[conditional probability function for bivariate case]
The \textbf{conditional probability function} of \(X\) given \(Y=y\) is denoted \(f_X(x|y)\), and is defined to be
\[
f_X(x|y)= P(X=x|Y=y) = \frac{P(X=x,Y=y)}{P(Y=y)} = \frac{f(x,y)}{f_Y(y)},
\]
Given that \(f_Y(y) > 0\).
\(f_Y(y|x)\) can be defined similarly.
\end{definition}

\subsection{\texorpdfstring{Probably function of \(U=g(X_1,\cdots,X_n)\)}{Probably function of U=g(X\_1,\textbackslash cdots,X\_n)}}\label{probably-function-of-ugx_1cdotsx_n}

With multiple random variables, we are even more interested in \emph{functions} of such variables. For example, Let \(A, M, F\) be the random variables for your assignment, midterm and final grades respectively. Then, it's natural to consider the overall grade \(G = g(A,M,F)\) as a function of random variables \(A, M, F\).

In general, we have the following formula for the \textbf{probability function of \(U = g(X_1, X_2, \ldots, X_n).\)}
\[
P(U = u) = \sum_{\substack{(x_1, \ldots x_n) \text{ such that }\\ g(x_1, \ldots, x_n) = u}} f(x_1, \ldots, x_n)
\]
Of course, we restrict ourselves to \((x_1,\dots,x_n)\) in the range of \((X_1,\dots,X_n)\) and omit terms with \(f(x_1, \ldots, x_n)=0\)

\subsection{Known results for named distributions}\label{known-results-for-named-distributions}

\begin{theorem}[Sum of independent Poisson is Poisson]
If \(X \sim Poi(\lambda_1)\) and \(Y \sim Poi(\lambda_2)\) independently, then \(T = X + Y \sim Poi(\lambda_1 + \lambda_2)\).
\end{theorem}

\begin{proof}
\begin{align*}
    P(X+Y=t) &= \sum_{(x,y):x+y= t} P(X=x, Y=y) \\
    &= \sum_{x=0}^t P(X=x, Y=t-x)\\
    &=   \sum_{x=0}^t  \underbrace{f(x,t-x)}_{=f_X(x)f_Y(t-x)}\\
    & =\sum_{x=0}^t e^{-\lambda_1} \frac{\lambda_1^x}{x!}e^{-\lambda_2} \frac{\lambda_2^{t-x}}{(t-x)!}  \dots\end{align*}
\dots rest is \emph{exercise} and in the course notes.
\end{proof}

\begin{theorem}[Sum of 2 independent binomial is binomial]
If \(X \sim Bin(n, p)\) and \(Y \sim Bin(m, p)\) independently, then \(T = X + Y \sim Bin(n + m, p)\)
\end{theorem}

Proof is left for an exercise.

This can be easily extended to the \(n\)-case.

\begin{theorem}
Let \(X_1, X_2, \ldots, X_n\) each follow \(Bernoulli(p)\) independently. Then,
\[
X_1 + X_2 + \ldots + X_n \sim Bin(n,p).
\]
\end{theorem}

This should not come as a surprise as it is the meaning of binomial distribution -- run \(n\) independent Bernoulli trials with the same probability of success \(p\). Similar, we may have the same relationship between the Negative binomial distribution and the geometric distribution.

\begin{theorem}[Sum of n independent geometric is Negative binomial]
Let \(X_1, X_2, \ldots, X_k\) each follow \(Geo(p)\) independently. Then,
\[
X_1 + X_2 + \ldots + X_k \sim NB(k,p).
\]
\end{theorem}

\begin{theorem}[Conditional distribution of two independent poisson]
Let \(X \sim Poi(\lambda_1)\) and \(Y \sim Poi(\lambda_2)\) independently. Then, given \(X + Y = n\), \(X\) follows binomial distribution. That is,
\[
X|X+Y = n \sim Bin\left(n, \frac{\lambda_1}{\lambda_1 + \lambda_2}\right).
\]
Similarly, for \(Y\), we have
\[
Y|X+Y = n \sim Bin\left(n, \frac{\lambda_2}{\lambda_1 + \lambda_2}\right).
\]
\end{theorem}

The proof is left as an exercise. Hint is to use \(X+Y\sim Poi(\lambda_1+\lambda_2)\).

\chapter{Lecture 29, March 20, 2024}\label{lecture-29-march-20-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}

In this lecture, we are going to introduce a named multivariate distribution -- the \emph{multinomial distribution}.

\section{Multinomial distribution}\label{multinomial-distribution}

The multinomial distribution is a generalization of the binomial distribution.

\begin{definition}[Multinomial distribution]
Consider an experiment in which:

\begin{enumerate}
\item Individual trials have *$k$ possible outcomes*, and the probabilities of each individual outcome are denoted $p_i,\;1\le i \le k$, so that *$p_1+p_2+\cdots+p_k=1$*.

\item *Trials* are *independently repeated $n$* times, with $X_i$ denoting the number of times outcome $i$ occurred, so that $X_1+X_2+\cdots+X_k = n$.
\end{enumerate}
\pause

In this case we say \(X_1,...,X_k\) have a \textbf{Multinomial distribution} with parameters \(n\) and \(p_1,...,p_k\). We use the notation \((X_1,...,X_k) \sim Mult(n,p_1,...,p_k)\).
\end{definition}

\textbf{Note}: The binomial distribution has only 2 class/categories, i.e.~the success/failture, head/tail\ldots with the associated probability \(p_1\) and \(p_2=(1-p_1)\) Whereas in the multinomial distribution, we have \(n\)-different classes/categories, \(1,2,\cdots,n\), with the associated probabilities \(p_1,p_2,\cdots,p_n=1-\sum_{i=1}^{n-1}p_i\).

\subsection{The probability distribution function}\label{the-probability-distribution-function}

If \(X_1,...,X_k\) have a joint multinomial distribution with parameters \(n\) and \(p_1,...,p_k\), then their \textbf{joint probability function} is

\[
f(x_1,...,x_k) = \frac{n!}{x_1!x_2!\cdots x_k!} p_1^{x_1}\cdots p_k^{x_k},
\]

where \(x_1,...,x_k\) satisfy \(x_1+\cdots+x_k = n\), \(x_i \ge 0\). \textbackslash{}
\pause
\medskip

The terms

\[
\binom{n}{x_1, x_2, \cdots,x_k} := \frac{n!}{x_1!x_2!\cdots x_k!}, \quad\text{where } x_1+\cdots+x_k = n,
\]
are called the \textbf{multinomial coefficients}

Just like the binomial coefficient where we can use one variable to two outcomes/class/categories, we can write the multinomial with \(k\)-class as \(k-1\) random variables:

Multinomial distribution over \((X_1, \ldots, X_k)\) can also be written in terms of \(k-1\) variables.

If \(x_1 + \ldots + x_n = n\), and we know \(x_1, \ldots x_{k-1}\), then we can let
\[
x_k = n - x_1 - x_2 - \ldots - x_{k-1}.
\]

Similarly, we can let
\[
p_k = 1 - p_1 - p_2 - \ldots - p_{k-1}.
\]

Thus, we can write the probability function of \(Mult(n, p_1, \ldots, p_k)\) as
\[
f(x_1,...,x_{k-1}) = \frac{n! p_1^{x_1}\cdots p_{k-1}^{x_{k-1}} \left(1 - \sum_{i=1}^{k-1}p_i\right)^{n - \sum_{i=1}^{k-1} x_i}}{x_1!x_2!\cdots x_{k-1}! (n - \sum_{i=1}^{k-1} x_i)!}
\]
\#\#\# Marginal ditribution and associated probability

We often wonder what is the marginal distribution of a multivariate joint distribution. So what's the marginal distribution of the multinomial random variable \((X_1, X_2, \ldots, X_k)\)?

::: \{.theorem name=Marginal distribution of multinomial''\}
Let \((X_1, X_2, \ldots, X_k) \sim Mult(n, p_1, \ldots, p_k)\).
Then,
\[
        X_j \sim Bin(n, p_j),
        \]
for \(j = 1, 2, \ldots, k\).
:::
This follows either by construction (since \(X_j\) counts the number of successes in \(n\) independent trials with constant success prob \(p_j\), or by computing the sum
\[ P(X_j=x_j)=f_j(x_j)= \sum\limits_{\text{all }x_1,x_2,\dots,x_{j-1},x_{j+1},\dots,x_k} f(x_1,\dots,x_{j-1},x_j,\x_{j+1},\dots, x_k)\]
which is illustrated in the case \(k=3\) in the course notes.

We can also write out the marginal distribution for 2 random variables in a multinomial distribution:

\begin{theorem}[Sum of individual rvs in multinomial]
Let \((X_1, X_2, \ldots, X_k) \sim Mult(n, p_1, \ldots, p_k)\). Then,
\[
        X_i + X_j \sim Bin(n, p_i + p_j),
        \]
for \(i \neq j\).
\end{theorem}

Again, the rationale behind it is either by construction (since \(X_i+X_j\) counts the number of successes in \(n\) independent trials with constant success prob \(p_i+p_j\),
or by computing the sum
\[ P(X_i + X_j=t)= \sum\limits_{\text{all }x_1,x_2,\dots,x_k:x_i+x_j=t} f(x_1,\dots, x_k)\]
\#\#\# Conditional distribution of multinomial

The other things that we are often interested in, when we have multivariate distribution, is the conditional distribution. For multinomial distribution, we have the following theorem:

\begin{theorem}[Conditional distribution of multinomial]
Let \((X_1, X_2, \ldots, X_k) \sim Mult(n, p_1, \ldots, p_k)\).
Then,
\[
X_i|X_i + X_j = t \sim Bin\left(t, \frac{p_i}{p_i + p_j}\right),
\]
for \(i \neq j\).
\end{theorem}

\chapter{Lecture 30, March 22, 2024}\label{lecture-30-march-22-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}

In this lecture, we will finish up where we left over -- the properties of multinomial distribution.

\begin{theorem}[Conditional distribution of multinomial]
Let \((X_1, X_2, \ldots, X_k) \sim Mult(n, p_1, \ldots, p_k)\).
Then,
\[
X_i|X_i + X_j = t \sim Bin\left(t, \frac{p_i}{p_i + p_j}\right),
\]
for \(i \neq j\).
\end{theorem}

\subsection{Summary statistics of multivariate distributions}\label{summary-statistics-of-multivariate-distributions}

Akin the univaraite distributions, we want to use the moments to \emph{summaries} the multivariate distributions. First, we define the Mulvariate Law of Unconciouss Statistician (mLOTUS).

\begin{definition}[Bivariate LOTUS]
Suppose \(X\) and \(Y\) are discrete random variables with joint probability function \(f(x,y)\). Then for a function \(g: {\mathbb R}^2 \to {\mathbb R}\),

\[
\mbox{E} \left[g(X,Y) \right] = \sum_{(x,y)} g(x,y)f(x,y).
\]
\end{definition}

\begin{definition}[Multivariate LOTUS]
More generally, if \(g:  {\mathbb R}^n \to  {\mathbb R}\), and \(X_1,...,X_n\) are discrete random variables with joint probability function \(f(x_1,...,x_n)\), then

\[
\mbox{E} \left[g(X_1,...,X_n)\right] = \sum_{(x_1,...,x_n)} g(x_1,...,x_n)f(x_1,...,x_n).
\]
\end{definition}

With mLOTUS, we may define the expectation for multivariate distributions. In addition, the expectation has the linearity property as in the univariate case.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\mbox{E}[ a g_1(X,Y) + b g_2(X,Y) ] = a \cdot \mbox{E}[g_1(X,Y)] + b \cdot \mbox{E}[g_2(X,Y)].\)
\item
  \(\mbox{E}[X+Y] = \mbox{E}[X] + \mbox{E}[Y]\)
\end{enumerate}

Observation: Regardless of the relationship between the random variables \(X\) and \(Y\), we HAVE the linearity property!

Proofs are left as exercises.

\begin{lemma}[Expectation for trivariate multinomial]
Let \((X_1, X_2, X_3) \sim Mult(n, p_1, p_2, p_3)\). Then
\[
E[X_1 X_2] = n(n-1) p_1 p_2.
\]
\end{lemma}

\subsection{Relationship betwen the random variables}\label{relationship-betwen-the-random-variables}

So far, we only discuss about the independence between the ranodm variables. E.g. if \(X\) and \(Y\) are independent, we have \(f(x,y) = f_X(x)f_Y(y)\). However, what if they are not independent? In this case, we say they are dependent. We want to first determine if two random variables are dependent, and then measure how strong the \emph{correlation} is. To do so, we use the terminology \textbf{covariance}.

\begin{definition}[Covariance]
For two random variables \(X\) and \(Y\), we define
\[
Cov(X,Y) = \mathbb{E}\left[ (X - E(X))(Y-E(Y)) \right].
\]
as the covariance between \(X\) and \(Y\), provided the expression exists.
\end{definition}

Similarly to \(Var(X)=E(X^2)-E(X)^2\), we have a \emph{shortcut formula} for the covariance:

\[
Cov(X,Y) = \mathbb{E}[XY] - \mathbb{E}[X] \mathbb{E}[Y].
\]

Actually, we can write the variance as a covariance as follows:

\begin{itemize}
\tightlist
\item
  \(\mathbb{V}ar(X) = cov(X,X) = \mathbb{E}[(X-\mathbb{E}X)(Y-\mathbb{E}Y)] = \mathbb{E}[XX] - \mathbb{E}X \mathbb{E}X\), which can be simplified as what we have above.
\end{itemize}

\textbf{Intuition}:
Why do we look at \(E \left[ (X - E(X))(Y-E(Y)) \right]\)?

Positive correlation: In this case, \(Cov(X,Y)>0\)

\begin{itemize}
\tightlist
\item
  Suppose \(X,Y\) are positively related (when \(X\) large, \(Y\) likely large; when \(X\) small, \(Y\) likely small)
\item
  If \((X-E(X))>0\) (so \(X\) large), then likely \((Y-E(Y))>0\) (so \(Y\) also large), hence the product \((X-E(X))(Y-E(Y))>0\)
\item
  If conversely \((X-E(X))<0\) (so \(X\) small), then likely \((Y-E(Y))<0\) (so \(Y\) also small), hence the product is likely \((X-E(X))(Y-E(Y))>0\)
\end{itemize}

Negative correlation:

\begin{itemize}
\tightlist
\item
  Conversely, suppose \(X,Y\) are negatively related (when \(X\) large, \(Y\) likely small; when \(X\) small, \(Y\) likely large). Then \(Cov(X,Y)<0\).
\end{itemize}

\subsection{Relationship between independence and covariance}\label{relationship-between-independence-and-covariance}

One may wonder that does indpendence and covariance relate, if so, in what way?

\begin{theorem}
If \(X\) and \(Y\) are independent, then \(Cov(X,Y)=0\).
\end{theorem}

\textbf{NOTE}: \textbf{The converse statement is FALSE!!!} That is, if \(Cov(X,Y)=0\) then \(X\) and \(Y\) are not necessarily independent. For instance, let \(X\sim U(-1,1)\), and let \(Y =X^2\). Then \(Cov(X,Y)=0\) but \(X\) and \(Y\) are not independent.

\chapter{Lecture 31, March 25, 2024}\label{lecture-31-march-25-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}

In today's lecture, we continue to investigate the covariance and its *standaridized'' version, the correlation. We will then look at the special form of the \(g(X_1,\cdots,X_n)\) to be a linear function

\section{Covariance and Correlation}\label{covariance-and-correlation}

\begin{theorem}[Independence implies uncorrelated]
If \(X\) and \(Y\) are independent, then \(Cov(X,Y)=0\).
\end{theorem}

NOTE: the coverse is in general NOT TRUE.

\begin{proof}
We show that if \(X,Y\) are independent, then \(E(X\cdot Y)=E(X)E(Y)\). This implies
\[ Cov(X,Y)=E(XY)-E(X)E(Y)=E(X)E(Y)-E(X)E(Y)=0.\]
And indeed, because \(X,Y\) are independent,
\[ f(x,y)=f_X(x)f_Y(y),\]
so
\begin{align*}
  E(XY)&=\sum_x \sum_y x\cdot y \cdot f(x,y)\\
  &= \sum_x \sum_y x\cdot y \cdot f_X(x)f_Y(y)\\
  &= \sum_x x \cdot f_X(x) \sum_y y \cdot f_Y(y)\\
  &= E(X) E(Y)
\end{align*}
\end{proof}

\begin{corollary}
If \(X_1,\dots,X_n\) are independent random variables, then
\[ E(X_1\cdot X_2 \cdot \dots \cdot X_n) = E(X_1) E(X_2)\cdot \dots \cdot E(X_n).\]
\end{corollary}

Problem: What does the numeric value tell us when we calculate the covariance? Does big number corresponding to a strong correlation? If two covariance are big, which one is bigger? Do we have an universal way to compare? \textbf{YES} we do! We want to look at the standardized covariance, which we called the \textbf{correlation}.

\begin{definition}[Correlation]
The \textbf{correlation} of \(X\) and \(Y\), denoted \(corr(X,Y)\), is defined by

\[
corr(X,Y) = \rho = \frac{Cov(X,Y)}{SD(X)SD(Y)}.
\]

We say that X and Y are \textbf{uncorrelated} if \(Cov(X,Y) = 0\) (or equivalently \(corr(X,Y)=0\)).
\end{definition}

Remark:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It follows from the Cauchy-Schwarz inequality that \(-1 \le corr(X,Y) \le 1\), and if \(|corr(X,Y)|=1\), \(X=aY+b\). We will look at this in the next open tutorial.
\item
  If \(X\) and \(Y\) are independent, then \(X\) and \(Y\) are
  uncorrelated.
\item
  \(Cov(X,X)=Var(X)\)
\item
  The correlation is unit-free.
\end{enumerate}

Thus, we can conclude the following properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\rho = corr(X,Y)\) has the same sign as \(Cov(X,Y)\)
\item
  \(-1 \leq \rho \leq 1\)
\item
  \(|\rho| = 1 \ \Leftrightarrow \ X = aY + b\)
\item
  \(X,Y\) independent \(\Rightarrow\) \(corr(X,Y) = 0\)
\item
  \(corr(X,Y)=0\) \red{$\not\Rightarrow$} \(X,Y\) independent in general
\item
  \(corr(X,X) = cov(X,X)/SD(X)^2 = Var(X)/Var(X) = 1\)
\end{enumerate}

Cautious: Correlation DOES NOT implies CAUSALITY! Two variables being correlated does not always imply that one variable causes another to behave in certain ways.

\section{Linear combination of random variables}\label{linear-combination-of-random-variables}

Suppose that \(X_1,...,X_n\) are jointly distributed RVs with joint probability function \(f(x_1,...,x_n)\). A \{\bf linear combination\} of the RVs \(X_1,...,X_n\) is any random variable of the form
\[
\sum_{i=1}^{n} a_i X_i
\]
where \(a_1,...,a_n \in \mathbb{R}\).

\subsection{Some examples}\label{some-examples}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  The total, obtained for \(a_i=1\) for all \(i\),
  \[
    T = \sum_{i=1}^{n} X_i .
  \]
  The sample mean, obtained for \(a_i=1/n\) for all \(i\),
  \[
    \bar{X} = \sum_{i=1}^{n} \frac{1}{n} X_i .
  \]
  When needed, we write \(\bar{X}_n\) instead of \(\bar{X}\) to highlight that it is the average of \(n\) many \(X_1,\dots,X_n\).
\end{enumerate}

Question: How do we summary the linear combination of the random variables? Similar as before, we use the \emph{moments} to do this. In particular, we use the first and the second moments (i.e.~the expectation adn the variance).

\begin{theorem}[Expected Value of a Linear Combination]
For any random variables \(X_1,\dots,X_n\) and any constants \(a_1,\dots,a_n\in\mathbb{R}\), we have
\[
E \left( \sum_{i=1}^{n} a_i X_i  \right) = \sum_{i=1}^{n} a_i E(X_i)
\]
\end{theorem}

This follows directly from the linearity of expected value.

NOTE: We \textbf{DO NOT make any assumption regarding the (in)dependence} of the \(X_1,\dots,X_n\) here.

\begin{theorem}[Bilinearity of $Cov$]
Let \(X, Y, U, V\) be random variables, and \(a, b, c, d \in \mathbb{R}\). Then,
\%pause
\begin{align*}
    &Cov(aX + bY, cU + dV)\\ 
    &= acCov(X,U) + adCov(X,V) + bcCov(Y,U) + bdCov(Y,V)
\end{align*}
\end{theorem}

NOTE: You can generalise this to a linear combination of arbitrary length, but the formula becomes messy.

\begin{theorem}[Variance of a linear combination]
Let \(X,Y\) be random variables, and \(a,b \in \mathbb{R}\). Then,
\[
Var(aX + bY) = a^2 Var(X) + b^2 Var(Y) + 2ab Cov(X,Y).
\]
\%pause
In general,
\[
Var\left(\sum_{i=1}^n a_i X_i\right) 
= \sum_{i=1}^n a_i^2 Var(X_i) + 2\sum_{1\le i < j \le n} a_ia_j Cov(X_i,X_j).
\]
\end{theorem}

\begin{theorem}[linear function of a normal $X$ is still normal]
Let \(X \sim N(\mu, \sigma^2)\) and \(Y = aX + b\), where \(a,b \in \mathbb{R}\). Then,
\%pause
\[
Y \sim \mathcal{N}(a\mu + b, a^2 \sigma^2).
\]
\end{theorem}

\begin{proof}
This can be easily shown by using the moment generating function (MGF).
\end{proof}

\chapter{Lecture 32, March 27, 2024}\label{lecture-32-march-27-2024}

\newcommand{\var}{\mathbb{V}ar}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathcal{N}}

In this lecture, we will see 1) more examples and properties of the linear combinations of \textbf{independent normal} random variables, and 2) indicators

\section{Linear combination (continue)}\label{linear-combination-continue}

\begin{definition}
Suppose that \(X_1,...,X_n\) are jointly distributed RVs with joint probability function \(f(x_1,...,x_n)\). A \{\bf linear combination\} of the RVs \(X_1,...,X_n\) is any random variable of the form
\[
  \sum_{i=1}^{n} a_i X_i
  \]
\%pause
where \(a_1,...,a_n \in \mathbb{R}\).
\end{definition}

We have seen
\[ E\left(\sum_{i=1}^n a_i X_i\right)=\sum_{i=1}^n a_i E(X_i)\]
and
\[
        Var\left(\sum_{i=1}^n a_i X_i\right) 
        = \sum_{i=1}^n a_i^2 Var(X_i) + 2\sum_{1\le i < j \le n} a_ia_j Cov(X_i,X_j)
        \]
In particular,
\[ Var(X+Y) = Var(X) + Var(Y) + 2 Cov(X,Y).\]

\subsubsection{Variance of a linear combination of random variables}\label{variance-of-a-linear-combination-of-random-variables}

Since independent random variables have zero covariance, we have the following corollary:

\begin{corollary}
Suppose \(X_1,\dots,X_n\) are \textbf{independent}. Then
\[\mathbb{V}ar\left(\sum_{i=1}^n X_i\right)=\sum_{i=1}^n Var(X_i).\]
\end{corollary}

\subsection{Linear Combination of Gaussian random variables}\label{linear-combination-of-gaussian-random-variables}

We have already seen the following theorem when we were standardizing normals:

\begin{theorem}
Let \(X \sim N(\mu, \sigma^2)\) and \(Y = aX + b\), where \(a,b \in \mathbb{R}\). Then,
\%pause
\[
Y \sim N(a\mu + b, a^2 \sigma^2).
\]
\end{theorem}

Previous discussion focused on the mean and variance of \(\sum_{i=1}^n a_iX_i\). If the \(X_i\) are independent and normally distributed, then we can even state explicitly the distribution of \(\sum_{i=1}^n a_iX_i\).

\begin{theorem}
Let \(X_i \sim N(\mu_i, \sigma_i^2), \ i = 1, 2, \ldots, n\) \textbf{independently}, and \(a_1, a_2, \ldots, a_n, b_1, b_2, \ldots, b_n \in \mathbb{R}\). Then,
\[
\sum_{i=1}^n (a_i X_i + b_i) \sim N \left( \sum_{i=1}^n a_i \mu_i + b_i, \sum_{i=1}^n a_i^2 \sigma_i^2 \right).
\]
\end{theorem}

We really do need that the \(X_i\) are normally distributed. A common mistake is to think that if \(E(X_i)=\mu_i\) and \(Var(X_i)=\sigma_i^2\), then we can apply the theorem. But this is false (take, for instance, the \(X_i\) as independent exponentials as an example).

NOTE: If \(X,Y\sim F\), \(X+Y\) and \(2X\) does not follow the same distribution (because of the variance of the joint distribution has non-linear relationship)!!

\begin{corollary}
Let \(X_1,\dots,X_n\) be independent and \(X_i\sim N(\mu,\sigma^2)\) for all \(i=1,\dots,n\). Then
\[\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \sim N\left(\mu, \frac{\sigma^2}{n}\right). \]
\end{corollary}

As \(n\) increases, the variance \(\sigma^2/n\) decreases, so the distribution of \(\bar{X}_n\) becomes more concentrated around \(\mu\).

\section{Indicator variable}\label{indicator-variable}

\begin{definition}
Let \(A \subset S\) be an event. We say that \(\mathbb{1}_{ A}\) is the \textbf{indicator} random variable of the event \(A\). \(\mathbb{1}_{ A}\) is defined by:
\pause
\[ \mathbb{1}_{ A}(\omega) =\begin{cases}
      1 & \mbox{ $\omega \in A$,} \\
      0 & \mbox{ $\omega \in \bar{A}$ }
   \end{cases}
\]
\end{definition}

NOTE: Actually, it's another way of defining a \textbf{Bernoulli random variable}.

\subsection{Properties of the indicator variable}\label{properties-of-the-indicator-variable}

Indicator (Bernoulli) variable has a few interesting properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\mbox{E}[\mathbb{1}_A] = P(A)\)
\item
  \(Var(\mathbb{1}_A) = P(A)(1-P(A))\)
\item
  \(\mbox{cov}(\mathbb{1}_A,\mathbb{1}_B) = P(A\cap B ) - P(A)P(B)\)
\end{enumerate}

\begin{proof}
These quantities are easy to compute as the random variable \(\mathbb{1}_A\) only takes 2 values. Indeed,
\[E(\mathbb{1}_A) = 1 \cdot P(\mathbb{1}_A = 1) + 0 \cdot P(\mathbb{1}_A=0) = 1 \cdot P(A) \]
and
\[E(\mathbb{1}_A^2) = 1^2 \cdot P(A) + 0^2 \cdot (1-P(A)) = P(A)\]
so
\[ Var(\mathbb{1}_A)=E(\mathbb{1}_A^2)-E(\mathbb{1}_A)^2=p-p^2=p(1-p)\]
Similarly,
\begin{align*}
    E(\mathbb{1}_A\cdot \mathbb{1}_B) &= 1\cdot P(\mathbb{1}_A =1,\mathbb{1}_B=1)+0=P(A\cap B)\end{align*}
giving us
\[ Cov(\mathbb{1}_A,\mathbb{1}_B)=E(\mathbb{1}_A\cdot \mathbb{1}_B)-E(\mathbb{1}_A)E(\mathbb{1}_B)=P(A\cap B)-P(A)P(B)\]
\end{proof}

Q: Why do we care whatsoever about indicator random variables??

A: To make many calculations (like computing the mean and variance) vastly easier, and to gain intuition about how random variables are constructed/behave.

  \bibliography{book.bib,packages.bib}

\end{document}
